Pyxis Software Specification
Executive Summary
Pyxis is the code name for Recogni’s technology which delivers a complete hardware and software stack for multimodal generative AI inference acceleration. The Product Requirements Document (PRD) defines all of its major components in detail; we provide a short summary here. 


Pyxis hardware begins with the Pyxis ASIC, our AI inference accelerator. Each ASIC is combined with HBM (High Bandwidth Memory) into an MCM (Multi-Chip Module). Multiple MCMs are combined into an AI Inference Line Card (ILC), which can plug seamlessly into a partner’s router chassis to make use of its high-speed fabric interconnect. Multiple ILCs plugged into such a chassis create a Recogni AI Inference Pod, or Pyxis Pod.


The Pyxis software ecosystem[a][b][c][d][e][f] enables customers to create and maintain inference systems within a datacenter for a number of users. Pyxis software falls into three broad categories: 


1. System software includes all the software that keeps the machines running. It includes all the operating systems, kernels, and firmware. System software also monitors temperature, power consumption, and other indicators of system health. It provides the framework upon which more specialized Pyxis software is run.

2. Datacenter software includes all the software to connect the internals of Pyxis calculations to the outside world. This class of software deploys and orchestrates applications, accepts client requests, load balances among various Pyxis pods, prepares requests for graph execution through tokenization and other processes, and packages and returns results to the client. Kubernetes manages ILC, Pyxis, and other limited resources. Finally, the Inference Server takes care of session management (if required) and model loading.

3. Model Preparation software includes all the tools needed to make a model run optimally on a Pyxis system. This class of software doesn’t run during the normal inference process. Model preparation brings user models from their PyTorch origins through the Python SDK conversion, PTQ optimizations, and compilation to a packaged archive file that can execute on the Pyxis hardware. Profiling and debugging tools include a proxy profiler, bit-accurate emulation, Conviewcius, and Racer to help achieve the best possible inference performance of the model on Pyxis. A Python API of our compiler’s intermediate representation (IR), called Lunarite, will allow customers to define custom operators. Comprehensive documentation and tutorials explain the SDK usage, and a model zoo will showcase our system’s capabilities. Early versions of this software will be released as a customer model analysis tool (CMAT), functioning as a pre-sales tool.
Software Overview
  

System Software Overview
The software bundle that Recogni plans to release includes not just the model preparation and data center management software. Recogni will also provide System Software, which covers all Recogni-managed software that deploys in our chassis. The components for which we provide software range from the firmware that runs on the Pyxis devices, to the OS/customizations that we provide to drive our devices for inference execution. There should be strong versioning between the System Software, Datacenter Software, and Model Preparation Software.


The first versions of the Recogni inference system provide a combination of hardware and software from Juniper Networks, who is our collaboration partner. Juniper packages their custom JunOS Linux operating system, which we plan to leverage on the management layer for the management cards (MC) and inference line cards. JunOS will run on the MCs and ILCs, with a Linux kernel (or similar) running on the Pyxis devices. The specifics of how the hardware is triggered, timed and managed is managed by the compiler runtime and is opaque to the system software (other than providing an environment for the runtime).
Datacenter Software Overview
Recogni’s Datacenter Software stack performs the clustering and group selection of Pyxis devices to cooperate on generative AI inference tasks. 


Kubernetes (K8s) is an open source system for automating deployments and managing containerized applications. It is the industry standard supported by all major cloud providers. We leverage K8s to orchestrate model deployments across our chassis.


Pyxis devices and their parent line card CPUs/memory are abstracted as K8s resources. Pyxis device capabilities and provisioning APIs are exposed with industry best practices in mind. Resource abstraction extends to device selection, workloads provisioning, and enlistment into load balancers. 


The Datacenter Software manages the relationship between Pyxis devices, connected in an any-to-any fashion via our fabric interconnect. Currently, our selection criteria for Pyxis devices is dictated by having one or more Pyxis devices attached to a worker node (ILC CPU) via PCIe. All remaining Pyxis devices can be any type if they’re local to the same chassis. The Fabric Interconnect section of this document covers this concept in greater detail.


The Inference Server is a software component that receives, queues, and batches client inference requests. It passes those requests on to the system that executes the required computations and gathers results to return to the client. To do this, the Inference Server works closely with the Recogni Device Manager (RDM), which communicates directly with the Pyxis chips. The RDM is responsible for retrieving the Pyxis device IDs and other info. It also deploys to the Pyxis chips the constants (weights, etc…), execution environment (e.g., LuaJIT compiler), and programs (e.g., shard.lua scripts) needed to perform the inference task. 


The LuaJIT Compiler is a Lua execution environment that makes use of Just In Time Compilation to improve performance. It runs on the Pyxis CPU cores, and embeds a C++ layer exposing the memory, interrupts, and other low level APIs necessary to control the DMAs, UIEs, etc..
Model Preparation Overview
Pyxis includes multiple customer tools that prepare models for execution. These tools are not generally run on the Pyxis hardware. They are intended for developers to use on their own in preparation for later inference runs.
Compilation Flow
Several Python machine learning (ML) frameworks like PyTorch, Tensorflow, and Jax have emerged. PyTorch leads in community adoption and support with its flexible high-level API for neural network implementations, so Recogni has focused on it. To promote efficient deployment to various hardware backends, PyTorch provides tools to transform those diverse neural net implementations into well-behaved graph representations. These tools leverage advanced graph capture and compilation methods such as fx and Dynamo. PyTorch graphs consist of nodes from PyTorch’s intermediate representation (IR), ATen.

Compilation begins with the Front End. Recogni supports PyTorch's fx infrastructure and ATen IR for deployments, adding a thin translation layer to our internal compiler IR. Other frameworks like Jax could be used by adding a similar translation layer. Use of the ATen IR streamlines the translation process between framework and compiler IR.


Recogni’s own IR is called Lunarite. When a Lunarite graph is serialized, it is represented as a standalone Lua script. The script is then interpreted by Recogni’s compiler ROCK and compilation of the graph commences therefrom. Lunarite graphs are constructed/transformed via a Python package that enables neural network graph construction, modification, serialization, and export. Various pieces of Model Preparation software depend on this package to implement legalization of PyTorch ATen graphs. A similar process could be implemented for any other ML framework.


The Conversion SDK is a Python library that allows developers to quantize PyTorch models to Recogni’s proprietary low-precision formats and to optimize the models via various PTQ methods. It contains automated methods to select the best quantization format (e.g., FP8 or FP16) and configurable Pyxis parameters like the exponent bias (EB) and the chunk-size of our accumulation. The SDK also provides state-of-the-art PTQ optimization methods like GPFQ, bias correction, etc., allowing developers to optimize model accuracy. A bit-accurate CUDA emulation enables developers to test the model accuracy without the hardware being available. The Conversion SDK imports models in ATen or Lunarite, and exports them in Lunarite for direct forwarding to the compiler.


ROCK, the Recogni Orchestration and Compilation Kit, is a heterogeneous, targetable, optimizing, graph compiler that uses a custom frontend to abstract away neural network development frameworks. It also supports dynamic shapes and type-deduction with both compile-time and runtime expression generation, predication, and evaluation. ROCK compilation finishes with the generation of a REX file that contains all the necessary parts to execute a graph on Pyxis; namely, it contains a metadata json file with the deployment configuration, location info about the constants (eg weights), and the so called shard.lua scripts to schedule and run the model. Each script runs on a Pyxis CPU core managing an associated Pyxis cluster. The REX file also contains a copy of the LuaJIT compiler used to execute the aforementioned scripts. Additionally, as an optional output, the ROCK can produce a debug-symbols database for debugging, tracing, profiling, and other such purposes.


HAAPI is the component of the ROCK that is responsible for lowering the ultimate Lunarite representation of a graph into the microcode and bit patterns that are included within shard.lua files. It consists of a library with APIs in Lunarite, C++, and Python. HAAPI contains facilities that enable programming Pyxis entities at multiple levels of abstraction, from microcode fragments for individual sequencers and UIEs, up to clusters and multi-Pyxis sharding tools. Beyond its role in the compilation flow, users of Design Verification (DV) and Virtual Platform (TLM) tools can leverage HAAPI to generate test vectors.
Analysis and Profiling Tools
The Model Zoo is a collection of state-of-the-art generative AI models that Recogni has evaluated on Pyxis. It showcases the inference performance and model accuracy on Pyxis via a visually appealing web interface. Developers can easily reproduce the results via provided conversion and deployment scripts. An initial version of the model zoo might be shipped as part of the conversion SDK, while the final version will be accessible via our website. The SparseZoo from NeuralMagic is a good example of such a model zoo.


The Lunarite toolset includes the ability to convert Lunarite models back into PyTorch. Such a back-converted model can be run through the Conversion SDK’s CUDA emulation code to verify the model on a functional level, producing bit identical output data to the final hardware.


The Proxy Profiler estimates time performance of a model by computing the number of cycles needed to execute specific ops on the Pyxis chip. A model is treated as a list of operations that are executed on the chip, ignoring how the computation graph is scheduled. Thus it can compute UIE and chip utilization for individual ops as well as a rough estimate of the overall runtime under the assumption that scheduling can be done without creating bottlenecks. Additionally, the profiler tracks HBM bandwidth requirements for parameters and cached activations, as well as communication bandwidth requirements for tensor parallel execution. Based on these capabilities it reports various metrics for LLMs like TTFT or tokens / s.


The Virtual Platform / TLM is a Transaction-Level (TL) functional Model that simulates the behavior of the hardware from a software programmer’s view (PV). Its primary goal is to emulate running production software before the hardware is available. This simulation model is bit-accurate and sequence accurate: it generates the same sequence of output data as hardware if the input sequence is the same. It reflects the microarchitecture interfacing with software and abstracts out the parts invisible to software, including handshaking protocols between individual blocks. It is neither timing accurate nor protocol accurate. 


Racer simulates the timing of dataflow through a model of a hardware system. For Pyxis, Racer includes a hierarchy of block models from the individual memories and computational components of UIEs up to a full Pyxis Pod. A layer of software schedules how particular tasks are performed on that hardware model, transaction by transaction. A final layer of software implements graphs within that description, allowing for annotated, detailed timing simulation of how particular graphs would run on a Pyxis system. This simulation provides utilization and performance information helpful for hardware design, compiler optimization tuning, and system parameter studies.


The Tempus Model is a fixed hardware model of Pyxis that can be programmed using an MLIR dialect. The Tempus model operates at a transaction level and trades cycle accuracy in favor of execution speed. It estimates the number of cycles for a given program. Tempus scales horizontally and utilizes native OS primitives. It can be utilized by a compiler to estimate performance of a program and make optimization decisions.


Conviewcius is an analysis tool that examines graphs from .rex files. It provides a graphical interface to explore information about memory consumption structure, the arrangement and tracking of tensors, and profiling statistics. Additionally, it can link to the Rock compiler, to support interactive compilation, deployment, and debugging of models. 


Pyxis has hardware facilities that enable tracing and profiling. As supported by firmware, Pyxis will have the ability to record trace data directly into a memory buffer located on the Pyxis device. This trace data, which captures various events close to the hardware, will be subsequently retrieved from the device and displayed on a unified graph via a trace analysis tool.
System Design Features and Programming Guidance
The Pyxis ASIC is Recogni’s inference accelerator that enables best-in-class Total Cost of Ownership (TCO) for generative AI workloads.To achieve this, we take advantage of certain key design features.
Our Pareto math system enables low power and high density.
Our math system allows us to build extremely low power and dense silicon and systems by innovating in our proprietary log quantization system, compression, and conversion techniques. Throughout Pyxis, each mathematical calculation is performed with a precision and representation that has been carefully chosen to balance accuracy with power consumption. When models are prepared for use, it is the responsibility of the conversion process to determine an appropriate level of accuracy for each constituent operation. 
The fabric connection between Pyxis MCMs is symmetric.
We gain another important advantage with our general any-to-any connectivity of Pyxis devices within a chassis, enabled by a 112Gig SerDes fabric interconnect. This interconnect enables us to rapidly move large amounts of data between Pyxis devices to produce different types of parallel execution of very large language models. For a primer on the types of parallelization techniques (some of which isn’t directly relevant for us), see the HuggingFace parallelization documentation. See our brownbag about LLM deployment challenges for another introduction to the topic.
All Pyxises are equivalent and interchangeable.
Because the communications fabric is symmetric, and because all Pyxis MCMs are the same, any algorithm or complication should be independent of which particular Pyxis it runs on. There is no runtime difference between combinations of Pyxises, no matter which ILC they reside upon. Compilation generates instructions for unspecified–sometimes called “virtual”--Pyxis devices. Linking compiled Pyxis devices with physical devices in the chassis or pod is one of the final steps before execution.
Avoid using off-Pyxis resources during graph execution.
Communication from the Pyxis to the line card CPU must flow through a relatively narrow, PCIe v3 x8 connection. The line card CPU itself is low-powered and expected to be busy with non-compute tasks. Alternatively, the fabric network between Pyxises is extremely fast. Pyxis itself contains a small number of CPU cores, but each Pyxis contains 6144 ALUs within its VPU capable of, admittedly, limited calculation.
Minimize scheduling during graph execution.
As much as possible, we intend to build silicon and systems that can be scheduled ahead of time, and run with minimal CPU intervention. Pyxis has a complex system of descriptor queues and barriers that can independently execute lengthy sequences of operations. In normal operation, Pyxis shouldn’t need to calculate scheduling at runtime. When cases arrive that require such intervention, such as MoE, the CPU impact should be minimized to avoid having idle compute hardware.
User Stories
The final user of a Pyxis system will be making a request to perform some calculation. For LLM requests, a description of the process can be found here as a Notion page, and here as a presentation. A more detailed description of LLM math can be found here.


TODO: How to prepare a model.[g]


TODO: How to start up and shut down a system.[h]
Tool Packaging
Recogni SDK
Several of the software components are combined into a package generally called the Recogni SDK, although it is sometimes referred to as the AI SDK, the Recogni AI SDK, or the SDK for AI Application Developers. It is composed of, more or less, all of the offline components of the Model Preparation Software. As defined in the PRD, the goal of the Recogni SDK is for AI application developers to quickly deploy their trained models.


The Conversion SDK acts as the primary developer interface to the Recogni SDK. Beyond conversion tools, it also includes access to the Rock compiler through a Python API, as well as the ability to perform bit-accurate simulation of graph computations and tools to analyze them. It can also import and export Lunarite files for connection to other tools.
Customer Model Analysis Tool (CMAT or CMAT Pro)
For pre-product engagements with potential customers and strategic partners, a subset of the Recogni SDK is available as the Customer Model Analysis Tool (CMAT). The complete version of CMAT, sometimes referred to as CMAT Pro, is centered around the Conversion SDK. It also includes the Proxy Profiler. By using these two tools, a customer can determine both accuracy and performance of Pyxis on their models. When the compiler is ready, it will also be included so as to provide its own, more accurate performance information.
CMAT Lite
Before CMAT Pro is ready, we’ll provide CMAT Lite, a static web page containing pre-computed accuracy and performance statistics for key graphs and system characteristics.
System Software
This section covers all Recogni provided software, firmware, and drivers that deploy to the Pyxis inference chassis. 


TODO: I am making a table for now, we will expand upon this in the near future.




Component
	Where does it run?
	What does it run (on)?
	Notes
	Pyxis Firmware
	Pyxis CPU cluster (SiFive p670 cores)
	We will run a Linux-like operating system
	Need to ensure that N cores can be dedicated to compiler / runtime executors. Responsible also for any fabric side maintenance and memory init / setup
	Management OS
	MC
	64 bit Intel (unknown?) CPU cluster, JunOS
	JunOS team will give us a binary release (initially) of this OS that we can layer software on top of. This will contain a customization for the inference line card which covers items like power budget, temperature, bootup order etc etc
	Inference OS
	Recogni ILC
	64 bit AMD SnowyOwl, JunOS*
	Working out if this will run a Recogni kernel with JNPR modules, or vice versa. TBD*
	ILC PCI switch
	Recogni ILC
	Driver runs on the ILC CPU
	

	ILC NIC / TOE
	Recogni ILC
	Driver runs on the ILC CPU
	

	Inference Service [N]
	Recogni ILC
	App runs on ILC CPU
	Likely will need at-least one Pyxis from the first TP group on the same ILC
	Health Service
	MC
	App runs on MC CPU?
	This could end up being a pub-sub-like protocol which removes the need for a service here. This may be able to move to the DC SW layer. TBD.
	TODO: 
   1. Describe various components: ILC CPU OS, MC OS, Pyxis FW
   2. Describe relevant kernel modules and connectivity
   3. Testing and verification plan
Datacenter Software 


Recogni’s software runs in a container runtime as opposed to a virtualization engine where we run an actual VM. The containers running Recogni’s software stack will be orchestrated with the industry standard for container orchestration Kubernetes.
Start of monitoring / cluster orchestration software
Kubernetes (K8s)
  



Figure 1: Kubernetes components required for orchestrating ML deployments on Recogni hardware. Colorless boxes depict standard Kubernetes components. Colored boxes represent components implemented by Recogni. The Kubernetes control plane is extended with a component called “Recogni Operator” which acts as a Kubernetes controller for the custom resource definitions “RecogniService” and “RecogniServiceReplica”. It translates the desired state of such Recogni services and replicas into Kubernetes Pods which run the Recogni inference server container and which have access to Pyxis accelerators (MCMs) and other non-Recogni K8s objects. Such a replica can consist of pods running on multiple line card nodes. The component “Recogni device plugin” running on each line card node discovers Pyxis accelerators, advertises them as resources to the line card node’s Kubelet, and monitors their health.
Kubernetes installation
Upon boot, the line cards and management cards either start built-in Kubernetes support, or the Kubernetes stack provided by Recogni[1] (TODO needs to be clarified[i][j]). Each line (and potentially management) card acts as a node in the Kubernetes cluster. A cluster can consist of multiple chassis.

The Kubernetes control plane components could run either 1) on the management cards in the chassis, 2) on dedicated general purpose control plane servers in the same network (as is the case in an Nvidia DGX system, see section 1.1 Hardware Overview) or 3) if only a single line card exists, on this line card itself. (TODO needs to be clarified[k][l])
Kubernetes device plugin
Recogni provides a Kubernetes device plugin which runs on every line card node and which is responsible for discovering Pyxis devices and advertising them to Kubernetes as requestable resources. The plugin is also responsible for health monitoring. Device plugins implement a gRPC interface which is called by the kubelet, the primary "node agent" that runs on each node, but there is no dictated interface for how a device plugin monitors device health[2].


Lastly, the device plugin could be made responsible for resetting Pyxis devices before making them available to a newly started container.
Kubernetes Custom Resource Definitions (CRD) and Kubernetes operator
Since large models will require more Pyxis accelerators than can be provided by a single line card, we require a concept of a “Recogni-specific distributed model deployment” that spans multiple line cards and involves multiple containers. The properties of such a distributed deployment, which accounts for the any-to-any connect property of our fabric, will be specified through a set of Kubernetes Custom Resource Definitions (CRDs) called Recogni Service and Recogni Service Replica. The life-cycle of these resources will be managed by a corresponding Kubernetes operator which acts as the controller for these CRDs. This object is called the Recogni Operator.
The Recogni Operator
The Recogni Operator is the K8s management controller for the following custom K8s resource definitions:


   * The Recogni Service (RecogniService) is an object that defines the full deployment of a model/an application on Inference One systems. It includes a configuration for the inference server, how many devices are used per replica, and how many replicas are required for load balancing requests.
   * The Recogni Service Replica (RecogniServiceReplica) represents a single instance of a deployed model. Each replica runs a set of K8s pods, one for each allocated pyxis chip. Each pod runs one container for the Inference Server but only the rank 0 Pod receives client requests. Replicas can use from 1 to 72/144 pyxis accelerators (depending on how many are connected to a single fabric). Replicas offer a great deal of deployment flexibility.


Recogni also uses a Kubernetes Service object tied to the RecogniService that controls load balancing for queries across all replicas. 


In a complete Recogni system deployment, the Kubernetes control plane remains exactly the same, with the exception of the Recogni Operator. As shown in the preceding image, it is a K8s custom controller that runs as an extension of the K8s control plane. It acts as a separate controller for all Recogni resources in the cluster. The Kubernetes API server defers to the Recogni Operator to manage the lifecycle for all Recogni objects in the cluster.


The Operator translates a RecogniService into the desired number of RecogniServiceReplicas, and translates each replica into a set of Kubernetes pods. The pods run containers on one or more line cards according to the scale of each replica. 


Due to the any-to-any connectivity of Pyxis devices, it doesn’t matter on which line cards the containers belonging to a single RecogniService replica run, as long as they are part of the same chassis. To give the orchestrator complete flexibility in spreading the containers across a chassis, the translation from a RecogniService to pods/containers has the following properties:


   * If a RecogniService specifies that per replica, n Pyxis devices are requested, the operator per replica creates an equal number n pods/containers, each of which request a single Pyxis.
   * For a RecogniService with a single replica requesting 36 Pyxis devices, this means that there will be 36 containers, each allocating a single Pyxis.
   * The only requirement the orchestrator needs to fulfill is that the pods/containers of a replica are allocated to line cards in the same chassis[3]. The containers (at least those that don’t run the inference server) shall be lightweight, potentially containing only a single binary (the so-called recogni-inference-server – see below).
The RecogniService
The custom resource RecogniService configuration consists of the following elements: 


Setting
	Description
	metadata
	Standard Kubernetes metadata fields including name, namespace, labels, and annotations for the user of the replica.
	spec


	Parent object that defines the attributes for the entire service.
	replicas
	Defines the number of replicas available to the service user.
	recogniReplicaSpec
	Object of parameters to configure the replica(s). 
	placementConfig
	Object with settings to control placement of the pods for a replica across linecards.
	devices
	Set the number of Pyxis MCMs available to the replica.
	serveConfig.modelManifestRef/modelManifestRef
	Configuration for the inference server
	

The RecogniService functions as a template from which you build the replicas. The following example illustrates:


apiVersion: recogni.com/v1alpha1
kind: RecogniService
metadata:
 name: jsmith-2
 namespace: models
spec:
 replicas: 1
 recogniReplicaSpec:
   storageConfig:
     storageApiEndpoint: https://storage.googleapis.com
     clientId: GOOG1EBACIHC4VQJOODY2NGSGXQTLNGO57G7K3MJ7B22TDHCGEHFXWKFPAMKO
     secretName: storage-backend
   placementConfig:
     compactPlacement: true
     leaderAntiAffinity: true
   devices: 3
   serveConfig:
     modelManifestRef: recogni-devops/pyxis-models/LLama3-70B/model_config.json


The code example specifies a recogni service with 1 replica and replica properties: a uri to get the inference server config from, required storage credentials, and a placement config (see below). While they’re not shown here, the YAML manifest also defines global settings for lifecycles and deployment in the cluster, and surrounding infrastructure including load balancing, networking, and container scaling.


Because large models can use resources beyond a single pod or a single Inference One line card, a RecogniServiceReplica can span multiple worker nodes and hence multiple ILCs. These resources are determined by the scale of the model deployment. 
Controlling replica placement
Because there is an any-to-any relationship between all Pyxis MCMs in an Inference One chassis, across all installed line cards, a replica can have all of its assigned MCMs located across any number of locations within the system. There isn’t a performance penalty for a more or less random distribution. However, for reliability reasons, you can group a replica's pods more closely together. If you're running a full Inference One system with a lot of widely scattered replicas, and a line card experiences problems, the incident can affect several replicas. Recogni supports the close grouping of pods to avoid this problem, through a Replica-wide placementConfig object that translates to a Kubernetes Pod Affinity which makes the K8s pods from a replica more likely to be assigned to the same nodes when possible. These settings are enabled by default.


The placementConfig parameters provide the following settings for control of replica placement in the Recogni K8s cluster. These settings don’t affect model performance.


Setting
	Description
	Default
	compactPlacement
	A flag to control ILC locations for all replicas. If true, this setting enables all replicas in the service to group all pods in the most compact placement, ideally a single ILC. It improves overall system reliability in the unlikely event of ILC failure, by limiting the number of replicas affected by an ILC outage.
	true
	leaderAntiAffinity


	A flag to control whether the leader pods of replicas (rank 0, which host the actual servers receiving client requests) have a tendency to occupy different line cards or not. This flag is used to distribute network traffic over multiple line cards instead of focusing it onto a single line card with the goal of reducing network bandwidth and PCIe bandwidth requirements.
	true
	The RecogniServiceReplica         
A model deployment can require pyxis resources on multiple line card nodes. This is enabled  through the RecogniServiceReplica.


The replica consists of one pod per allocated pyxis in the replica. Each of the pods/pyxis devices is assigned a so-called rank, a number between 0 and the number of accelerators - 1. 


The pod with rank 0 executes the inference server which receives client requests. The pods with rank > 0 are used 1) during the model loading phase to load the respective shards of the model and 2) they register their pyxis device with the rank 0 pod (leader), see below.
Metrics and monitoring
Each rank 0 pod executes an inference server. The inference server produces OpenTelemetry metrics that you can read collect e.g. using the off-the-shelf OpenTelemetry Collector and visualize using off-the-shelf monitoring software such as Prometheus/Grafana. You can use your preferred monitoring tool.
Start of inference software
The pods belonging to a Recogni Service execute a binary called recogni-inference-server (likely to be renamed).


This binary has the following responsibilities:
   * Coordinates the inference process between multiple line cards, i.e. establishes a mapping from virtual to physical Pyxis ids (see below)
   * Model/rock shard downloading
   * Run an inference server receiving client requests. Only rank=0/leader pods actually receive client requests.[m]
   * Use a component called Recogni Device Manager (RDM) to communicate with pyxis devices.
  
[n]
Translation of virtual to physical Pyxis ids
When compiling a model with the Rock, we only know how many Pyxis devices are available at runtime (n) and what their virtual ids (0 to n-1) are. We do not yet know which physical devices get allocated by the orchestrator. This allows us to scale or move a deployment to different line cards/chassis without having to recompile the model.


However, to actually perform RDMAs, at runtime the shard needs to know which virtual Pyxis id corresponds to which physical device that’s allocated by the orchestrator.


The recogni-inference-server takes the responsibility of establishing this mapping: In a service replica requesting n Pyxis devices, n pods are started, each running the recogni-inference-server process and each requesting one Pyxis. Each pod is configured with a rank/virtual Pyxis id ranging from 0 to n-1. The rank 0 recogni-inference-server runs an http server (not the inference server) to which the rank 1 to n-1 recogni-inference-servers register their virtual and physical Pyxis id. The rank 0 recogni-inference-server passes the resulting virtual-physical Pyxis id mapping table to the RDM.
Inference server
In the case of LLM serving, the rank 0 pod, will run a language model inference server which performs (among others) the following tasks:


   * Downloads model shards produced by the compiler
   * Runs a web server which receives requests with user prompts
   * Queues requests and continuously forms batches of prompts in pre-fill and generation mode
   * Passes batches to the RDM
   * Performs sampling of the next token, potentially beam search, etc.
   * Streams back generated tokens to clients via server-sent-events
   * Potentially loads LoRas/Adapters on the fly according to user requests


For this inference server, we took inspiration from popular inference servers like vllm and TensorRT-llm. Since the tooling landscape is evolving rapidly, new relevant projects might be published as well.


Model Preparation Software
Conversion SDK
Online conversion SDK documentation can be found here.
The ROCK Compiler
Online compiler documentation can be found here. Also see the following subsections.
Compilation pipeline
The figure below illustrates the typical compilation pipeline:
     1. The ML engineer has a model that they’d like to compile for a Pyxis system. There are three levels of customizability to achieve this:
   1. Direct conversion from the ML framework of choice to lunarite
   2. Customization and Legalization overriding.
   3. Whole program reimplementation in Lunarite.
   2. The Model along with configuration flags (eg. Sharding strategy, optimization config, hard constraints, machine config, etc …) gets sent to the compiler after being converted to Lunarite.
   1. This is facilitated by the base Lunarite package along with feature expansions implemented within the SDK which shall wrap Lunarite.
   3. Lunarite is Recogni’s own IR infrastructure that serves as a serialization/decentralization of ROCK IR.
   1. Lunarite is a means of exposing the compiler IR to Python through a thin Lua serialization layer.
   2. The Library is embedded in a Python package that allows graph construction, manipulation, serialization and export of a neural network.
   3. The SDK shall depend on Lunarite, and use it to implement legalization and conversion of PyTorch Aten graphs.
   4. The same (meaning point c) can be done for any other ML framework
   4. Once exported, the graph (expressed in Lua via Lunarite) can be passed to the compiler, it can also be passed to Lunarite-runner to be run on the CPU and/or GPU (for bit-accuracy testing).
   1. The compiler may break compilation after any number of passes, and generate the latest state of the graph in Lunarite to allow inspection, and Lunarite-runner testing.
   5. The compilation finishes with generating a REX file that contains:
   1. A metadata json file, that contains the deployment configuration
   2. Shard.lua scripts that shall be run by LuaJIT on the Pyxis internal RISC V CPUS to schedule and run the model. Each of the scripts run on a core managing each cluster.
   3. A copy of the LuaJIT compiler that embeds a C++ layer exposing the memory, interrupts, and other low level APIs necessary to control the DMAs, UIEs, etc..


ROCK Compiler Functionality
This subsection focuses on a fairly high-level exploration of the ROCK compiler. 


When you use the ROCK compiler to convert a model to Recogni Inference One, ROCK caches the entire compiled graph in memory and re-uses it for all future inputs during the compile job.


The goal of the ROCK compiler is to make the Recogni Inference One hardware programmable for all users. It enables users to have complete programmable access to the full performance of Recogni hardware from Python, without using other languages.


The ROCK compiler produces a binary file that runs as a deployment artifact on Recogni hardware. As a result, the fully converted model runs its inference tasks on Recogni's Universal Inference Engines (UIEs), which we also refer to as the Pyxis MCMs.


The end result of ROCK compilation allows running the optimized model on the Recogni Inference One system, and to submit queries to the model as you would in a normal LLM prompting scenario.


The Recogni SDK provides the Python user interface to program the hardware in the Recogni system. It uses an enabling feature called Lunarite to program the Recogni Inference One through Python.
Introducing Lunarite
Lunarite is a Recogni-specific scripting language that is based on the Lua scripting standard. Lunarite is a domain-specific language (DSL) that's interpreted and compiled by ROCK. Recogni defines Lunarite as a mixed-mode programming language for expressing and exporting graph neural networks to the Recogni Inference One. It supports the following features:


   * Express high-level model operations in a functional manner;
   * Offer programmatic access to low-level instructions that target Recogni compute modules, including Vector Processing Units (VPU) and Grid;
   * Supervise transformation passes on the graph during compilations;
   * Apply a diverse battery of compiler and AI model optimizations to further boost model performance;
   * Meta programming;
   * Control Flow (ordering of instructions);
   * Sharding control settings;
   * Symbolics and dynamic shape capabilities.


As a programming interface, Lunarite builds upon Lua scripting to define all of its syntax and rules. Lunarite produces a high-performance Lua library that uses ROCK for the heavy lifting during the compile job. Working through the ROCK compiler, Lunarite enables users' neural network Python programs to directly address the Recogni Inference One hardware.


Developers that use the ROCK compiler don't interact with or write Lua code for their projects. Lua effectively functions as a "transport" layer between your Python code and the C++ code that ROCK produces. You keep your standard Python code as the means to develop your model, and the graph that you submit to the compiler. Lunarite serializes and de-serializes the graph in ROCK to produce a complete representation of the model. It also enables C++ to build the machine-language, optimized version of the model to leverage the performance of Recogni's Inference One.
Obtain and Install ROCK
You'll need the following utilities in your Linux or Unix system to build the ROCK compiler:


- llvm-18
- cmake-3.30 or newer
- make-4.3 or newer
- uv-0.2.36 or newer
- git
- curl
- patch
- diffutils
- luarocks
- stylua
Install on MacOS
You use the Homebrew package manager brew install directive to install the Unix prerequisites for building and running the ROCK compiler on MacOS. Do the following:


1) If you don't have Homebrew installed, open a terminal window and enter:


    /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/
install.sh)


2) Install all necessary requirements through brew:


    brew install cmake uv gnu-sed make grep llvm luarocks stylua


3) Add the environment variable to your PATH statement:


    export PATH="/opt/homebrew/opt/gnu-sed/libexec/gnubin:/opt/homebrew/
    opt/make/libexec/gnubin:/opt/homebrew/opt/grep/libexec/gnubin:/opt/
    homebrew/opt/llvm/bin:$PATH"
Install on Linux Flavors
Take the following steps to perform a Recogni ROCK installation on Linux.


1) Enter the following commands to install on Linux (this example supports Ubuntu 20.04 LTS):


     sudo apt update
    sudo apt install -y --no-install-recommends cmake make git curl patch 
    diffutils luarocks


2) Install the UV package installer/resolver:


    curl -LsSf https://astral.sh/uv/install.sh | sh


3) Install the LLVM infrastructure for the compiler:


    bash -c "$(wget -O - https://apt.llvm.org/llvm.sh)"
    wget https://apt.llvm.org/llvm.sh
    chmod +x llvm.sh
    sudo ./llvm.sh 18


4) Install Stylua:


    curl -LsSf "https://github.com/JohnnyMorganz/StyLua/releases/download/
    v0.20.0/stylua-linux-$(arch | sed 's/arm64/aarch64/g').zip" | funzip > 
    /usr/bin/stylua sudo chmod a+x /usr/bin/stylua
How ROCK Works
You can use ROCK in two different ways:


   * Use your own graph capture process to capture your model into a graph, and then convert that Aten/Torch IR graph into a legalized graph that's usable by the ROCK compiler;
   * Using the ROCK compiler, directly import a Torch IR graph, skip the ML Framework step and start building the model graph directly into the legalized Recogni format (ROCK IR).


In either case, the compiled graph is optimized, lowered to a binary executable, and used to program the Recogni hardware.
Lunarite ROCK IR Example
NOTE: ROCK is a retargetable compiler, which means that it can be quickly redeveloped to support different processor instruction sets and architectures.


Lunarite automatically produces a Python interface from the ROCK source code. This interface builds the legalized neural network graph into a ROCK-compatible IR without user intervention. The following code illustrates the building of a few operations:


# -------------------------------------------------
# This code is automatically generated. DO NOT EDIT
# -------------------------------------------------
from typing import Any, Optional


from lunarite,ir import Graph, Op, Process, Type


class Builder(Graph) :
  def absolute(self, x: Process, output_type: Optional [Type] = None) -> Process:
  attrs = (
  ор = 0p.make(args=(x,), attrs=attrs, name="Absolute")
  return self.insert(op=op, output_type=output_type)
  def add(self, x: Process, y: Process, high_prec: bool, output_type: Optional(Type] = None) -> Process:
  attrs = ("HighPrec": high_prec}
  op = 0p.make(args=(x, y), attrs=attrs, name="Add" )
  return self.insert (op=op, output_type=output_type)
  def broadcast(self, x: Process, axis: int, axis_size: int, output_type: Optional[Type] = None) →> Process:
  attrs = ("Axis": axis, "AxisSize": axis_size}
  op = 0p-make(args=(x,), attrs=attrs, name="Broadcast")
  return self.insert(op=op, output_typesoutput_type)


The legalized graph remains the source of truth for all network operations. Any attribute and data type changes in the graph also propagate into Lua and Python during subsequent code refreshes, so nothing gets out of sync.
Stages of the ROCK Compiler
The ROCK compiler uses the following high-level stages in order:


   * ML Framework (optional) - The compiler captures a Python model and converts it to the Aten graph format, as a new Torch IR. The ROCK compiler reads and imports the graph for further processing. This stage is optional and won't apply for some Recogni use cases;


   * Core - The Core compiler stage performs the initial passes on the ROCK IR, focusing on the passes that it finds to be numerically variant or inconsistent. It applies EB Init and data type selection to the ROCK IR, and builds the graph. It then forwards the revised ROCK IR to the next stage for optimization and conversion to Recogni numerical compatibility;


   * Backend  - The final compilation stage. This stage includes an extensive battery of optimizations, which execute after the IR is numerically consistent. When it's finished with the compiler optimizations, it "lowers" the ROCK IR into machine code, sets the necessary hardware flags and performs barrier management and memory management.
The ML Framework Stage
NOTE: Advanced ROCK compiler use cases don't require the ML Framework stage as part of a graph compilation job. In the SDK, you'll apply sample ROCK jobs with and without this stage.


The ML Framework stage accepts an existing PyTorch graph and produces a new Aten graph IR for analysis and further compilation. It represents and combines all the Tensor computations of the model as an intermediate representation (IR). You use that graph IR for compilers and other output engines. In our case, the graph functions as a bridge from friendlier Python code to Recogni's optimizing low-level code.


Graph conversion is a widely used process through PyTorch and other model frameworks. Consult your framework documentation for more information about conversion to Aten graph format.


At this end of this stage, the graph is still hardware-independent.
The ROCK Core Stage
In this stage, the ROCK compilation ensures that the model is executable on Recogni hardware.


The Core stage legalizes the torch.fx.Graph for consumption by the ROCK compiler. What is legalization? The Core stage uses a serialization layer to convert the torch.fx.Graph to the new ROCK IR format. It's the first ROCK compilation step. External developers don't interact with or configure this action.


The Core stage then uses Lunarite as a transport between the Python graph and the C++ compiler layer. It transcribes what the developer wrote in Python into C++, applies key numeric operations in passes through the graph, and enables the altered graph C++ code to transport its information back to Python. In this way, the ROCK IR (which represents the original graph) undergoes a series of programmatic transformations in the compiler.
ROCK Core Stage Elements
The ROCK IR is a complete in-memory graph representation of a neural network model, translated from a high-level Torch IR into a new intermediate representation. Using ROCK compilation jobs, the Core stage applies the following calculations to the graph in the ROCK IR:


   * EB Init
   * Data Typing


During the core stage, the compiler job processes and produces the following graph objects:


Operation
	Network computations to perform simple math tasks and complex tasks like convolutions and matrix multiplication. In a ROCK IR, operations form part of every process, but they aren't treated as separate objects in ROCK compilation jobs.
	Process
	A process encapsulates multiple NN nodes into a single object. The operations that a process contains run in order during the compilation job. Processes are the minimum unit of execution in the ROCK compiler.
	Subgraph
	A superset of a Process object. Subgraphs enable the NN developer to encapsulate multiple processes into a single entity for larger-scale compiler tasks in the model conversion. Subgraphs support several important compiler use cases:
   * Group processes for tasks such as graph scheduling;
   * Partitioning, or sharding, a neural network graph for model parallelism across multiple UIEs.
	Composite operations
	A "nested" operation in which multiple layers of interconnected operations apply a transformation to each of their inputs. The output of one operation becomes the input of the next.
	Dynamic shapes
	AI model capability to produce tensors that contain different sequence lengths, such as sentences of different word counts. Other factors can determine dynamic shapes, such as batch sizes.
	The ROCK Intermediate Representation (IR)
After the initial ROCK IR conversion, the Core stage pushes the converted graph through the data typing and EB Init processes as previously noted. EB Init also performs the following tasks in sequence for graph preparation to the compiler:


   1. Statistics extraction: This task extracts the absolute minimum and maximum values of the tensors in each layer for the graph.
   2. Mixed precision: Based on the layer statistics, ROCK detects the outlier layers with the greatest magnitudes and sets them to use 16-bit data types. All remaining layers keep their original 8-bit data type.
   3. EB Initialization: This task configures the Exponent Bias (EB) of the data types for each group of 16-bit layers, and for each group of 8-bit layers. The Bias setting is based upon the minimum and maximum magnitude values that the compiler detects in each layer group.


  

NOTE: For more details, see this Recogni AI SDK doc about how EB Init works in the Recogni system.


EB Init checks each of the converted graph's machine learning layers to find those that contain outlier values beyond the stated magnitude. After EB Init and data typing tasks resolve the consistency and coverage for all operation numerics, the Core stage pushes the processed ROCK IR to the Backend stage.
The ROCK Backend Stage
The ROCK compiler backend stage lowers the optimized ROCK IR into a binary file that supports model execution on Recogni Universal Inference Engines (UIEs). To do this, it applies a series of optimizations to the ROCK IR to prepare it for lowering to the binary executable. Compilation finishes with a Recogni Executable (REX) file that also contains the support files for the Recogni model conversion:


   * A metadata JSON file that contains the deployment configuration.
   * A set of shard.lua scripts that execute on the Pyxis' internal RISC V CPU. The CPU uses an embedded, lightweight LuaJIT compiler to schedule and run the model. Each of the scripts executes on a CPU core that manages each cluster.
   * A copy of the LuaJIT compiler. It embeds the C++ code to expose memory, interrupts, and other low level APIs for control of DMAs, UIEs, and other Recogni hardware elements.


The end result of the compile job is a binary artifact that runs directly on the Recogni Inference One system.


NOTE: You use an API call through the Recogni SDK to submit the completed graph to the Recogni hardware.
ROCK Compiler Backend Optimization Passes (INTERNAL ONLY)
NOTE: Optimization Passes described in this section are under development and this information is subject to change.


After the Lunarite graph transitions to the Backend stage for reduction to machine code, the ROCK compiler subjects the neural network graph to a series of optimizations that cumulatively transform the network. The following diagram provides a simple example of how optimization forward passes work.
  

The image shows a graph that undergoes two different passes. The graph representation changes as each optimization pass completes during the ROCK job. The optimizations reduce and remove ML decision errors, and improve inference performance. Note that in this example, the numbers of edges and vertices both get reduced in the network.


Some semantic ROCK optimizations are common to graph compilers for TensorFlow, PyTorch, Caffe, and other frameworks. All Recogni optimizations act on the entire in-memory graph codebase. Applied by the ROCK compiler, optimizations include the following, though this is subject to change:


   * Op folding: ROCK-specific optimization to "fold" various repetitive operations to speed performance of some graph processes. Examples: if 2 Pad instructions follow each other, you can fold them into 1 Pad instruction. You can also fold constants, so if you have a simple expression like 1+2, the compiler folds it to a constant value 3.
   * Dead code elimination: Removes code in the graph representation that doesn't affect model results, so it can produce a faster and smaller executable.
   * Common subexpression elimination: Searches for identical expressions that produce the same value and determine if it can replace those expressions with a single variable.
   * Transitive reduction: A graph theory concept in which the compiled graph keeps the full set of vertices but reduces its edge count to the fewest necessary, while keeping reachability and transitive closure.
   * Slice alignment: Slice alignment is necessary for left slices that aren't aligned to a multiple of the PartitionWidth. It does so by shifting the tensor to the left, then takes a slice with Left = 0. Embed operations unconditionally fire new Slice Alignment passes. To protect Embed efficiency, the pass does the alignment with the simplest possible method.
   * Shift desugaring: IN DEVELOPMENT Program manipulation that "desugars" simplified coding language syntax by expanding simplified "sugared" constructs into their more verbose equivalents. 
   * Shift tree construction: A compiler code-parsing strategy where the parser builds a parse tree by progressively reading input tokens, shifts them onto a stack, and applies grammar rules to "reduce" groups of tokens into higher-level non-terminals, so it constructs the parse tree from the leaves upwards to the root.
   * Trip planning: IN DEVELOPMENT Trip planning involves consolidation of operation patterns and their surrounding infrastructure operations. Consider a MatMul operation, followed by a SwiGLU operation:


    AMEM Read -> MatMul -> AMEM Write and then AMEM Read -> SwiGLU -> AMEM Write


Trip planning eliminates the intermediate Write and Read operations:


  AMEM Read -> Mat Mul -> SwiGLU -> AMEM Write


   * Pad slice desugaring: IN DEVELOPMENT
   * Embed Elimination: Compiler optimization to remove unnecessary embedded instructions in the graph code.
   * Graph Scheduling: Graph scheduling defines the execution order for all nodes in the graph.
Lunarite ROCK IR Analysis (INTERNAL ONLY)
After the ROCK compiler imports the legalized neural network graph, Lunarite analyzes the ROCK IR and extracts all neural network Python operations into C++ code, and into separate Lua code. It does this without user intervention. The following code block illustrates a conv2d operation that Lunarite converts from the native C++ into Lua:


Conv2D = opBuilder {
  name = "Conv2D",
  id = 474250937,
  target = "generic",
  argsCount = 2,
  attributes = {
    Padding = {
      Type = "list[tuple[int, int]]"
      Optional = false,
    },
    Stride = {
      Type = "int",
      Optional = false
    },
    Dilation = {
      Type = "int"
    Optional = false,
    },


This code is compiled without Lua being visible to the user during the job.


When Lunarite completes this process for the entire graph, a new lunarite.Graph entity exists in the project. The lunarite.Runner module exercises that new graph during all compilation runs. lunarite.Runner iterates new ROCK IR builds for each graph update, and subjects the graph to eb_init and ROCK optimization passes. It produces these graph builds with PyTorch and Recogni emulation. For every ROCK operation, lunarite.Runner provides equivalents for fast, float, and emulated execution mode. (Emulated mode takes place through bindings to the recogni.torch_emu module.)


lunarite.Runner allows you to run the complete graph after any compilation pass. It can execute any Lunarite graph on non-Recogni CPUs and GPUs. It also fully supports the Recogni Pareto AI Math computations and standard floating-point. You'll always be able to debug, develop, and verify correctness of any ROCK and SDK pass.


  

Lunarite Language Reference
As previously noted, Lunarite is based on the Lua scripting language. Lunarite is designed as an extensible language for any AI application or neural network architecture. Lunarite provides features to allow higher-level integration, including implementation of standard libraries and utilities. You can load it into an object model, programmatically change it, and dump it back to code without the need for a parser or AST.


ROCK/Lunarite also fits some of the elements of many deep learning compilers. It uses some standard compiler optimizations and reflects the basic Frontend-Backend architecture. One significant difference is that [all software and hardware optimizations](#backend-optimizations) take place in the ROCK compiler Backend stage.


To fully understand Lunarite, you need to understand the data structures that it works with:
  



   * Decorator: The wrapper function for the graph.
   * Graph: A collection of nodes connected by edges, that comprise the representation of a neural network. Lunarite represents graphs through Lua functions in serialized form, or through graph objects as the in-memory representation. The graph acts as a container and possesses none of its own properties.
   * Node: A Node is a single object inside a given graph. Nodes are just a notion as far as Lua and Python are concerned. In C++, nodes are a true object. A node in C++ is a wrapper object that can have one of two types of contents: a graph, or a Process. This enables graphs to contain subgraphs, and processes, in a strongly typed manner. Nodes that contain a graph or a subgraph also contain the decorator information about the graph.
   * Operation: Operations manipulate model data. As an object, operations have two separate definitions:
   * ROCK IR (C++): an Operation is a class that composes and combines an Instruction and a Geometry. An Instruction defines the semantics of the operation, and expresses the semantics to the target hardware. A Geometry (also see below) calculates the dependencies between the input and output on the element level (i.e. the pixel or matrix element). RockIR operations are always nullary, unary or binary. They don't accept variadic numbers of arguments. You can also configure operations with alternatives. Alternatives are Instruction/Geometry pairs that have the same semantics but can execute differently on Recogni hardware. They enable the compiler to choose the best alternative during optimization passes.
   * Lunarite: Operations are predefined (collectively called OpSpec or just Spec). ROCK generates them during build time. The operations are configurable with attributes and can accept a certain number of arguments. (The OpSpec defines the attributes and the allowed number of arguments.) An operation operates on zero, one or more tensors (variadic), and produces a tensor. For example, you can call a Convolution operator on a tensor Input with the configuration Kernel Stride = 2: Conv {Stride = 2}(Input, Weight).</li></ul>


   * Process: A Process is a sequence of operations to execute in one or more Trips. Think of a process as a "Trip Template", in which all the code it describes is instantiated once per trip (with multiple trips scheduled based on the tiling needs of the input and output tensors). A process is the leaf level node in a graph. It notionally describes an expression. It may accept zero, one, or more arguments, but produces exactly one output. Tensors are the inputs and outputs of a Process.
   * Geometries: Geometries exist only in RockIR and don't exist in Lunarite. Geometries are a core concept in the Recogni system and play an essential role for dealing with tensor shapes. A geometry is simply a calculator. It has three main functions that you must implement in your ROCK jobs:
   * TensorShape shape(ShapePair InputShapes): The TensorShape function receives input tensor shapes, if those apply (you can have undefined Shapes depending on the rarity of the operation). From there, it calculates the output shape of the operation that the shape represents. For example, the output shape of a nearest neighbor upsample is twice the input shape.
   * ElementDependency requiredInput(ShapePair InputShapes, Slice OutputSlice):      This function calculates the slice in the input tensor required to produce the OutputSlice. In convolutions, this is sometimes called the receptive field. Since the ROCK's geometry calculation is applied not just to convolutions, we use the more general and purposefully vague term "Element Dependency" in the ROCK.
   * ElementDependency producibleOutput(ShapePair InputShapes, SlicePair InputSlices): This function is the exact opposite of the previous one listed. Given InputSlices of the input tensor, it calculates the producible output slice.


These three geometric functions enable the ROCK compiler to calculate data dependencies between tensors. They also enable tiling, sharding, bufferizing, type deduction & shape inference, and other core ROCK features. Geometries are purely mathematical and don't depend on the hardware. They are a bit more complicated than what we describe above.
Data flow simulator: Racer [o][p][q][r][s]
Racer is the name of the tool that we’re building to perform data flow simulation, enabling the creation of objects with the Hardware Description below. Its simulation engine is composed of three primary abstractions:
   1. A fixed hardware description, listing blocks and the FIFO-like connections between them.
   2. A set of tasks that can be performed on each block. Each task precisely describes the timing of how a block processes data received on its inputs and sent to its outputs.
   3. A multi-threaded program, describing the chain of tasks necessary to execute an entire network graph.
Racer begins as a Python library composed of a set of objects that implement the abstractions above. 
2004-3-1 The ABCs of Racer Introduction
Goals[t]
   1. Create a modeling system that can be used to experiment with various design choices, taking into account HW arrangements and details like latency, throughput, and bandwidth.
   1. At a system level, model chassis connections and fabric latency and bandwidth.
   2. Enable experiments for system usage:
   1. SoTA LLM model parsing, to define memory requirements.
   2. Try out different sharding and communications strategies, e.g. weight vs data stationary sharding, or reduce_scatter + all gather vs all reduce.
   3. Varying numbers and sizes of KV-Cache instances.
   4. Dynamic batching vs variable sized prompts.
   3. Enable the HW team to confirm and close a few key constraints on device shape and sizes, and how they interact with changes in programming:
   1. Grid size
   2. SRAM sizing and bandwidth
   3. Number of ALUs
   4. Effects of (de)compression on HBM3 bandwidth.
   5. Effects of outlier (FP8 vs FP16) math in different rows of LLC matrices.
   6. Different dataflows between different clusters.
   7. Deal with the possibility that different ALU instructions may have different latency, and that ALU programs may need to be modified to compensate or fill in holes.
   4. Make sure that the UX for the HW team is reasonable:
   1. Defining and modifying hardware configuration should not require arcane scripting abilities.
   2. Include easy connection to visualization tools, like NVIDIA’s visual profiler or similar visualization tools.
Roadmap
An early prototype was created in early 2023-12, able to run a simple 4-block model of a UIE executing a transformer layer. Future work needs to cover three different paths:
   1. Racer’s simulation core code development.
   2. Pyxis hardware model development, written within Racer’s framework (blocks and tasks).
   3. Lowering LLMs and other networks onto Racer/Pyxis-specific tasks.


Development on all three components will be linked, but hopefully can progress as a team.


Milestones include:
   * 2024-2-29 : 
   * Racer’s simulation core: Clean up the prototype, fix obvious architectural bugs, and implement a few more basic tools.
   * Pyxis hardware model: Complete for a single Pyxis without communication protocols.
   * Lowering: One or two LLMs inefficiently lowered (no fancy memory layouts, FP formats, sharding, etc.) onto a Racer/Pyxis model.
   * 2024-3-31:
   * Racer’s simulation core: Fill in the details of gathering complex statistics, allow for simulation of communication protocols, begin C++ performance improvements.
   * Pyxis hardware model: Complete pod simulation with fabric, PCIe, and ethernet connections.
   * Lowering: More general lowering capabilities with options skipped before. Use of an external framework (like XLA) with Racer as a backend.
   * Q2 ‘24:
   * Actual use as a tool for confirming HW architectural decisions.
   * Actual use as a tool to experiment with sharding strategies, batching strategies, etc.
   * Serve as an early reference for hardware performance models in the compiler or other systems.
   * Q3 ‘24 and beyond:
   * Continue to be a reference for various development experiments.
   * Possibly connect as a compiler back end.
   * Possibly available as a later-generation CMAT component. Possibly not.


More Detailed Motivation and Description
The final implementation of the Pyxis description in RTL will be cycle-accurate. Pyxis will be broken down into many individual block models, each of which describes all the details of what’s going to happen on a cycle-by-cycle basis. In the end, that’ll be an exact timing description of everything that happens… but that will have too much detail for most experiments.


A simpler system can be built by just considering the layout of the system, block-by-block. Instead of providing a model like the RTL model that is cycle-accurate, we can build a model that describes the timing of whole blocks in more rough terms. For every block, we need a few characteristics:


   * Latency–how long it takes for a particular piece (or pieces) of data to pass through the block from input and appear at the output.


   * Throughput–how much additional time is added to compute each piece of data passing through.
The difference between latency and throughput is subtle, but important. For example, a 128x128 matrix multiplier might have a 128 cycle latency, in that it takes 128 cycles from when the first piece of data is given to the input before the first data appears at the output. If the individual matrix elements are stored in one byte, the throughput might be one cycle/multiply, but if the individual matrix elements are stored in two bytes, the throughput might be two cycles/multiply, as the data takes extra time to pass through the system.

Timing of data passing through a block follows the formula 
Where the  accounts for latency due to any number of variables including setup time.  is the time of processing and is dependent on the number of operations.  is optional and is there mostly for symmetry and completion's sake. While it’s thought to be zero most of the time, It could actually have a value greater than zero if the block has to tear down resources or its state before it can accept a new block.

In the compiler, the Geometries are responsible for calculating the number of ops depending on the shape of the input and calculated shape of the output. Adding the latency preceding the operations is a matter of asking the hardware model for the number potentially computed via a proxy function. (More on that in the Compiler section)


      * Choice of task–most blocks can perform more than one task. Different tasks will cause the block to have different behaviors, e.g. the block that performs matrix multiplication may also perform convolution. Different tasks may change the latency or throughput values, e.g. an ALU block may have a latency of 4 cycles for a table lookup, but only 2 cycles for an addition. 

The choice of task will change during runtime. That’s how the program is executed.

There may also be latency (or possibly throughput) implications to changing the task. Doing so may require it to insert cycles of latency, or possibly even drain its contents completely.

Changing a block’s task may also include side data that needs to be loaded into the block before it can operate, e.g. a 4-bit -> 8-bit decompression block may need a table of 16 values to describe how decompression is accomplished. Loading that data may take time.


         * Input and output connections–every block has a set of inputs and outputs, on which the data for our model flows. We don’t care about the details of the data, but we will want to keep track of the width of the connections, as well as what they connect to. These connections are generally static; changes in which inputs and outputs are being used may be better modeled by task changes.

We want these connections to be named and easy to rearrange in the tool. We’ll have vast arrays of blocks with connections, and we’ll want to easily be able to describe how one block connects to another.

Further, the connections are key places for monitoring how data is flowing through the system. We’ll want to log, for every cycle, if possible, whether or not there’s data flowing through each connection. Looking at the activity of these connections will tell us where data is flowing, and where it’s getting stuck.

Inputs and outputs are also where we’ll deal with synchronization between different flows of data. A convolution can’t run until all its inputs (data from AMEM, configuration weights from elsewhere in AMEM) and outputs (an open path to the ALU) are available.


Systems of blocks and tasks are controlled through threads and barriers. Threads list the sequence of tasks necessary to perform some higher level operation, like a complete LLM run through. Barriers provide synchronization points between different threads.
Sample Racer   Output


Racer produces trace output, which can be viewed with the Perfetto tool. In general, trace output shows details of when particular tasks were running on particular blocks over the course of a test run. This particular output shows the result of a simulation of a single layer of a Llama LLM simulation, run on a single UIE cluster.

The bottom half of the screenshot shows the behavior of one of the UIE’s read datapath and write datapath blocks. The block for each unit shows the various tasks being run from top to bottom, and which tasks–if any–were running at any given time from left to right. Labels from within the simulation show what kinds of tasks were being performed at any given time. For example, the first UIE 0 read datapath operation was reading fp8 values from AMEM to the VPU, as part of performing the pass’ initial rmsnorm operation.


Racer also calculates utilization for each block, displaying the percentage of time in each task across the window and the total utilization percentage at the left. This screen capture has utilization calculations for all the blocks in UIE 0 displayed at the top.
Transactional chip models
The loosely-timed (LT) transaction level models (TLM) implements 2-phase blocking transactions b_transport. They are bit-accurate and sequence accurate (for data). They do not implement clocks, so they are NOT timing approximate / accurate, despite timing annotations to the transactions being possible for blocks with static delays.


IP-level models could be instantiated in one of two modes: 1) C++ mode; 2) SystemC TLM mode. The following diagram illustrates how target and initiator sockets are implemented. 


  
  

		Tracing and Profiling
Tracing involves recording specific information about neural network inference. In the case of the Pyxis cluster, this can include details such as per Pyxis “compute”, “data transfers”, and “wait for data” events.


Profiling is about measuring inference performance. It focuses on aspects like execution time, memory usage, and utilization.


Tracing and profiling serve distinct purposes but share the underlying infrastructure, thus it makes sense to develop them together.
Use Cases
Will mainly be used by compiler developers and model developers.
            * Understanding and debugging
Provide insights into the behavior of the graph execution, e.g. detect bottlenecks
            * Performance Optimization
Help to identify performance bottlenecks, e.g. contentions, congestions, load imbalances
            * Profiling
               * Time - how long computing takes
               * Memory - how much memory is used
               * Transactions - how much time is spent in data transfers
               * Utilization - how much each Pyxis device is used
               * Power consumption?[u][v][w]
Modes
Mode
	Pros
	Cons
	Streaming 
	               * No storage required
               * Simple decentralized implementation
               * Wireshark/perf-like tool to record tracing data
	               * Many small network transactions
	Buffering
	               * Less pressure on the network
               * Can work offline
	               * Requires SW to merge traces
               * Needs a memory buffer/storage
	

The current proposal is to use a combination of streaming and buffering mode. The tracing data will be buffered on each Pyxis device up to some size or time limit (whatever comes first) and then streamed. This will allow the pros and cons of both approaches to compensate each other.
HW Requirements
To precisely trace events, the time between different Pyxis devices must be synchronized. Juniper supports HW Precision Time Protocol (PTP) and we shall use it in Pyxis firmware. This allows timestamping events across Pyxis devices and enforces unified time.
        A 64-bit timestamping clock resolution is required to mitigate dealing with time overflow.
        Which communication channel will be used by tracing to transfer data is TBD. In the case of the network (fabric connection), we need a dedicated channel to reduce interference.
        OCPU time and a small amount of memory to buffer data will be needed.


Summary:
               * HW time synchronization between Pyxis devices[x][y]
               * 64-bit timestamping clock
               * A dedicated channel for debugging/tracing data transfers
Requirements
               * Tracing granularity control
               * Minimal performance impact
               * Tracing data management
               * Recording, retrieval, decoding, and storage of the tracing data
               * Tracing data visualization and analysis
               * Visualize tracing events for manual analysis
               * Automated analysis tools
               * Accuracy and Reliability
               * HW time synchronization between Pyxis devices for precise measurement of intra-Pyxis events (e.g. data transfer timestamping)
               * Lossless event recording
               * Extensibility
               * Allowing users to add reasonable (TBD) custom tracing points
               * User Interface and Documentation
               * Ease of Use: User-friendly interface for setting up and analyzing traces
               * Comprehensive Documentation: Detailed guides on how to use the tracer and interpret its outputs.
Appendix: Hardware Components
Recogni’s design concept defines a chassis that contains inference line cards (ILCs) that have up to 9 Pyxis devices each. We also estimate that we can build systems up to 16 ILCs to share a single fabric (chassis). At full scale, you can run models that can straddle up to 2.6TB of memory (9 x 16 x 144GB). We also plan to support smaller revisions of 8, and 4 ILCs for smaller deployments. However, most of this document, and most beta planning efforts, assume an initial bench-top deployment followed by a full scale 16-ILC deployment on the first beta chassis.
Chassis components
This section briefly describes the various hardware components that make up the Recogni inference chassis and outlines the rough connectivity using diagrams and figures that will be referenced throughout this document.


1. 2x Management  Cards (MC) - management portal, control channel portal.
2. Up to 16x Inference Line Cards (ILC), these carry Up to 9 Pyxis gen1 devices.
3. 6x Fabric cards which implement a vertically running back-plane
Figure 1: PTX 16 chassis  


  
Figure 2: Top of rack management card (MC)
  
Figure 3: Fabric and ILC interconnect
Pyxis ASIC
Number of clusters
	4 UIE clusters
	Engines per cluster
	12 UIEs
	On chip SRAM (AMEM)
	256MB (64MB per cluster)
	HBM3 memory size
	24GB or 36GB
	Number of HBMs per chip
	4 (1 HBM3 per cluster)
	Total HBM size
	96GB or 144GB 
	CPU architecture
	SiFive RISC-V P670  (Datasheet)
	Number of CPU cores
	16
	Interconnect bandwidth
	500GB/s, any-to-any device intra-chassis
	

Each Pyxis will have:
               * 4 UIE clusters, each of which has a dedicated HBM3 interface to a large capacity memory (24GB or 36GB). 
               * One 64MB SRAM (known as AMEM) per cluster. AMEM is the interface to each of the UIE clusters. Both temporary activations as well as layer parameters and KV cache will need to be available here. The layer parameters and KV cache are typically stored in AMEM and will be buffered in AMEM.
               * One Risc-V P670 CPU cluster with 16 cores.


The latest FSpec for Pyxis talks in detail about the clusters, engines, sequencers, data formats, programing model, and more. In addition, there is a dedicated Vector Processing Functional Spec that deals with the details of our VPU and its capabilities.


The specific software programing required for the Pyxis device can be broken up into two parts:
               1. Pyxis firmware / OS
               2. Pyxis inference execution


The Pyxis firmware running on the Risc-V CPU is fairly straightforward, and primarily exposes the Pyxis device and the relevant memories as a PCI endpoint device to be controlled by the ILC CPU.


The inference execution portion of the code that runs on Pyxis is the bit that drives all of our HW engines via their descriptors. This is a program or an executable (TBD) that will be generated by the compiler for a specific Pyxis device to be executed to keep the HW fed. Ideally, the only patching that needs to be done by this inference executor are the dynamic shape patching and address patching for any descriptors that are about to be in flight. 
Pyxis CPU Complex and Firmware
Each Pyxis MCM has one RISC-V P670 processor with 16 cores. The various Risc-V cores will be running some combination of Linux, RTOS and/or bare metal, exact combo TBD. (The x86 Linecard CPU runs a version of WindRiver linux, this may or may not be a consideration when selecting a linux version for Pyxis cpu.)
Preliminary requirements
               * Any Initialization and/or configuration that must be handled in SW. Possibly including:
               * Boot up
               * Memory initialization
               * Initializing Pyxis side of the fabric and links as required
               * Enabling chip to chip data transfers
               * Configuring and controlling the Risc-V cores designated to run lua ELF executables. These cores need to be configured such that they won’t be interrupted by scheduler or interrupts while the lua executable is running.
               * Fulfilling any requirements mandated by Junos/Chassis SW (eg health monitoring), if any.
               * Stats, debug info collection?
Boot Sequence
SiFive currently uses this default boot sequence on their development boards and all code is available and packaged as part of their SDK on github:  U-Boot-SPL ⇒ OpenSBI ⇒ U-Boot ⇒ Linux.  We are free to use any or all or none of this, it's just what SiFive is using (and provides) as part of their SDK.


The boot sequence for Pyxis is still TBD. There is no NIC or disk that will provide a linux image. Most likely an image (probably Linux or OpenSBI with Linux as payload) will be pushed into memory via PCI and the processor will start executing the image. Things will start coming into focus as our hw matures and we get more info from Juniper on what chassis resources are available to us (e.g storage access, etc)
Open Items
               * Requirements are not yet formed.
               * Assuming Linux OS until some reason not to use it arises. Linux is
               * Flexible enough to do almost anything we need
               * Can be made to reasonably small
               * Lets us configure quiescent cores using standard tools like cpu_sets, task & irq affinity, isolcpus, etc.
               * Familiar code base.
               * Boot sequence is TBD.
               * Bootloader? OpenSBI?
               * How many cores are needed for RDM? [4?  Rest for Linux and/or anything else]
               * Persistent storage? (Assume not)
Fabric Interconnect
Chip-to-chip transfers are defined as data that needs to be moved from one Pyxis device AMEM, or from one of the connected HBM memories to any other on-chassis Pyxis device. Note that sending data directly to an HBM could run into bandwidth mismatch issues; data intended to reach an HBM will need to stop in AMEM along the way. The transfers will need to be facilitated by a combination of a DMA engine and potentially a hardware dispatch scheme (see below). 


Each device is connected to the chassis back-plane fabric which facilitates a chip-to-chip fabric switch. This is built on top of 112 Gig SerDes which require some link-training, and the occasional maintenance packet to validate link strength. This fabric allows us to target another device's HBM3 memory on the chassis, which likely stops through the AMEM–all at the same latency and cost. We have about 500GB per second of bandwidth to do so, and this is one of the biggest architectural advantages for us to leverage. The primary use-cases of this chip-to-chip fabric are to facilitate the re-aggregation of distributed work after tensor-parallel steps.


TODOs:
               1. Fabric setup and maintenance as it pertains to firmware
               2. Fabric options, diags, link-degradation detection etc
               3. Fabric errors, fatal conditions
               4. Fabric channels, network-over-fabric?
Inference Line Card (ILC) 
ILC Block diagram (WIP)
  

High level operation
               * All system power-on/boot (including hot-plug, warm re-initialization, etc.) is coordinated by the top-of-chassis "routing and control boards".
               * The CPU on the ILC mezzanine PXE (or similar) network boots by fetching its boot image and the latest Pyxis firmware.
               * Once booted, the ILC CPU enumerates and initializes Pyxis MCMs associated with their local PCIe switch and pushes firmware into them.
               * It also starts running some services that encapsulate Pod resource coordination traffic and ROCK runtime.
               * Once ready, it enters a waiting state for work to be allocated to it.
               * Work can be as much as a whole network or as little as a single shard.
               * When work is scheduled, network weights (likely pre-sharded) are fetched using the front-panel Ethernet interfaces and placed into the HBM memories of the Pyxis chip, or the chips on the ILC.
               * The lower layer of the ROCK runtime (a.k.a. "Pyxis.bin"), comprised of the pre-compiled orchestration logic corresponding to the specific graph (or the specific shard), loads into the Pyxis chip or chips on the ILC.
               * All prompts arrive via the front-panel Ethernet interfaces and the ILC CPU triggers graph execution starting with the first layer.
Management Card and OS
Similar to the Inference Line Cards described above, each Management Card operates through an x86 processor, which runs our “partner OS” with a few Recogni services and containers to handle inference execution and model dispatch, health monitoring and state maintenance. 


For the most part, after a set of ILCs boot and register to the server farm, the Management card stays out of the way. It provides a way to find Pyxis devices through its 10Gig control interface, which also is connected to the datacenter.
Glossary
ATen is the primary PyTorch tensor and mathematics library upon which every other PyTorch machine learning tool depends. Almost all Python and C++ PyTorch interfaces are built using this library. See this link for more details. Short for A Tensor library. Aten is a single computational graph of a PyTorch model.


Bayesian Network - A graphical model that uses Bayesian Inference for classification, regression and clustering for machine learning. Bayesian reasoning changes the probabilities for a hypothesis as more evidence or information becomes available.


Batch - a selection of samples from the dataset. Batch is a hyperparameter that controls the number of samples to compute before an update of the model’s parameters.


Continuous batching: In LLM inference, a fixed batch size, start executing when the batch is full or time out. The practice of batching requests on a per-generated token basis, which is called iteration-level scheduling. It takes place after each forward-pass determines the batch composition, and uses prefill and decode requests.eline


Biases - Biases are parameters that you use in conjunction with weights to help models make better predictions. A bias allows the model to more flexibly fit the data, so it shifts the output function. It's similar to an intercept in a linear equation - it allows the model to make a prediction even when all input features are zero. Certain software tools exist to allow engineers to evaluate, visualize, share, and monitor weights and biases during model training. Also see Weights.


Canonicalization - Also known as Canonical Correlation Analysis (CCA), canonicalization looks for correlation between different data sets/tensors in the model.


Chassis - The enclosure that houses N line cards, 1-2 management cards and M fabric cards.


Convolution - Deep learning operation to extract features from data residing in tensors. It bears resemblance to the convolutions in the human brain.


Convolutional layer - a fundamental building block of Convolutional Neural Networks (CNNs), used in image processing, computer vision, and other spatial data analysis. Conv layers automatically detect and learn edges, textures, shapes, and patterns from input data, typically from images. Also frequently employs tiling.


Dynamic Shape - A feature that is popular among compiler-based AI frameworks. Dynamic shape support enables AI compilers to work with tensors of varying sequence lengths in the dataset. Batch size and token counts aren't normally known before compile jobs, so reasoning about graph shapes is helpful during compilation. It builds on the methods of preceding compilers like Glow and XLA.


Epoch - A single pass by the model through an entire training dataset. You can have too few or too many epochs when you train a model. If you overtrain a model by using too many epochs, the model can learn the noise in the training data, which affects performance and causes hallucinations and other problems. If there are too few epochs, the model doesn’t have enough time to learn the patterns in the data, which also affects model performance.


Dynamic batching: In LLM inference, adjust batch size based on workload, tradeoff between throughput and latency.


Exponent Bias (EB) - An editable value in model conversions to shift the magnitude of representable numeric ranges to smaller or greater values. Set EB values for the activations and weight tensors in the neural network.


Fabric - The any-to-any switch fabric which is proprietary IP of our “counterpart”.


Fabric Card - A vertically running card in the back of the chassis which allows any-to-any routing. They have a 10gig uplink to the Management Complex and no on-card CPU driving them.


Fabric Link - a configured link between any two Pyxis Devices in the chassis. It is important to note that there is no distinction between a link of two devices on the same line card as opposed to two different line cards. All Pyxis devices in the chassis can see all the other ones with the same latency and bandwidth.


Fully Connected linear layer - A ML model layer in which every input neuron connects to every output neuron.

Forward Pass - The process of transmitting the input data through the layers of a neural network to produce an output. Synonym: Forward Propagation.


Graph - A graph is a body of structured data that's organized as a network of connected nodes. It can represent an actual language model. To convert the model for Recogni system compatibility, you submit this graph entity to the ROCK compiler. Graph neural networks offer greater performance of inference tasks.

Hyperparameters - Configuration variables that data engineers set before converting models to Recogni operations. Examples include the following:
               * Batches
               * Exponent Bias
               * Iterations
               * Temperature
               * Learning Rate
Recogni systems aren't used to train models, so the hyperparameters for Recogni systems differ from others.
Inference Line Card (ILC) is a horizontal card that contains N (usually balanced to the any-to-any bandwidth afforded by the fabric card that it intercepts) Pyxis devices connected to a Inference Line Card CPU over PCIe as endpoints. ILCs also have a 10gig uplink to the Management Card for the control plane.


Inference Line Card CPU is an AMD EPYC3000 CPU complex (SnowyOwl) that is responsible for managing the N Pyxis Devices and also potentially submits work to them based on what the Management Card (MC) requires. 


Management Card (MC) refers to the top-of-rack cards that are not connected directly to the fabric. They command an ethernet switch that extends the local control plane network to all inference line cards and fabric cards. This device will usually have a primary and backup replication. It is the only currently imagined link to the outside world. They were formerly referred to as “Route Engine Cards” or “Supervisor Cards”.


Pyxis is the Recogni inference architecture.


Pyxis Device is our first generation ASIC for data-center LLM inference.

Tensor: a tensor is an algebraic object that describes a multilinear relationship between sets of algebraic objects tied to a linear space. They’re also commonly used in physics. Many tensors can be considered a type of multi-dimensional array.[4]


UIE, short for Universal Inference Engine, is the primary compute unit of Pyxis. It consists of a Grid, which performs array operations like convolution and matrix multiplication, a VPU and associated VBuffer, which perform vector operations, and various hardware for preparing and transforming data from AMEM. The UIE was previously called “NNU”.


Extra Bits That Haven’t Been Incorporated Yet
NB: These drawings should be considered collection points for information which will later be combined into something neater. It may be appropriate to consider a hierarchy of such drawings, with a top-level SW drawing and separate drawings zooming in on sub-systems / acting as top-level drawings for those subsystems.
https://lucid.app/lucidspark/3b30d868-7b2f-4c8d-bcee-de17f69ba2c3/




  

Lukas Rinderhas created a system drawing describing all software components from an AI-SDK perspective. 


A similar drawing with more focus on the datacenter and system software running on the chassis components can be found here, and is copied below:Pyxis SW High Architecture.pptx
  

—
We currently have a board with an AMD SnowyOwl CPU to test performance. There was a hot discussion about what is to be tested with the very first version of the dummy line card. We can put the ideas here:
               * Difference (performance, boot time, …) between regular Linux and Juniper substrate
               * How Juniper services/drivers affect us
               * If we need to build Linux distribution ourselves - this is a time for board-specific adaptations


________________
[1] If Kubernetes will be provided by us, we would most likely build the images for the base system with Packer, perhaps with QEMU backend instead of Google Compute.
[2] The Nvidia device plugin for instance uses the nvml library for this purpose. The device plugin runs as a containerized application on every node and typically has access to all device nodes /dev/*.
[3] This will be ensured by labeling all nodes in a chassis with the same “fabric-id” label and then requesting that all pods/containers which are part of a replica are scheduled on line card nodes with the same label.
[4] From Wikipedia
[a]Shaba, Harold, can we please maintain consistency with PRD for  SW Product definitions.  In PRD Summary "Systems Software" is defined differently. Suggestion: Systems Software  ("Pod System Software for Data Center Operators") = Platform SW (chassis + ILC, what you call system software) + Data Center Software.  Recogni SDK/"SDK for AI Apps Devs" should be a separate document that is referenced.  There is also a category of internal development tools which can be included or excluded from this doc (Racer, SimC/TLV, Conviewcious, Tracer,...).
[b]This document is supposed to cover all the software relevant to Pyxis, so we can't reasonably exclude internal development tools. This spec is not just a derivative of the PRD.


As to naming conventions... I think every person I've talked to has had a different idea as to what some of the names should be. I think the first step here has to be figuring out *how* to come up with standard names. I don't really care what the names are; I just don't want to have to keep editing documents to change them.
[c]"Recogni SDK" refers to a particular subset or packaging of the software, and has a section of its own later in this document.
[d]For the terms that are already defined in PRD lets please use the same, so the specs tie back. If we make changes we make changes all across.
[e]..and similarly, feel free to change nomenclature in the PRD if there are inconsistencies (though please coordinate with Vinay/Gilles if/where applicable).
[f]I agree with the sentiment here, and will endeavor to keep things in sync in the future. I ask that, rather than leaving a general comment here, you just mark specific issues that you see in the future, and I'll update them individually.
[g]When ready, we need to describe how to prepare a model here.
[h]When ready, we need to describe how to start up and shut down a system here.
[i]Juniper resources about kubernetes:


https://www.juniper.net/documentation/en_US/contrail20/topics/concept/kubernetes-cni-contrail.html
1 total reaction
Richard Grace reacted with 👍 at 2024-05-28 22:44 PM
[j]https://github.com/Juniper/contrail-kubernetes
[k]Juniper resources about kubernetes:


https://www.juniper.net/documentation/en_US/contrail20/topics/concept/kubernetes-cni-contrail.html
1 total reaction
Richard Grace reacted with 👍 at 2024-05-28 22:44 PM
[l]https://github.com/Juniper/contrail-kubernetes
[m]Calling the binary `recogni-inference-server` suggests that all pods belonging to a replica run a server while only the rank 0 pod does so. We'll need to select a better client facing name before release.
[n]This diagram comes from very early days. The components "Entrypoint", "Server", and "RockRT" are a single binary now. Apart from this, everything is still correct.
[o]@harold.zable@recogni.com can we update this region to describe racer, and point to some examples of the types of stuff it generates? I suspect some of the roadmap / goal stuff can be removed from here and memorialized in corresponding JIRA ticket items
_Assigned to harold.zable@recogni.com_
[p]Sure. Give me a day or three...
[q]I'm going to need some help with moving things to JIRA, as I'm still a JIRA newbie. 


The text here does already describe Racer's basic functionality. I could also add more user-level documentation, but I'm not sure that's appropriate for this document. The SW spec, in general, seems to have high-level motivations and not system details.


I keep wanting to compare this to the HW FSpec. The difficulty is that HW docs usually need a lot of detail before the system is completed, while SW docs are usually sketchy at first, and only get details after the SW is in progress.


I can add a sample trace output and describe what's in it. I'll do that while we figure out what to do next.
[r]@tobias.spath@recogni.com can we borrow some of your help on Racer JIRAfication?
_Reassigned to tobias.spath@recogni.com_
[s]of course @harold.zable@recogni.com feel free to schedule a meeting at your earliest convenience in my calendar!
_Reassigned to harold.zable@recogni.com_
[t]Right now, the goals seem to be very much tailored to hardware design choices. Can we also elaborate here on the goals in the next 1-2 years (i.e., when we don't design hardware). What I have in mind:
- Completely internal tool or also supposed to be used by customers (e.g., later versions of CMAT)?
- How does it relate to the Python proxy profiler and a potential Rock profiler?
- How will we use it for the Pyxis, where most hardware design choices are done already?
[u]@shaba@recogni.com , do we need to measure power consumption as part of tracing tool or is it a separate topic?
_Assigned to shaba@recogni.com_
[v]I suspect our portion of power consumption from a compute only perspective is less interesting that total chassis power, which may come from a different level of abstraction. Thoughts?
[w]Coming up with estimated power numbers is one of the key CMAT requirements. Hardware guys are still spending a lot of effort minimizing power usage. If there are reasonable numbers to give, I think people will want to see them.
1 total reaction
Mikhail Lappo reacted with 👍 at 2024-05-15 01:26 AM
[x]@shaba@recogni.com , can we get HW synchronized time across Pyxis devices?
_Assigned to shaba@recogni.com_
[y]Hmm, we will have a very low latency fabric to connect them to one another, they will also have "networking" like access via PCIe to the outside world (virtual ethernet if you will). So networking protocol based time sync is easy to a global clock, but down to the clock-cycle synchronization, I am not sure. Need to look into how we could even achieve that from a chassis perspective (maybe possible within a line card?).