Recogni Inc. | Recogni GmbH
Silicon Valley (US) | Munich (Germany)
	  

	

Vector Process (VPU) Functional Specification For Pyxis


Sept/07/2022
________________


1. Introduction        5
1.1. Brief Overview of Vector Processor Unit In UIE Data Path        5
1.2. Vector Processor Interactions With Rest Of Chip        6
2. Vector Processor Data Formats        8
2.1. Cast FP16 Data to INT16 Format        9
2.2. Cast INT16 Data to FP16 Format        10
3. Generate Data Beyond 16-Bits        11
3.1. Addition Result Beyond INT16        11
3.2. INT16 Multiplication To Generate INT32 Results        12
3.3. Accumulation, Multiplication And MAC Operations Beyond FP16        12
3.3.1. Accumulation Result In FP21        12
3.3.2. Multiplication Result In FP17        13
3.3.3. MAC Result In FP21        13
4. Complex Function Support        13
4.1. Generic LUT Approach        15
4.1.1. Index Generation And Mathematical Formulation        15
4.1.2. Function Value When Input Equal To Zero        15
4.1.3. Bias Handling For Generic LUT        16
4.2. Enhanced LUT (eLUT) Approach to Improve Precision Without Increasing Cost        17
4.2.1. Index Generation And Mathematical Formulation        19
4.2.2. Function Value When Input Equal To Zero        22
4.2.3. Bias Handling For Enhanced LUT        23
4.2.4. LUT Versus Enhanced LUT (eLUT) Results        23
4.3. Hardwired LUT        23
4.3.1. Exponential Function And Bias Handling        23
4.4. Dedicated LUT Instructions for Specific Functions        24
4.4.1. Reciprocal Function (FP16)        24
4.4.1.1. Bias Handling For Reciprocal Function        25
4.4.2. Square Root Function (FP16)        25
4.4.2.1. Bias Handling For Square Root Function        27
4.4.3. Reciprocal Square Root Function (FP16)        28
4.4.3.1. Bias Handling For Reciprocal Square Root Function        30
4.5. Expanded Usage of LUT Tables        31
4.6. LUT Table Configuration By Software        31
4.6.1. LUT Logical View For CSR Access        32
4.6.2. CSR Writes To Two LUT Tables        33
5. Combined SEA Function And FlashAttention For Efficient SoftMax / LayerNorm Calculation        34
5.1. Attention Data Operation In Pyxis        35
5.2. Data Operation Within One Tile        37
5.2.1. Find Maximum Value in SoftMax Input Vector Within One Tile        38
5.2.2. SEA Function To Generate Exponential Value And Accumulation        38
5.2.2.1. Converting Scaling Operation Into Exponent Adjustment —- Mathematical Aspect        40
5.2.2.2. Converting Scaling Operation Into Exponent Adjustment —- Instruction Aspect        42
5.2.2.3. Instruction Examples        43
5.2.2.4. Mathematical Details Of Instruction RND_FP_TO_INT        44
5.2.3. Fuse SoftMax Exponential Function And Value Tensor MatMul In One Path        45
5.3. Data Operation Across Multiple Tiles        46
5.4. Attention Data Path In Pyxis        46
5.5. SEA Function To Support Efficient LayerNorm        48
6. Vector Processor Instructions        50
6.1. Registers in VPU        50
6.2. Instructions For VPU Arithmetic Operation        51
6.2.1. OPCODE And Special Values        52
6.2.2. Special Values For GPR Index        53
6.2.3. Counter Control Field In Every Instruction        54
6.2.4. Exponent Adjustment to Align EB (Bias) For Tensors        55
6.2.5. R-type OP-INT Instruction for INT16        56
6.2.6. R-type OP-FP Instruction for FP16        58
6.2.7. R-type OP-MIXED Instruction for Mixed Mode        61
6.2.8. I-type OP-IMM-INT Instruction for INT16        62
6.2.9. I-type OP-IMM-FP Instruction for FP16        64
6.2.10. R-type Enhanced OP-INT Instruction for INT16        69
6.2.11. I-type Enhanced OP-IMM-INT Instruction for INT16        70
6.2.12. I-type LOAD Instruction        71
6.2.13. R-type STORE Instruction        74
6.2.14. R-type SEA Instructions For SoftMax        77
6.2.15. R-type Extended Floating Point Instructions        81
6.2.16. R-type SEA Instructions For LayerNorm Variance        84
6.2.17. Counter Register Operations        86
6.2.18. No-Op Operation        87
6.3. Rules For Register Access And Instruction Sequences        87
6.3.1. Rules For Accessing VPU Registers        87
6.3.1.1. Rules For Accessing GPRs        87
6.3.1.2. Rules For Writing To OUTFIFO        89
6.3.1.3. Rules For Reading INFIFO        89
6.3.1.4. Rules For Accessing NN-REG        89
6.3.1.5. Rules For Accessing FP21 Registers        89
6.3.2. Rules For Instruction Sequences        90
6.3.2.1. Rules To Avoid Data Dependence Hazards        90
6.3.2.2. Rules To Avoid Write Operation Hazards        91
6.3.2.3. Rules For SEA Instruction Sequences        91
6.3.2.4. Rules For SEA_LAYRN Instruction Sequences        92
6.3.2.5. Rules For FP21 and FP16 Instruction Sequences        92
7. Special Cases In Design        94
7.1. Overflow And Underflow        94
7.2. NaN Handling        94
7.2.1. Basic Rules To Handle NaN        94
7.2.2. A Few NaN Handing Cases        95
7.2.2.1. IMM[15:0] Field In I-Type Instruction        95
7.2.2.2. FP16 Max / Min Instructions        95
7.2.2.3. PreLu / Leaky_ReLu Instructions        95
7.2.2.4. Threshold_ReLu Instruction        95
7.2.2.5. M_partial Register Update in SEA Instruction        96
7.2.2.6. LUT Functions        96
7.3. Mul(0, inf)        96
7.4. Divided By Zero        96
7.5. Square Root Of A Negative Value        96
7.6. Add(-max, +max)        97
8. Algorithm Examples Implemented By VPU        97
8.1. SoftMax-Based FlashAttention        97
8.1.1. Algorithmic-Level Math        98
8.1.2. Single-Tile FlashAttention (STFA)        100
8.1.3. Dual-Tile FlashAttention (DTFA)        103
8.1.4. Additional Details        105
8.2. Sigmoid-Based FlashAttention        105
8.3. LayerNorm        106
8.4. ArgMax        106
8.5. Multinomial Sample        106
9. Appendix        106
9.1. Special Handling In Near-Zero Rounding        106
10. Design Reviews        107
10.1. Functional Specification Review August 10, 2023        107
10.2. Functional Specification Review, Part 2, November 16, 2023        107
________________




1. Introduction
Vector Processor Unit (VPU) is a SIMD structure (or ALU array) in Pyxis that was specifically designed to handle non-convolutional and non-matmul operations, which are required at various stages in AI algorithms. The fundamental idea behind this structure is to provide a fully programmable environment and take the advantages of tensor operations, where a single instruction can initiate massive parallel computations for multiple sets of data. This document summarizes the current status of the Vector Processor proposal for Pyxis, outlining various aspects of the proposal, including
* Vector Processor instructions
* Vector Processor internal structure
* Vector Processor integration into UIE structure
   1. Brief Overview of Vector Processor Unit In UIE Data Path
​​The VPU is introduced as a separate processing unit, next to LLC Grid, as shown in Figure 1.1.
  

Figure 1.1: VPU in UIE Data Path (from Pyxis Function Spec).
VPU operates concurrently with the LLC Grid. The VPU can do dedicated vector-wise operations on activation data, or the ALU operation may be “fused” to the end of a convolution or MatMul from the Grid. The VPU operates on FP16 data natively. 
Writeback data from the LLC Grid may be routed through the VPU. The VPU has a VectorBuffer (Vbuffer) memory which can buffer multiple tiles of output from the LLC Grid, so this serves as a  decoupling buffer for writeback data. The VPU can then execute a fused activation function on the writeback data before forwarding it to AMEM.  
For dedicated vector-wise operations, data is fetched from AMEM as required by the specific algorithm,  and sent directly to the VPU, bypassing the LLC Grid. Data is first written into a Vector Buffer (Vbuffer)  memory for use by the VPU algorithms. The VPU has its own custom microcoded instruction set and  general-purpose registers, and can execute multiple microinstructions on each data element received  from AMEM. When complete, the results are sent back to AMEM via the Write datapath.  
   2. Vector Processor Interactions With Rest Of Chip
Figure 1.2 shows the NNU sequencer hierarchy, VPU has its own sequencer.  VPU operations need to get instructions from sequencer, input data (from AMEM) and have to send the post-processed data back to AMEM. The whole process can be complex, involves many modules, and has long latency from the beginning to the end.
  

Figure 1.2: NNU Sequencer Hierarchy (from Pyxis Function Spec)
In order to simplify the design and verification, make the control easier and less error prone, and handle the non-deterministic congestion which may occur somewhere along the path, three decoupling FIFO structures are introduced around ALU as shown in Figure 1.3, which are:
* INFIFO: Data from VBuffer will be stored in this FIFO, passively waiting for ALU processing. To move data in AMEM or Accumulator in Grid to ALU for processing, move that data to VBuffer first.
* OUTFIFO: Post-processed ALU data is stored in this FIFO, waiting for writing back to VBuffer by sequencer. Data in OUTFIFO can be moved to AMEM, or to Grid for further processing.
* CTRL_FIFO: After 36-bits instruction is decoded by IDU (Instruction decoding unit), the decoded control signals (82 bits for now) are stored in CTRL_FIFO, waiting for a popping signal from ALU to fetch the control signals. These control signals will instruct ALU on what actions to perform.
  Figure 1.3: ALU interactions with rest of chip


ALU sequencer’s job is to issue instructions for data processing. Other Sequencers (e.g. AMEM read or write, Datapath for read or write) don’t care how ALU will process the data and how long ALU will take. 
Whenever the following conditions are all met, ALU will start operations.
1. ALU is in “Active” mode, which is controlled by the central sequencer.
2. INFIFO has data pending for processing.
3. OUTFIFO has space to take post-processed data.
4. CTRL_FIFO has the decoded instruction control signals.
When ALU starts processing, it issues the pop signals to CTRL_FIFO to fetch control signals, conditionally pop INFIFO to get input data (some instructions may process data already in ALU GPR, so don’t pop INFIFO), and push post-processed ALU data to GPR or OUTFIFO. 
As shown in Figure 1.3, when sequencer sends the instruction to IDU, it also provides a signal cnt_limit associated with the instruction; this signal specifies how many times the corresponding instruction would be used. This feature is particularly useful in tensor flow applications, because it is very common that a massive amount of data would be processed by the same instruction (e.g. ReLu, GeLu, etc).
________________


Pending: Need to work out the details of cnt_limit and see how the sequencer will handle it.
________________


It is the other sequencers' job to move data from AMEM to INFIFO, and from OUTFIFO back to AMEM. If ALU is in “Active” mode, but FIFOs around ALU are not all ready, ALU will just be waiting. ALU can be pulled out of “Active” mode by the central sequencer.
It is important to note that ALU itself will never introduce any unpredictable nor unexpected latency. That means the central sequencer should know exactly the ALU status, and the status of INFIFO, OUTFIFO and CTRL_FIFO, even though the ALU can be far from the sequencer. This is very important for the central sequencer to orchestrate the whole process. 
________________


Note: It is important to note that the ALU sequencer will not actively push the decoded instructions or data into the ALU. Instead, it will only push them into the CTRL_FIFO and INFIFO. The ALU itself will be responsible for determining when to fetch from these FIFOs. This approach significantly reduces the design complexities, particularly in the ALU sequencer design, since the ALU processing latency and the ALU sequencer control are completely decoupled. This means that any changes to the ALU processing latency or the introduction of a new instruction with a large processing latency will not affect the sequencer's design in any way. This will also substantially reduce the design verification complexities, as it eliminates the need to verify the coordination between the sequencer latency and the ALU processing latency.      
________________


2. Vector Processor Data Formats
ALU supports two types of data formats
1. Recogni Floating Point: FP16 (Sign: 1b; Exponent: 5b; Mantissa: 10b)
2. Recogni Floating Point: FP17 (Sign: 1b; Exponent: 6b; Mantissa: 10b)
3. Recogni Floating Point: FP21 (Sign: 1b; Exponent: 6b; Mantissa: 14b)
4. Integer:                         INT16
The floating point value of FP16 is computed as follow
                                (E.q.2.1)[a][b]


Where bias  can be set by software, for example set it to -15.
________________


Note[c]: 
1. The ALU natively supports FP16 format for inputs, outputs, and internal operations. If the data in AMEM is in FP8 format and needs to be processed by the ALU, it will be converted to FP16 by appending 4 bits of zero in the least significant position before entering the ALU. Bias adjustment may be necessary during this process.
2. ALU always outputs data in FP16 format. If FP8 is required to reduce memory footprint and data transfer bandwidth, FP16 data will be converted into FP8 by applying IEEE compatible rounding policy (described in Section 2.2). This conversion happens outside ALU, and should be instructed by sequencer.
3. In the current POR, subnormal values are not supported in FP16. This means that the value 1.0 in Equation 2.1 cannot be set to 0. However, the exponent bias (EB) in Equation 2.1 can be adjusted, as ALU instructions do support EB adjustment. With EB adjustment, FP16 can effectively accommodate a wider range of subnormal values.
________________


Although the ALU can receive 16-bit input or output 16-bit data in one cycle, AMEM only supports data widths of 8 bits. As a result, a circuit structure must be implemented outside of the ALU to prepare data larger than 8 bits from or to AMEM over two cycles.
In certain applications, a combination of FP16 and INT16 data may be required. Consequently, it is imperative to support data format casting from FP16 to INT16 or vice versa. For instance, in prior steps of a CNN, the normalized pixel index values, expressed in FP format, are generated between -1 and 1. Now, these normalized pixel index values must be converted to actual indices in INT16 format to access pixels.
________________


Note: ALU supports data format conversion between FP16 and INT16, and will not support data conversion and calculation in a single instruction. 
________________


   3. Cast FP16 Data to INT16 Format
Assume data A is in FP16 format, the procedure to cast data A from FP16 format to INT16 format is described as follows
1. The data in FP16 is shown in (E.q.2.2) 
                                        (E.q. 2.2) 
where  is the 10-bits fraction in FP16. EB is the bias.
2.  Apply right and left shift to get the absolute value 
                (E.q. 2.3)
   3. Take 2’s complement if the sign bit is set to get final 
                                 (E.q. 2.4) 
If  has non-zero bit in the bit range higher than 15, then  is overflow. Positive overflow will saturate at 0x7fff, and the negative overflow will saturate at 0x8000.
________________


Note: Rounding policy in (E.q.2.3): 
      1. Simple rounding policy is applied. If the shiftout most-significant-bit is 1, rounds up; otherwise rounds down.
      2. The rounding is applied to the absolute value, before taking 2’s complement if the value is negative. This will ensure the symmetrical behaviors for both positive and negative data.  
________________


________________


Note: When () is negative: 
      1. If (,  will be equal to 1. This is consistent with the rounding policy discussed above.
      2. If (,  will be equal to 0.
________________


________________


Note: Both FLOOR and CEILING are supported in FP16 to INT16 conversion, see Table 6.4
________________




         4. Cast INT16 Data to FP16 Format 
Assume data  is in INT16 format, the msb bit is the sign bit. 
                        (E.q. 2.5)
The procedure to cast data   from INT16 format to FP16 format is described as follows
         1. Apply 2’s complement to data A to get its absolute value (15-bits)
                        (E.q. 2.6)
         2. Apply find-first-one algorithm on  to get value n, starting from the left. 
        (E.q. 2.7)  
            3. The linear format for  can be represented as follows, ONLY keeps 11 bits to comply with 10 bits fraction in FP16. Rounding is applied in this step due to truncation.
        (E.q. 2.8)
By definition . The rest of 10 bits can be treated as fractions bits in FP16 format.
               4. The FP16 results are
               1. FP16.sign         =  INT16.sign. 
               2. FP16.exponent = 14 + EB - n  (INT[14:0] != ‘0). 
               3. FP16.fraction    = 
                  5. If INT[15:0] = 16’h8000, the decimal value is ,  it will be handled separately.
                  6. A simple round scheme is applied in Equation 2.8. If the shiftout msb (most significant bit) is set, round up, otherwise round down. The rounding is applied to the absolute FP value to avoid bias. 
________________


Pending: Check the accuracy of casting functions (FP16 to INT16, AND INT16 TO FP16).
________________


                     3. Generate Data Beyond 16-Bits
Certain applications may require the generation of integer data beyond INT16, such as producing memory addresses that exceed . VPU provides instructions which can generate integer values larger than  (Table 6.7, Table 6.8).  
                     5. Addition Result Beyond INT16
The fundamental addition operation remains based on 16-bits. One carry-register (1-bit) and three operations are provided to generate addition results beyond 16-bits (Table 6.7, Table 6.8)
                     1. Sign extension to convert INT16 to INT32 which will be stored by two GPRs.
                     2. 16-bits unsigned addition with carry in and carry out, but no saturation at the maximum value.
                     3. 16-bits signed addition with carry in. Results can saturate at the maximum value.
The process to generate addition result beyond 16-bits is described as follows
                     1. Take the first INT16 input, apply sign extension to convert it to INT32, and clear the carry-register.
                     2. Take the second INT16 input, apply sign extension to convert it to INT32.
                     3. Take the lower 16-bits of the first and second inputs, conduct unsigned addition with carry in from the carry-register and store the carry out data in carry-register, also store the addition result in GPR.
                     4. Take the upper 16-bits of the first and second inputs, conduct signed addition with carry in from the carry-register, clear the carry-register. Detect overflow and saturate the data at maximum value if overflow occurs. Store addition result. 
________________


Note: Following the process described above, INT32, INT48, INT64, etc results can be obtained.
________________


                     6. INT16 Multiplication To Generate INT32 Results
INT16 multiplication generates a 32-bit product. As shown in Table 6.2,  three multiplication instructions are available (MUL, MULL and MULH). MUL offers standard INT16 outputs but can overflow. MULL and MULH produce INT32 outputs. MULL stores lower 16 bits of products, while MULH stores upper 16 bits results. MULL and MULH enable INT16 multiplication to generate INT32 results. 
                     7. Accumulation, Multiplication And MAC Operations Beyond FP16
As AI large language and multi-modality models grow larger, encompassing trillions of parameters, the dynamic range and precision of accumulation exert greater influence on the overall performance of AI networks. To address escalating demands and future-proof our design, the VPU introduces a new set of instructions (Table 6.12) to facilitate extended floating point accumulation, multiplication and MAC (e.g. EXTD_FP_ACCUM, EXTD_FP_MUL, EXTD_FP_MAC). 
                     1. Accumulation Result In FP21
In this extended accumulation mode, 5 bits are added into FP16, effectively converting it into FP21. The enhancements in accumulation operation as listed as follows
                     1. The accumulation result has a 6-bits exponent, instead of 5-bits in FP16. This provides the dynamic region 4-billion times larger than FP16.
                     2. The accumulation result has a 14-bits fraction, instead of 10-bits in FP16. This increases the precision by 16X as compared to FP16.
                     3. Extended accumulation introduces its own 4-entry dedicated accumulator with a data width of 21 bits. The number of dedicated accumulator entries matches the accumulation latency, enabling it to accept new data every cycle and operate at full speed.
                     4. FP21 extended accumulation results can be converted back to FP16 by bias (EB) adjustment and rounding, and moved to regular GPRs (16-bits wide) for additional processing (check instruction EXTD_FP_TO_FP16 in Table 6.12 for details.)
                     1. Multiplication Result In FP17
There are applications, such as variance calculation in LayerNorm, that may necessitate a broader dynamic range in the intermediate results to prevent overflow. A dedicated instruction is introduced (EXTD_FP_MUL in Table 6.12) to generate multiplication results in FP17 format by adding one bit to the FP16 exponent; the mantissa remains the same as FP16, resulting in FP17 having a 6-bit exponent. This extra exponent bit provides the dynamic region 4-billion times larger than FP16.
The EB adjustment in EXTD_FP_MUL is the same as regular FP16 multiplication. The overflow detection in EXTD_FP_MUL takes into account the 6-bit exponent. 
Instruction EXTD_FP_MUL has to take two FP16 data as inputs, the FP17 output can only be stored in FP21 registers, shared with SEA function (see section 5).
                     2. MAC Result In FP21
In order to speed up the popular MAC operation in AI applications, a dedicated MAC instruction is introduced (EXTD_FP_MAC) which can perform MAC operations in one instruction.
The multiplication in MAC operation is the same as the instruction EXTD_FP_MUL, taking two FP16 inputs and producing FP17 results. The accumulation is the same as the instruction (EXTD_FP_ACCUM), the summation is done in FP21. The result is stored in the FP21 register.




________________


Note: 
                     1. FP21 is enabled ONLY for accumulation or extended MAC operations. NO other operations can be performed in FP21. EXTD_FP_MUL generates FP17 results which are stored in the FP21 register.
                     2. The 8-entry dedicated accumulator (FP21) is introduced to store FP21 results and is shared with SEA function (see section 5) to support both Transformer SoftMax and LayerNorm variance generation. That means accumulation beyond FP16 and SEA function cannot be run interleaved.  
________________


                     4. Complex Function Support
Some complex functions (e.g. GeLu, Sigmoid, tanh, 1/x, square root, etc) are employed in AI algorithms, and need to be supported by VPU. One implementation approach is to use LUT to store pre-calculated values of a specific function; when the function is called by the algorithm, VPU just reads LUT to fetch the value based on the input. LUT can be configured dynamically by PIO, so have the flexibility to support almost any complex functions.  
There are a few implementation challenges by using LUT.
                     1. LUT costs are quite high. For example, FP16 has 16-bits and can generate 64K unique outputs if FP16 is being used as the input of the complex function, that means LUT should have 64K entries, and each entry has 16-bits.
                     2. AI algorithms may use multiple complex functions at once. If a LUT keeps swapping its contents in order to support different complex functions, it will inevitably hurt the performance. Multiple LUTs might be needed in the implementation to avoid performance degradation, this will increase the cost substantially.
VPU deploys several ways to address the challenges mentioned above (the following discussion assumes FP16 is the data format for both input and output of LUT).
                     1. VPU design lets multiple ALUs (e.g. 16) share one LUT to amortize the cost. Each LUT has only 256+1 entries (one extra special entry discussed in section 4.1.2), rather than 64K, to reduce cost.  Each entry stores base value and slope of 256+1 differently chosen input points. The function value at the input point which is not stored in LUT will be calculated by interpolation like the first order of Taylor expansion based on the base value and slope fetched from LUT.
                     2. In order to improve the precision without increasing LUT size, allocate LUT resources in a non-uniform fashion. For example, allocate most of LUT entries in the region where the function output is highly sensitive to the input, and a small number of LUT entries in the region where the function output is less sensitive to the input.
                     3. Use hardwired implementation to implement some most popular complex function(s). The current plan is to implement an exponential function by this approach and every ALU has it. The cost of hardwired implementation is relatively costly (~30% of one ALU without it).
                     4. Introducing dedicated instructions to handle some specific functions and VPU implementation can exploit the characteristics of those specific functions further to improve precision (e.g. 1/x, sqrt(x), etc). 
________________


Note: 
                     1. In current POR, each VPU has two logical LUT structures, and can store data for two different functions. I-type OP-IMM-FP instructions are used to implement LUT operations, IMM[15] is used to select between LUT_0 and LUT_1, see Table 6-6. 
                     2. Each entry in the LUT is 32 bits, storing a 16-bit function value in the upper 16 bits and a 16-bit slope in the lower 16 bits. The data structure lut_logical_entry_t is created to define the bit fields in each LUT entry.
                     3. LUT contents are configured and loaded by software. When software is configuring LUT, VPU can not access LUT simultaneously.
________________


                     8. Generic LUT Approach
This section discusses the mathematical formulation involved in generic LUT approach and bias handling.
                     3. Index Generation And Mathematical Formulation
The function input  in FP16 can be expressed in Equation 4.1, where  has 5-bits, and fraction  has 10-bits.  can be divided into  (msb part) and  (lsb part)
                        (E.q. 4.1)
LUT has 256 entries, and an 8-bits index to access LUT. In the generic LUT approach, the LUT index is constructed as 1-bit sign, 5-bits exponent and 2-bits (msb) fraction (), shown in Equation 4.2
                                                        (E.q. 4.2)
Each LUT entry has two data, both in FP16, the first one is the base value at each index, and the second one is the corresponding slope for the base value, as shown in Equations 4.3 and 4.4, respectively, where  is the bias of the input , may NOT be the same as the output bias.
                (E.q. 4.3) 
                                 (E.q. 4.4)
where  , is the next adjacent point larger than  
The final function data is shown in Equation 4.5, where 
                        (E.q. 4.5)
                     4. Function Value When Input Equal To Zero
As shown in Equations 4.3 and 4.4, when input  is zero (,  and  are all equal to zero), the basevalue and slope are computed as  and , rather than  and , where  for IEEE standard FP16. For some functions, this may result in inaccuracies, not only for input values exactly equal to zero but also for values close to zero.
One straightforward approach to addressing this issue is to include a special entry in the lookup table (LUT) to store the base value for input exactly equal to zero. This results in a total of 257 entries. In the implementation, this special entry has an index equal to 256. The rest of LUT entries have indexes from 0  to 255.
The rules to generate contents for LUT entry 0 and 256 are described as follows
                     1. If input  has , , and , generate the basevalue and slope according to Equations 4.3 and 4.4, and load contents into LUT entry 0.
                     2. If input  has , , generate basevalue as , the slope is don’t-care value. Load the contents into LUT entry 256.    
 
________________


Note: 
                        1. The index shown in Equation 4.2 has 8 bits and can not address the special entry 256. The logic implementation will detect whether input  is zero, and addresses the special entry 256 accordingly. 
                        2. For PIO access to write and read the contents of LUT, it will have an address equal to 9 bits.
________________




                           5. Bias Handling For Generic LUT
Assuming the general function  is implemented by generic LUT approach, the input and output are  and  respectively, both in FP16 format, and . 
Let  and  be bias of  and  respectively, they are not necessarily equal to each other. 
 has been used to generate LUT contents ( and ) shown in Equations from 4.1 to 4.5.  is used in bias adjustment during multiplication in Equation 4.5. 
Data of ,  and the final result should have bias .[d][e][f][g]
Users can choose  and  based on the application needs. The selection of  will affect input value , which is used to construct the index. Therefore,  should be chosen in such a way to maximize the utilization of all LUT entries. For example, 5 bits exponent of input  () are used to construct the index, the poor selection of  can make  less than 4, 3 out of 5 bits are always 0, and waste most of the LUT entries. 
                           9. Enhanced LUT (eLUT) Approach to Improve Precision Without Increasing Cost
As shown in Equation 4.5, 8-bits fraction is used for first-order interpolation, this may degrade the precision. This section presents an advanced LUT methodology designed to enhance precision while maintaining cost efficiency. 
The following is an incomplete inventory of functions requiring support
                           * Sigmoid (shown in Figure 4.1)
                           * Tanh (shown in Figure 4.2)
                           * Gelu (shown in Figure 4.3)
                           * Silu  (shown in Figure 4.4)
                           * Atan (shown in Figure 4.5)
  

Figure 4.1: Sigmoid function value (blue) and corresponding slope (orange)
  

Figure 4.2: Tanh function and corresponding slope
  

Figure 4.3: Gelu function value (left) and corresponding slope (right)
  

Figure 4.4: Silu function value (left) and corresponding slope (right)
  

Figure 4.5: Atan function value (blue) and corresponding slope (orange)
All the listed functions above share a common characteristic: within their entire input FP16 data range, a highly confined region exists where both the function output and its slope exhibit sensitivity to the input value. We call this region the "interesting region" or “IR”. Outside this crucial area, the function either flatlines to a constant output (e.g. Sigmoid) or exhibits a constant slope (e.g. Gelu or Silu), this is called the OR region. 
By leveraging this advantageous characteristic, we can realize a substantial enhancement in result precision. For example, a needs-based allocation strategy can be deployed to prioritize resource distribution to areas with the most requirements.
                           6. Index Generation And Mathematical Formulation
Here is how the LUT index is constructed in the enhanced LUT(eLUT)  approach
The index construction for both positive and negative input  is the same, input sign bit of  will be included in the index. 
The whole input range is divided into four regions ( is the exponent of input ):
                           * NZ Region: Near zero region, input is very close to zero, which is defined as follow
                                                (E.q. 4.6)
                           * IR Region: Interesting region where both the function output and its slope exhibit sensitivity to the input value, which is defined as follow
         ()                                        (E.q. 4.7)
More LUT resources than the general LUT scheme are allocated in this region to improve accuracy.
                           * NOR Region: Near OR region, introduced to smooth out transition between IR and OR regions, which is defined as
        ()                                        (E.q. 4.8)
NOR region has similar accuracy (for example when ) as the general LUT approach.
                           * OR Region: Outside interest region where the function slope is insensitive to the input, which is defined as follow
        ()                                        (E.q. 4.9)
Less LUT resources than the general LUT scheme are allocated in this region, so IR region can use more LUT resources.
As shown in Figure 4.6, the enhanced LUT approach divides input x (with EB=-15) into four regions. The NZ region occupies 18 entries, IR 218 entries, NOR 8 entries and OR occupies the rest of 12 entries.
  

Figure 4.6: eLUT divides input x values (w EB=-15) into four regions 
(NZ, IR, NOR and OR Regions)
                           1. NZ Region Index Construction: 
The NZ region occupies 2 x 9 = 18 entries. 
        (E.q. 4.10)
Index range without sign [32, 40]. 
 share the same index and LUT entry. When , and 2 the base value is set with .  
The LUT entries in NZ region are corresponding to the following input  values (ignoring sign): 0, , , , , , , , , , where 0 is handled in a special way as discussed in Section 4.1.2. If = -15, the upper limit of NZ region is = 0.0625. 
The slope is the average slope between  

                                                (E.q. 4.11)

                              (E.q. 4.12)
 :
                    (E.q. 4.13)
Note: if , the delta term () in Equation 4.13 may result in increment in exponent.
                           2. IR Region Index Construction: 
The IR Region occupies 2 x 112 = 224 entries.  ()
                                (E.q. 4.14)        
Index ranges without sign are [0, 28] (corresponding to =16, 17), and [48, 127] (corresponding to =11 to 15).
If  and , the index is set to , regardless .
The input  values corresponding to the LUT entries in IR region can be represented by . 
        (E.q. 4.15) 
                (E.q. 4.16)
where , is the next adjacent point larger than 
        
                :
                        (E.q. 4.17)
                           3. NOR Region Index Construction:
There are eight entries reserved for NOR region, with four designated for inputs with positive sign, and four with negative sign.  
Index = () ? 41 : (28 +  );                        (E.q. 4.18)
Index range without sign is [29, 30, 31, 41]. The corresponding values are =18, , but  can be both 18 and 19. If the input has , the index to access LUT should be  (entry 31 if the sign is 0).
The basevalue and slope are calculated in Equation 4.19 and 4.20.

                                 (E.q. 4.19)
                (E.q. 4.20)
Where if has , then ), where 
The Result and delta term are calculated in Equation 4.21 and 4.22
                (E.q. 4.21)
        (E.g. 4.22)
Note: if  = 19,   in Equation 4.22 may result in an increment in the exponent. 
                           4. OR Region Index Construction:
The OR region occupies 2 x 6 = 12 entries. ()
In OR region, ] = 1’b1, and  can not be , because  it is occupied by . 
                                 (E.q. 4.23)
 
Index range without sign is [42,47]
                (E.q. 4.24)
                (E.q.4.25)
where , is the next adjacent point larger than 
        (E.g. 4.26)
        (E.q. 4.27)
                           7. Function Value When Input Equal To Zero
The function value when input equal to zero is handled in the same way as the generic LUT (see section 4.1.2) 
                           8. Bias Handling For Enhanced LUT
Bias handling for the enhanced LUT is the same as the generic LUT bias handling (see section 4.1.3)
                           9. LUT Versus Enhanced LUT (eLUT) Results


                           10. Hardwired LUT
Some special functions may be implemented by hardwired approach for two reasons
                           1. The function may be hard to be supported with good precision by the LUT approach. For example, if the function’s slope changes very quickly, the first order interpolation may not provide good precision, e.g. exponential function.
                           2. The algorithms may employ multiple functions which have to be supported by LUT (e.g. Transformer needs SoftMax, layernorm, GeLu, etc). Reloading one LUT dynamically to support multiple functions could be challenging, and may hurt the performance. Therefore, implementing some challenging and popular functions by hardwired approach is an option. 
                           1. Exponential Function And Bias Handling
In the current proposal, exponential function will be implemented by the hardwired approach. 
A specific bias has to be used for both input and output of the function to generate the contents in a hardwired approach. Standard FP16 bias (-15) is being used to implement the exponential function (both input and output) in the hardwired approach. 
________________


Note:
                           1. If exponential function input  has bias = -15, it can use exponential function as is. If  -15, the same instruction which invokes exponential function provides a way to scale input  in order to utilize the exponential LUT properly. See details in table 6.6.
                           2. If users wish to modify the output bias, such as using a value other than -15, they can also adjust the output bias in the same instruction. See details in table 6.6.
                           3. If the input is NaN (0x8000), the exponential function output would be NaN (0x8000).
________________


________________


Note: Hardwired approach for special functions can be costly, and should not be used as a default approach. It must be evaluated carefully. 
________________


                              11. Dedicated LUT Instructions for Specific Functions
Some functions have special characteristics which can be leveraged to improve precision, for example, reciprocal function, square root function, etc. VPU has dedicated instructions to support these functions, see Chapter 4 for more information.
                              2. Reciprocal Function (FP16)[h][i]
The reciprocal function is defined as division .  is chosen to make sure the result can be presented in FP16. [j][k][l][m][n][o][p][q][r][s][t][u][v][w][x][y][z]
The reciprocal  function needs LUT support. The special characteristics of reciprocal function can be exploited to improve precision without increasing LUT size.
The procedure to compute reciprocal function by using LUT
                              1. LUT index is constructed in Equation 4.28
                                                         (E.q. 4.28)
                              2. LUT will provide base value and slope
                                        (E.q. 4.29)  
        (E.q.4.30)
where  , is the next adjacent point larger than [aa][ab][ac]
                              3. The final mantissa data is shown in Equation 4.31
                (E.q. 4.31)
                              4. Calculate the exponent of the result in Equation 4.32
                                (E.q. 4.32)
        where  may be 1 or  0, it is generated in Equation 4.31. 
        For example, , ,  will be 1. Other than this case,  should never be 1, since the slope for reciprocal function is always negative. 
________________


Note:​​ The entire reciprocal LUT operation (LUT content generation and calculation) can disregard the input sign bit, functioning as if the input were positive. For example Equation 4.31 does not include the sign bit. The final output has the same sign as the input.
________________


________________


Note: If the input x is zero, the reciprocal function result will be the value programmed in the LUT table (special entry which stores the data corresponding to input equal to 0). For example, FP16 positive maximum value is one option in this scenario.
________________


                              1. Bias Handling For Reciprocal Function
As shown in Equations 4.28 to 4.32, the input data bias  is not involved. 
Typically, the reciprocal function is utilized as the initial step in division computation, such as . We can first determine the reciprocal of data , and then employ a multiplication instruction to compute .
Bias Handling For 
Assume ,  and  are the bias for inputs ,  and division result, respectively. and  are the exponents of the reciprocal function () and division () result.
        
The bias adjustment  for multiplication is shown in Equation 4.33a
                        (E.q. 4.33a)
 The result exponent  is calculated in Equation 4.33b
                        (E.q. 4.33b)
________________


Note: Table 6.3 has an instruction to support , it also shows how the bias adjustment is set. Table 6.6 has an instruction to support Reciprocal function.
________________


                              3. Square Root Function (FP16)
The square root function needs LUT support. The special characteristics of square root function can be exploited to improve precision without increasing LUT size, for example, the LUT index has less number of bits from exponent, but more bits from fraction part.
________________


Note: Table 6.6 has an instruction to support Square Root function.
________________


The procedure to compute square root function by using LUT
                              1. Calculate bias adjusted exponent in Equation 4.34
                                                (E.q. 4.34)
where  is a signed integer and specified by IMM field (see Table 6.6).  calculation is discussed in the section 4.4.2.1
                              2. LUT index is constructed in Equation 4.35
                                         (E.q. 4.35)
                              3. LUT will provide base value and slope
                        (E.q. 4.36)  
        (E.q.4.37)
where  , is the next adjacent point larger than 
                              4. Conduct computation shown in Equation 4.38, take the mantissa of the result, which will be the mantissa of the final result. 
                        (E.q. 4.38)
                              5. The final exponent is shown in Equation 4.39
                                                (E.q. 4.39)
where  is generated in Equation 4.38 when the summation with rounding in fraction introduces increment in the exponent. For example, the summation result in Equation 4.38 has exponent larger than the exponents of  and , in this case, the  will be equal to 1.
The final output has the positive sign.
________________


Note: If the input to the square root function has a negative sign, the final output is NaN.
________________


________________


Note: There is one corner case for Equation 4.38, when , but  and there is only one bit set in . In this case the second term in Equation 4.38 has no contribution, because  will be treated as 0 by the multiplier, since our FP16 can not represent perfect 1.0 and will treat it as 0. The way to workaround this issue is to set  in the design. Test shows this approach does not introduce error, because  will not contribute to the least significant fraction bit in the final result. 
________________




                              2. Bias Handling For Square Root Function
This section will discuss the bias handling for the square root function, it includes the calculation of the bias adjustment value , which is used in Equation 4.34.
Calculation of 
Let  and  be the exponent of input  and bias, respectively,  and  be the exponent of output  and bias, respectively.  and  are not necessarily equal to each  other.  
Equation 4.39 is true for square root function, 
                                        (E.q. 4.40)
                                (E.q. 4.41)
Where INT( ) means integer part of the result. Compare Equation 4.34 and 4.40,  is calculated as follows in Equation 4.41.
                                                        (E.q. 4.42)
Bias for  and 
The data  and  are shown in the Equations 4.36 and 4.37, both should have the same bias. 


________________


Note:  calculated in equation 4.34 has to be positive value.  shown in equation 4.42 should guarantee  to be positive in normal cases. For example, the typical cases have  -15
________________




________________


Note: In order to make sure both  and  can be represented properly in FP16, the bias for  and  has to be -3[ad][ae] or more negative integer. This is because the Equation 4.37 can be re-written as follow, assuming 
        
If the bias for  is -3 or more negative, the slope shown in the expression above will be able to be represented in FP16 properly.
In order for the second term in Equation 4.38 can make contribution, the bias for  has to be less than or equal to -12. As shown in Equation 4.37, only the mantissa of the result is relevant, for simplicity, just set the bias = -15 when generating  and .
________________




                              4. silnsReciprocal Square Root Function (FP16)
The reciprocal square root function is defined as , where  is the bias of input , it is a signed value and is 6-bit.  is  divided by 2 with result round up to an integer (e.g. 15/2 round up to 8; -15/2 round up to -7), or  (sign extension has to be included in the shift operation if  is negative)
 is chosen to make sure the result can be presented in FP16, avoiding any premature saturation ( is typically negative).
The reciprocal square root function can be very useful in LayerNorm calculation, and it needs LUT support. The special characteristics of reciprocal square root function can be exploited to improve precision without increasing LUT size, similar to the approaches used in reciprocal function and square root function. For example, the LUT index has less number of bits from the exponent, but more bits from the fraction part.
________________


Note: Table 6.6 has an instruction to support the Reciprocal Square Root function.
________________


The procedure to compute square root function by using LUT
                              1. Calculate bias adjusted exponent in Equation 4.34
                                                (E.q. 4.43)
where  is a signed integer and specified by IMM field (see Table 6.6). 
                              2. LUT index is constructed in Equation 4.35
                                         (E.q. 4.44)
                              3. LUT will provide base value and slope
                        (E.q. 4.45)  
        (E.q.4.46)
where  , is the next adjacent point larger than 
                              4. Conduct computation shown in Equation 4.47, take the mantissa of the result, which will be the mantissa of the final result. 
                        (E.q. 4.47)
                              5. The final exponent is shown in Equation 4.48
                        
            (E.q. 4.48)
where  is generated in Equation 4.47 when the summation with rounding in fraction introduces increment in the exponent. For example, the summation result in Equation 4.47 has exponent larger than the exponents of  and , in this case, the  will be equal to 1. Or if ,   will be equal to 1[af], because  is equal to 2 under these conditions. 
The final output has the positive sign.
________________


Note: If the input to the square root function has a negative sign, the final output is NaN.
________________


________________


Note: There are two corner cases that need to be handled specifically.
                              1. For Equation 4.38, when , but  and there is only one bit set in . In this case the second term in Equation 4.47 has no contribution, because  will be treated as 0 by the multiplier, since our FP16 can not represent perfect 1.0 and will treat it as 0. The way to workaround this issue is to set  in the design. Test shows this approach does not introduce error, because  will not contribute to the least significant fraction bit in the final result. 
                              2. When the exponent calculated in Equation 4.48 and fraction calculated in Equation 4.47 are both zero, the result represented in FP16 would be zero, same as underflow. The better way to handle this case would be to set the least significant fraction bit equal to 1.  
________________




                                 3. Bias Handling For Reciprocal Square Root Function
This section will discuss the bias handling for the reciprocal square root function.
For reciprocal square root itself, the input bias  should be used, as shown in Equation 4.43.
Typically, the reciprocal function is utilized as the initial step in division computation, such as . We can first determine the reciprocal square root of data , and then employ a multiplication instruction to compute .
Calculation of  for 
Assume ,  and  are the bias for inputs ,  and division result, respectively. and  are the exponents of the reciprocal function () and division () result.
If the fraction parts are ignored, the exponent part of function  can be described by Equation 4.49
                (E.q. 4.49)[ag][ah]
The bias adjustment  for multiplication  is shown in Equation 4.50, where  is  divided by 2 with result round up to an integer (e.g. 15/2 round up to 8; -15/2 round up to -7), or  (sign extension has to be included in the shift operation if  is negative)

                 (E.q. 4.50)
 The result exponent  is calculated in Equation 4.51
                                (E.q. 4.51)
The data  and  are shown in the Equations 4.45 and 4.46, both should have the same bias. 


________________


Note: The bias  and exponent  in Equations (4.50) and (4.51) are for multiplication function , which is used to generate the result of , NOT reciprocal square root itself.  If user just wants to calculate 1/sqrt(x) with an appropriately chosen bias for the final result, it still has to go through the step of  , and set A=1. This will have a unified way to handle both  and 
________________


________________


Note:  Through this chapter, all the equations for the slope calculation are located precisely where the  is computed. However, the slope can be computed using alternative methods, such as averaging the slope between two adjacent points or computing the slope at the middle point of two adjacent points. Users can choose the most suitable method for generating slope values based on the specific function. 
________________




                                 12. Expanded Usage of LUT Tables
The VPU offers the ability to leverage LUT tables as general-purpose memory, storing pre-computed values for efficient retrieval by instructions. These retrieved values can be put into GPRs for further processing. In this application mode, LUTs are not solely intended to implement specific complex functions as described in sections from 4.1 to 4.4.
The instructions LOAD_LUT_H and LOAD_LUT_L (see Section 6.2.12, Table 6.9 ) can fetch the upper or lower 16 bits of a LUT entry and place the data into GPR. 
                                 13. LUT Table Configuration By Software
Logically, each LUT table contains 257 entries, including one additional special entry discussed in section 4.1.2. Software configures the LUT table by loading data through CSR write operations. The Pyxis CSR write data bus is 64 bits wide, twice the size of a single LUT logical entry. The VPU design leverages the 64-bit CSR bus to configure two LUT entries in a single CSR write operation.
________________


Note: 
                                 1. ALUs only read the LUT table during normal operations, and ALU read operation accesses only one entry (32 bits) at a time. LUT configuration by software will NOT impact ALU LUT read operations.
                                 2. CSR write access to the LUT always involves two entries (64 bits).
                                 3. CSR read access to the LUT is not supported in Pyxis to simplify the design and keep the cost low. LUT read can be done through ALU operations (instructions LOAD_LUT_H and LOAD_LUT_L) 
________________


                                 5. LUT Logical View For CSR Access
For ALU functional operations, the LUT logical view consists of 257 entries, each containing 32 bits, as illustrated in Figure 4.7. Entry 256 is a special entry which is used to store data when the input x is zero for function , as discussed in section 4.1.2.
ALU operations can read the LUT table, and the read access index by ALU operations is the same as the entry number, shown in FIgure 4.7.


  

Figure 4.7: ALU operations access one LUT table, reading 32 bits per operation. 
________________


Note: Within each entry, lower 16 bits are slope value, and upper 16 bits are base value. The data structure lut_logical_entry_t is created to define the bit fields in each LUT entry.
________________


The LUT table can be configured by software through CSR write operations. For CSR write operations, the LUT logical view consists of 129 entries, each containing 64 bits, an even entry and an odd entry are paired together as a single logical unit for CSR write operations, as illustrated in Figure 4.8. The write access index by CSR is shown in Figure 4.8.


  

Figure 4.8: CSR operations access one LUT table, writing 64 bits per operation. 
________________


Note: The data structure lut_wr_entry_t is created to define the bit fields for LUT write operation by CSR. lut_wr_entry_t is 64-bit wide, and has two LUT logical entries. 
________________


                                 6. CSR Writes To Two LUT Tables
VPU has two LUT tables. When configuring a LUT table through CSR write operations, software should adhere to the logical view presented in Figure 4.8, grouping data from two entries together to form one entry (64 bits) and setting the write access index accordingly. 
The special entry_256 is paired with a dummy entry; only the lower 32 bits are relevant when the write index is set to 128, the upper 32 bits are dropped. 
CSR write index to two LUT tables (LUT_0, LUT_1) in one VPU has 9 bits, wr_index[8:0]
                                 * if (wr_index[8]==0), writes to LUT_0. if (wr_index[8]==1), writes to LUT_1.
                                 * Within one LUT, if (wr_index[7:0]==128), writes to the special entry (entry_256).
                                 * Within one LUT, wr_index[6:0] is used to write 128 64-bit entries, shown in Figure 4.8  [ai]




                                 5. Combined SEA Function And FlashAttention For Efficient SoftMax / LayerNorm Calculation[aj]
SoftMax is one of the most challenging bottlenecks in the widely used Transformer algorithm and its variations, especially during the GPT prefill phase, where it can account for over 40% of the computation. For computer vision tasks, SoftMax’s demand is also increased with resolution quadratically. This is a cross-industry challenging task.
Pyxis ​​enhances the performance of SoftMax and Transformer Attention in the following ways
                                 1. The design not just emphasizes SoftMax optimization, but focuses on overall Transformer attention generation, and SoftMax is a key part of the attention calculation. Fuse softmax and attention calculation together to improve efficiency.
                                 2. Facilitate tile-style FlashAttention and SoftMax data flow, crucial for large LLM and tensor parallel sharding support.
                                 3. Introduces a set of dedicated instructions (SEA instruction in 6.2.14) and SEA function to enhance SoftMax performance (e.g. fuse multiple operations in one instruction).
                                 4. Enable both Grid and VPU operations concurrently, reduce the latency and improve hardware utilization substantially.
                                 5. Introduce a dedicated buffer (VPU buffer, or VBuffer) to hold the intermediate data locally, rather than in AMEM. VBuffer can transmit FP16 data between VBuffer and VPU (or Grid) in one cycle, eliminating FP16 data transmission bottlenecks between AMEM and VPU (or Grid), and minimizing AMEM access throughout the entire attention generation process. 
The equation (E.q. 5.1) shows the computation of the SoftMax function for a vector x of dimension up to tens of thousands, and large number of vectors of this scale are encountered in Transformer attention generation
                                                (E.q. 5.1)
Softmax involves exponential functions, large vector summations, and divisions. Vector processors (VPUs) in Draco and Pyxis can handle these operations, but they pose challenges due to four main reasons.
                                 1. SoftMax calculation for each element takes a few instructions, this will hurt the performance. 
                                 2. The large size of key-value tensors and SoftMax outputs increases the memory footprint.
                                 3. SoftMax requires multiple memory accesses, which degrades performance. Additionally, the data involved in SoftMax must be in high precision to ensure accuracy, but each high-precision data element (e.g., FP16) takes two cycles to transfer in Draco/Pyxis.
                                 4. Both the exponential function and division in SoftMax require the use of a lookup table (LUT) to calculate the value, and LUTs are quite costly.
The FlashAttention (see blog) approach works with existing NVIDIA chips (e.g., A100) to divide the big matrices (Query, Key and Value) into tiles, repartition the algorithm workflow, making Transformer algorithms more parallel-friendly, improving overall performance, reducing latency, and reducing memory footprint.
To support this new scheme, the existing NNU structure requires some enhancements and new hardware structures, which are expected to be modest in terms of cost. The details will be discussed in the following sections.
________________


Note: In the following discussion, the tensor of one tile has size 128 rows x 128 columns. This matches one physical LLC grid which has 16 physical rows and 128 columns. 
________________


                                 14. Attention Data Operation In Pyxis
In order to ensure numerical stability, the SoftMax equation shown in (E.g. 4.1) is typically replaced by the one shown in (E.q. 2.1), where m is the maximum value within the input vector ’s. 
                                        (E.q. 5.2)
The pseudo code to describe the SoftMax procedure is shown in Figure 5.1 
  

Figure 5.1: Softmax with online normalization


Figure 5.2 shows the core of Transformer attention, which has four main steps
                                 1.  MatMul operation. Its output becomes the input of the SoftMax function.
                                 2. Find the maximum value of each SoftMax vector, and subtract it from each SoftMax vector.
                                 3. Conduct SoftMax step to generate attention score matrix 
                                 4.  MatMul generates Attention output Z.
  

Figure 5.2: Attention in Transformer Algorithm
________________


Note: Figure 5.2 shows the will be divided by a constant  , but E.q. 5.3 doesn’t have this term. In Draco / Pyxis, division by a constant  can be absorbed into  MatMul step, for example, can be folded into the conversion step which converts input data from linear format into LNS format. The following discussion will not include division by a constant  , but algorithms can have this term and will be supported.
________________


FlashAttention divides the Q, K, and V tensors into tiles, processes them sequentially, and yields a reduced memory footprint for intermediate data, as illustrated in Figure 5.3.




  

Figure 5.3: FlashAttention process attention in tiles. Superscripts (1), (2) represent tiles. Scaling Factor is shown in Equation 5.5
This proposal optimizes the entire attention flow shown in Figure 5.3, not just SoftMax, and follows the FlashAttention's approach of dividing large tensors into tiles and processing them sequentially. The following discussion will focus on the data flow and process within one tile, and then discuss the process across multiple tiles.
                                    15. Data Operation Within One Tile
This section focuses on the data flow within one tile, e.g. one tile shown in Figure 5.3
                                                         (E.q. 5.3)
Assume  has dimension 128 x , and  and will have dimensions 128 x 128.   will be multiple of 128. Dimension 128 is chosen to fit into Grid size which has 128 virtual rows and 128 columns.
 is generated by MatMul and stored in AMEM. The current assumption is has to be in high precision data format (FP16). 
 is the input of SoftMax, the maximum value of each softmax vector must be identified as shown in Equation 5.2.
                                    7. Find Maximum Value in SoftMax Input Vector Within One Tile
Although the ALU array can be utilized to determine the maximum value m, analysis indicates that implementing a dedicated fixed function on the datapath, spanning from LLC Grid to VBuffer, to find the maximum value m can facilitate increased parallel processing and enhance overall performance, because ALU array can perform other functions while the LLC Grid is moving data from LLC Accumulator to VBuffer, and the dedicated fixed function snoop the data and find the max value.
For each tile, one row (128 elements) of maximum value m will be created and stored in VBuffer, ready for downstream processing. The maximum value has FP16 format.
________________


Note: In order to ensure that all elements in the vector x, e.g. along each row of , go into the same ALU to find maximum value m, we may generate , instead of . All elements in the same column in  will go into the same ALU.  
________________


                                    8. SEA Function To Generate Exponential Value And Accumulation
This scheme introduces an instruction-driven fixed function (called SEA function, shown in Figure 5.4)  which is part of ALU. The fixed function has following operations
                                    1. Subtraction x-m
                                    2. Exponential function exp(x-m)
                                    3. Accumulation sum(exp(x-m))
Within the SEA function, the exponential function exp(x-m) can be implemented as a hardwired LUT function to have high precision with modest cost.
The SEA function can take one input data every cycle. The total latency for the SEA function is 7 cycles (3 cycles for subtraction and accumulation, and 1 cycle for the exponential function).
The maximum value m vector is first loaded into this fixed function from VBuffer, then each element in vector x is fed into the SEA function sequentially. 
The SEA function maintains a local register () to store the maximum value among tiles that have already been processed by the SEA function. The local maximum value m of a new tile entering the SEA function is compared against , and  is updated accordingly by the updated maximum value. The updated  is then used in the exponential function. The SEA function will also compute the difference between the old  and new new one, generate the scaling a factor as follow 
                (E.q. 5.4)
                        (E.q. 5.5)
When the SEA function processes a new tile, it generates a new  that is used to scale the results of the previous tiles (See section 5.1.2) 
The SEA function generates three outputs: 
                                    1. A vector with the same number of elements as the input vector x containing.
                                    2. A Scaling Factor  between tiles.
                                    3. A scalar containing .
The SEA function pipelines are shown in Figure 5.5. The first output is ScalingFactor, which will be used to scale data in VectorBuffer. Then  are generated in pipelined fashion. The final accumulated data will be available 7 cycles after the last data from the tile is fed into the SEA function (not shown in Figure 5.5)
  

Figure 5.4: SEA Function


  

Figure 5.5: SEA Function Pipelines Within One Tile (1 cycle per input data)
________________


Note: Three operations are shown in Figure 5.4, Accumulation is conducted in FP21 with larger dynamic range and higher precision, while the remainder are performed in FP16. A special instruction is invoked to move the Accumulation result from FP21 register to ALU FP16 GPR, bias adjustment and rounding scheme will be applied in this special instruction (check the instruction EXTD_FP_TO_FP16 in Table 6.12 for details.)
________________


                                       4. Converting Scaling Operation Into Exponent Adjustment —- Mathematical Aspect
As shown in Figure 5.6, the accumulated value  is in FP21 and the accumulation takes place in the FP21 domain, while the scaling operation between tiles in Flash Attention is performed in the FP16 domain, as FP21 multiplication is not supported in Pyxis. This forces the conversion of the FP21 accumulated value to FP16, performs the multiplication (or scaling) in the FP16 domain, and then moves the result back to FP21. This process inevitably results in some precision loss, and requires managing potential overflow issues.
  

Figure 5.6: Scaling multiplication in FP16, accumulation in FP21 domain.
This section outlines an approach that converts the floating-point multiplication-based scaling operation into a simple FP21 exponent adjustment, entirely avoiding the conversion between FP21 and FP16, thus preventing any precision loss. The scaling factor, shown in Equation 5.5, can be expressed as follow
                (E.q. 5.6)
Here is the procedure to convert  into , as shown in Equation 5.6.
                                       1. Take the tile-based maximum value , scale it up by a first factor , then round to the near integer by truncating the fraction part.  After scaling and rounding, the result is still in FP16 format, represented as , but can be converted into a perfect integer.
                                       2. Scale the result in the first step  by a second constant value  to produce . 
                                        
             (E.q. 5.7)
                                          3.  will be used in the SEA function, replacing original tile maximum value .
                                          4. The scaling factor  can be expressed as follow
                
                                                                    (E.q. 5.8)
        Since  is a perfect integer after rounding, so  in Equation 5.8 has to be a perfect integer. It is a negative integer in Flash Attention application, as expressed in Equation 5.6.


                                             5. Converting Scaling Operation Into Exponent Adjustment —- Instruction Aspect
This section explains how the specialized instruction (SEA_SOFTMAX_SCALING_EB_GEN in the section, Table 6.11)  supports the operations outlined in the previous section 5.2.2.1. 
One instruction (RND_FP_TO_INT in the section, Table 6.3) is introduced to round up FP16 value to a near integer by truncating the fraction part, the operation discussed in the previous section. The rest of discussion assumes that the tile-based maximum value  has undergone this rounding process, represented by , the result is stored in a GPR.  is still in FP16 format.
A special instruction SEA_SOFTMAX_SCALING_EB_GEN is introduced that takes a new tiled-based maximum value, conditionally updates the data in the register m_partial, and generates the scaling factor which can be expressed as , shown in Equation 5.6. This is similar to the instruction SEA_SCALING_GEN (described in the section, Table 6.3), shown in Figure 5.4.
Instruction SEA_SOFTMAX_SCALING_EB_GEN involves five registers, explained as follows (the instruction format is discussed in this section)
                                             1. RS1: The FP16 GPR which stores the new tile-based maximum value  which has gone through the first scaling and rounding process. The rounding is done through instruction RND_FP_TO_INT (see section for details).
                                             2. RS2[ak][al][am][an][ao]: The FP16 GPR which stores the previous tile-based, scaled and rounded maximum value . 
                                             3. RSS: The FP16 GPR which stores the re-scaled value (), which is , shown in Equation 5.7
                                             4. m_partial: The FP16 register which stores the re-scaled maximum value of the previous tile. The value in m_partial is 
                                             5. RD: The destination register stores the signed scaling factor in integer format (7 bits), representing the exponent part in Equation 5.8. The most-significant-bit in 7-bit scaling factor is the sign bit. The remaining 6 bits represent the magnitude. 7-bit is chosen to cover the entire exponent range in FP21.
The procedure of converting scaling operation into exponent adjustment is summarized in Figure 5.7. 
  

Figure 5.7: The procedure of converting scaling operation into exponent adjustment
________________


Note: The effective max value used in SEA in this proposal is , where [ ] represents FP to INT rounding. A more general expression can be , where the term  can be chosen by the users, if they find a better constant value than  to get overall better results. By choosing  other than   will not break the algorithm. 
________________


                                                6. Instruction Examples
This section shows two examples of instruction sequences. The first one is to generate the scaling factor which can be expressed as   to enable the scaling process into the exponent adjustment. The second example is to apply scaling.
Case 1: Generate scaling factor -integer in . Purple (MUL), green, red (MUL) and gray boxes show four instructions for every one or two tiles in Figure 5.7. Four gray boxes represent one SEA_SOFTMAX_SCALING_EB_GEN instruction.
MULI_DEP_FP GPR0 <- INFIFO,1.442695;        // Two cycles to complete
RND_FP_TO_INT GPR0 <- GPR0;                // One cycle to complete
MULI_DEP_FP GPR1 <- GPR0,0.693147;        // Two cycles to complete                
SEA_SOFTMAX_SCALING_EB_GEN GPR3 <- GPR0(RS1), GPR2(RS2), GPR1(RSS)        // Five cycles
        (Some instructions in the pseudo-code shown above take more than one cycle to complete, it is possible to optimize the code to improve the latency) 
Case 2: Apply scaling factor to the FP21 accumulated value and FP16 tensor
EXTD_FP_DYN_EXP_ADJ FP21_GPR0 <- FP21_GPR0, GPR3;  // FP21 output, one cycle
DYN_EXP_ADJ_FP OUTFIFO <- INFIFO, GPR3;        // FP16 output, one cycle
                                                7. Mathematical Details Of Instruction RND_FP_TO_INT
This section documents the mathematical details of the instruction RND_FP_TO_INT with the objective to help synchronize the hardware design (e.g. RTL) and software emulations. 
This instruction (RND_FP_TO_INT) is introduced specifically for SoftMax calculation in Attention algorithms, and should not be used as a general instruction for converting the floating point values to integers. For general purpose floating point to integer conversion, refers to this dedicated section and this instruction section. 
The instruction (RND_FP_TO_INT) rounds up a FP16 value into a near integer by truncating the fraction part, but the result of this instruction is still represented in FP16 format, because the downstream logic will use this result as a FP16 value for further operations, as shown in Figure 5.7.
Assume the input to the instruction is in_fp16, output is out_fp16, the bias is EB. FP16 data structure has three components, e.g. in_fp16.sign (single bit sign), in_fp16.e (5-bit exponent) and in_fp16.f (10-bit fraction), and where EB is a  6-bit signed integer value.
Assume exp_add_eb = in_fp16.e + EB, which is an FP16 exponent that takes EB into consideration. 
If (exp_add_eb < 0) 
                        out_fp16 = 0;
                Else if (exp_add_eb == 0)
                        out_fp16.sign = (in_fp16.e==0) ? 1’b0 : in_fp16.sign;[ap][aq]
                        out_fp16.e    = in_fp16.e;
                        out_fp16.f    = 10’b0;
                Else if (exp_add_eb < 10) 
                        out_fp16.sign = (([ar][as]in_fp16.e==0[at][au])&&(out_fp16.f==0))?
                               1’b0 : (in_fp16.sign;[av][aw]
                        out_fp16.e    = in_fp16.e;
                        out_fp16.f    = in_fp16.f & rounding_mask[9:0];
                        rounding_mask[9:0]=({10'h3ff} << (10-exp_add_eb));
Else if (exp_add_eb >= 10) 
                        out_fp16          = in_fp16; 
                                                
________________


Note: In the instruction SEA_SOFTMAX_SCALING_EB_GEN, when taking data from GPR(RS1) and GPR(RS2) and subtracting them, EB must be taken into account. EB is specified in FUNCT7[5:0], see section.
________________




                                                9. Fuse SoftMax Exponential Function And Value Tensor MatMul In One Path
The MatMul between Attention score A (or SoftMax result) and Value matrix shown in Figure 5.2 can be reorganized as follow
                        (E.q. 5.9)
N is a constant introduced to prevent summation  from being too big or overflow.  
                                         (E.q. 5.10)
As shown in E.q. 5.6, the MatMul between SoftMax and Value matrix becomes a MatMul between the exponential function  and the Value matrix. With the SEA function, the exponential function  calculation and MatMul can be fused into a single pass, for example SEA function in VPU generates the exponential value, drives the results to LLC Grid through VBuffer output FIFO, then kicks off MatMul in LLC Grid, assuming the sequencer can drive Value matrix from AMEM to LLC Grid on time. The MatMul output is sent to VBuffer for further processing (e.g. scaling by maximum value m from new tiles, and accumulation over multiple tiles). 
________________


Note: The exponential function is implemented as a hardwired LUT function, see section 4.3, which uses IEEE FP16 standard EB=-15 for both input and output data. When Transformer attention calculation uses the SEA function instruction, if the input to the exponential function has EB NOT equal to -15, the instruction provides the option to adjust input EB (see Table 6.11), but the output EB of the exponential function still has -15. Since Accumulator in attention calculation supports FP21, this should be acceptable. 
________________


                                                16. Data Operation Across Multiple Tiles
The VBuffer and SEABuffer in Figure 5.8 are introduced to support data aggregation between multiple tiles. 
After one tile is processed, the following data are generated and stored in VBuffer / SEABuffer
                                                1. An array of maximum values with each element corresponding to one SoftMax vector. This array is stored in SEABuffer.
                                                2. An array of accumulated values  with each element corresponding to one SoftMax vector. This array can be stored in FP21 GPR.
                                                3. One tile of MatMul result MatMul(exp(x-m), Value) is stored in VBuffer.
When a new tile of data is fed into Grid / VPU, the maximum value of the SoftMax tile vector may be different from the previous one which is already stored in SEABuffer. If the new maximum value is larger than the previous one, the data in SEABuffer will be updated, scaling factors (E.q. 5.5) are generated accordingly, and data in FP21 GPR ( and MatMul(exp(x-m), Value)) have to be scaled accordingly, as shown in Equations 5.8 and 5.9.
Updated_sum(exp(x-m)) = ScalingFactor x Prevous_sum(exp(x-m)) + 
                                                        New_sum(exp(x-m))                                        (E.q. 5.11)
Updated_MatMul(exp(x-m), Value) = ScalingFactor x Previous_MatMul(exp(x-m), Value) +  
                                                                        New_MatMul(exp(x-m), Value)                (E.q. 5.12)
The Updated_sum(exp(x-m)) and Updated_MatMul(exp(x-m), Value) will be written back into Vbuff. This process continues until the results of all tiles have been aggregated. The final step is to compute the Attention result as follows (Same as E.q. 5.6)
        Attention = Final_MatMul(exp(x-m), Value) / Final_sum(exp(x-m))                (E.q. 5.13)
The final Attention data will be written back into AMEM.
                                                   17. Attention Data Path In Pyxis
The Attention data path in Pyxis is shown in Figure 5.8. This data path has advantages of high performance, low latency, and lowest AMEM access, because it stores all intermediate results in Vector Buffer (VBuffer) and SEA Buffer, it avoids spending two cycles to move each 2-Byte intermediate results between Grid / ALU and AMEM. 
  

Figure 5.8: Attention data path in Pyxis
(The associated tags 1 and 2 on the line represent the first and second trips, respectively.)
Figure 5.9 shows the data path between ALUs and VBuffer. VBuffer has multiple memory banks and each bank can hold one tile of tensor (128x128, data in FP16). The VBuffer banks can be divided into three categories
                                                   1. Input Tile FIFO: This FIFO can take one tile of data from AMEM or Grid. The sequencer can push data in the FIFO through VPU to conduct mathematical operations. The post-VPU processed data can be written into Tile Buffer, or Output Tile FIFO. 
                                                   2. Output Tile FIFO: Output Tile FIFO can hold one tile of data. VPU can push post-processed data into this FIFO, then the sequencer can move data in output tile FIFO  to either AMEM or Grid.
                                                   3. Tile VBuffer: Tile VBuffer is a general storage and each one can hold one tile of data. The sequencer can push data from AMEM or Grid directly into the tile VBuffer. The sequencer can also fetch data from tile VBuffer and move data through VPU for processing.
As shown in Figure 5.9, the data movement between VBuffer and VPU has to go through INFIFO and OUTFIFO in front of VPU.
  

Figure 5.9: Data path between ALUs and VBuffer
________________


Note: Table 6.11 has instructions to support SEA function.
________________


                                                   18. SEA Function To Support Efficient LayerNorm
LayerNorm, another function extensively used in Transformers, requires multiple VPU instructions. This could potentially impact the latency of LLM inference applications and degrade the overall performance.
There is an opportunity to leverage the similarities between LayerNorm and SoftMax to expedite LayerNorm by reusing the SEA function (with slight modifications) originally designed to hasten SoftMax.
SEA function combines three operations in one instruction for each input , which are
                                                   1. Subtraction:                 (where  is the maximum value in the SoftMax vector)
                                                   2. Exponential Function: 
                                                   3. Accumulation:             
The timing consuming part of LayerNorm is to calculate the variance and aggregate the squared distance from the mean value  (M2), which also has three steps for each input 
                                                      1. Subtraction:                 (where  is the mean value in the LayerNorm vector)
                                                      2. Square:                
                                                      3. Accumulation:        
Therefore if the second step in SEA function has an option to do either exponential function  or square function , SEA function can support both SoftMax and LayerNorm, enhancing the performance of both (there are some minor microarchitecture level differences), as shown in Figure 5.10. 
  

Figure 5.10: SEA function to support LayerNorm variance calculation
LayerNorm can also be processed tile by tile (with each tile having a size of 128x128), just like SoftMax. Within one tile, SEA function approach (with option to support LayerNorm) is much more efficient than Welford's online algorithm. For example, SEA function approach will take total 2-instructions per data ( 1-instruction to compute the mean value, and 1-instruction for SEA function), and Welford’s online algorithm takes ~7 VPU instructions.
A parallel algorithm can be used to combine results from multiple tiles, as shown in Figure 5.10.
   
Figure 5.10: Parallel Algorithm for variance calculation
________________


Note: Three operations are shown in Figure 5.10, the result of  has a 6-bit exponent, which provides a much larger dynamic range to mitigate the risk of overflow.  Accumulation is conducted in FP21 with larger dynamic range and higher precision, while the remainder are performed in FP16. A special instruction is invoked to move the Accumulation result from SEA function dedicated register (FP21) to ALU GPR (FP16), bias adjustment and rounding scheme will be applied in this special instruction (check instruction EXTD_FP_TO_FP16 in Table 6.12 for details.)
________________




                                                         6. Vector Processor Instructions
ALU array is a customized structure tailored to our specific requirements and applications. The instructions can be customized to enhance the performance. 
For most AI applications, if not all, the operations are predetermined, the ALU instructions can be generated at compile time and stored in the memory. 
During operation, ALU instructions are fed into a custom instruction decoder, which generates control signals that instruct the ALU to perform various arithmetic operations on data. 
Conceptually, the ALU operation can be described by the following pseudo-code. The outer for loop will access AMEM or the result from a Grid calculation, e.g. read tensor data from AMEM and put data into Data INFIFO, shown in Figure 1.3. The inner for loop will use the same set of instructions to operate on one input data. The same set of instructions will be used to process the entire tensor.    
For loop to access AMEM
For loop for ALU operations
Read AMEM
ALU Instruction 0
ALU Instruction 1
……
ALU Instruction n-1
Write back to AMEM
End loop
End loop
As shown in Figure 1.2, AMEM read part will be handled by two sequencers, one is for AMEM read, and second part is Read Datapath sequencer, which is responsible for shifting data into the right column. ALU operation instructions will be handled by ALU sequencer. The write-back of post processing data is handled by Write-back Datapath sequencer and AMEM write sequencer.
                                                         19. Registers in VPU
Besides the registers around ALU (e.g. INFIFO, OUTFIFO, CTRL_FIFO, shown in Figure 1.3), each ALU has the following registers which can be accessed by instructions
                                                         * GPR:          There are 16 general purpose registers (GPR), each GPR has 16 bits.
                                                         * NN-Reg:    One nearest-neighbor register with 16 bits, which is used to move data to its nearest 
                   neighbor ALU.
                                                         * Carry-Reg: 1-bit register to store carry out information for enhanced INT16 ADD operation, discussed in the section 6.2.10 and 6.2.11.  
                                                         * FP21 Reg:  There are eight FP21 registers, each has 21 bits, dedicated for FP21 operations
                                                         * CNTR-Reg: A Counter register storing an INT16 counter value. 
________________


Note: Counter Register (CNTR-Reg) is introduced to facilitate automatic generation of tensor row indices, a feature beneficial for ArgMax function, as well as row scatter/gather operations. See section 6.2.3 
________________


                                                         20. Instructions For VPU Arithmetic Operation 
The current proposal for ALU arithmetic instructions is shown in Figure 6.1, which has 38-bits, two types, R-type and I-type. R-type has two GPR data as operation inputs, and one GPR destination. The I-type has one GPR data as operation input, and 16-bits immediate constant value, and one GPR destination.
  

Figure 6.1: ALU arithmetic instruction formats
The following sections will discuss ALU instructions in different categories
                                                         1. R-type OP-INT:  Instructions for INT16. Source data are from two GPRs (See section).
                                                         2. R-type OP-FP: Instructions for FP16. Source data are from two GPRs (See section).
                                                         3. R-type OP-MIXED[ax]: Instructions to support data conversion between FP16 and INT16 (See section).
                                                         4. I-type OP-IMM-INT:  Instructions for INT16. Source data are from instruction Immediate field and a GPR  (See section).
                                                         5. I-type OP-IMM-FP:  Instructions for FP16. Source data are from instruction Immediate field and a GPR (See section).
                                                         6. R-type Enhanced OP-INT: Instructions to support data value beyond INT16 (See section).
                                                         7. I-type Enhanced OP-IMM-INT: Instructions to support data value beyond INT16 (See section).[ay]
                                                         8. I-type LOAD: LOAD instructions for both FP16 and INT16 (See section).
                                                         9. R-type STORE: STORE instructions for both FP16 and INT16 (See section)
                                                         10. R-type SEA: Instructions for SoftMax and LayerNorm functions, FP16 (See one section for SoftMax and the other section for LayerNorm).
                                                         11. R-type Extended OP-FP: Instructions for FP21 Accumulation and MAC operation, FP17 Multiplication (See section).
                                                         12. Counter Register Operation: All instructions can control counter operation (See section) 
________________


Note: 
                                                         1. The combined use of Opcode, Funct4, and Funct7 defines the nature and types of instructions. The definitions of each field are documented in ./common/rtl/alu_types.vh. The source file is ./common/rtl/alu_types.rdlp. 
                                                         2. In R-type instructions, Funct7 assumes a default value of 7'h0 unless explicitly specified. The Opcode and Funct4 fields are primarily used to define the instruction.
                                                         3. RSS field is a specialized source register which can only specify FP16 GPR entries, and can not reference INFIFO, NN-REG, ZERO, ALU_ID, or CNTR-REG. Only one specialized instruction (SEA_SOFTMAX_SCALING_EB_GEN in the section, Table 6.11) uses this field. It can be utilized for future expansion.
________________


                                                         10. OPCODE And Special Values
OPCODE[5] has special meanings. 
ALU instructions may take more than one cycle to complete, OPCODE[5] is being used to indicate whether the subsequent instruction(s) has data dependence on the current instruction. To distinguish such dependence at the instruction level will make ALU pipeline design much simpler, and also make compilers to schedule instructions and detect data hazards much easier. 
                                                         * OPCODE[5]==0,  before the current instruction completes the processing, the next instruction(s) can be fetched, because the next instruction(s) has NO data dependence on the current instruction result. It also covers LOAD / STORE instructions. For example, if the current instruction takes three cycles to complete, and none of the next two instructions has data dependence on the result of the current instruction, OPCODE[5] can be set to 0 in this case.
                                                         * OPCODE[5]==1,  before the current instruction completes the processing, the next instruction(s) can NOT be fetched, because the next instruction(s) has data dependence on the current instruction result. For instance, if the current instruction requires three cycles to complete and at least one of the subsequent two instructions depends on the result of the current instruction, then OPCODE[5] should be set to 1.
For instructions which can complete in one cycle, OPCODE[5] is irrelevant and set to 0 by default.
ALU decoder uses OPCODE[4:0] to decide the types of instructions, and uses OPCODE[5] of the current instruction to determine when to fetch the next data and/or instruction.
Table 6.1 lists the OPCODE[4:0] for different types of instructions (defined in ./common/rtl/alu_types.vh)
Instruction Types
	OPCODE[4:0]
	OPCODE[5]
	R-Type OP-INT
	5'b11101
	1’b0 or 1’b1
	R-Type OP-FP
	5'b11100
	1’b0 or 1’b1
	R-Type OP-MIXED
	5'b01101
	1’b0
	I-Type OP-IMM-INT
	5'b01001
	1’b0 or 1’b1
	I-Type OP-IMM-FP
	5'b01000
	1’b0 or 1’b1
	R-Type OP-ENH-INT
	5'b11110
	1’b0
	I-Type OP-ENH-INT
	5'b01010
	1’b0
	I-Type LOAD
	5'b00001
	1’b0 or 1’b1
	R-Type STORE
	5'b10001
	1’b0
	R-Type SEA-FP
	5’b10010
	1’b0 or 1’b1
	Table 6.1: OPCODE for different instruction types
                                                         11. Special Values For GPR Index
Both source and destination GPRs have a special case to support applications more efficiently.
The source GPR (RS1 or RS2 in the instruction) and destination GPR (RD) have a special value, so source and destination can be extended to INFIFO, NN-Reg[az] and OUTFIFO.
Source Register
                                                         * RS[4:0]==1_0111, it refers to INFIFO.
                                                         * RS[4:0]==1_0110, it refers to constant ZERO.
                                                         * RS[4:0]==1_0101, it refers to NN-Reg.
                                                         * RS[4:0]==1_0100, it refers to ALU_ID[6:0[ba][bb]].
                                                         * RS[4:0]==1_0011, it refers to CNTR-Reg.
                                                         * RS[4:0]=={0, RS[3:0]}, it refers to regular GPR, RS[3:0] is index.


Destination Register
                                                         * RD[4:0]==1_1111, it refers to OUTFIFO.
                                                         * RD[4:0]==1_1110, it refers to Left NN-Reg.
                                                         * RD[4:0]==1_1101, it refers to Right NN-Reg.
                                                         * RD[4:0]=={0, RD[3:0]}, it refers to regular GPR, RD[3:0] is index.
                                                         * RD[4:0]=={1, RD[3:0]} and RD[3:0] !=1111, RD[3:0] !=1110, RD[3:0] !=1101, ALU results will be driven to both OUTFIFO and GPR with index RD[3:0]. 
________________


Note[bc]: 
                                                         1. ALU can NOT write operation results into both OUTFIFO and GPR15/14/13 (index is 15, 14, 13) at the same time[bd][be][bf], because RD=1_1111, 1_1110 and 1_1101 are reserved for OUTFIFO, Left NN-Reg and Right NN-Reg.
                                                         2. RS1 and RS2 can specify INFIFO in the same instruction, INFIFO will be popped only once in one instruction.[bg][bh] Both operand1 and operand2 in ALU operations will get the same value from INFIFO.
                                                         3. RS1 and RS2 can specify NN-REG at the same time.
                                                         4. RS1 and RS2 can specify CNTR-REG at the same time.
                                                         5. Each VPU contains 128 ALUs, each identified by a unique ALU_ID ranging from 0 to 127. The ALU in the left-most position is assigned ALU_ID 0
                                                         6. ALU_ID[6:0] and CNTR-REG can only be accessed by the instructions in the following categories (not including FP instructions)
                                                         1. OP-INT
                                                         2. OP-IMM-INT
                                                         3. Enhanced OP-INT
                                                         4. Enhanced OP-IMM-INT
                                                         5. OP-MIXED
                                                         6. LOAD
                                                         7. STORE
________________


                                                         12. Counter Control Field In Every Instruction
Every instruction has a 2-bit counter control field (CNTR_CTRL[1:0]) utilized to manage the Counter Register. The Counter Register stores an INT16 counter value.
The CNTR_CTRLl[1:0] has four code points as follows
                                                         1. CNTR_CTRL == 2’b00 :  Set Counter Register to 0.
                                                         2. CNTR_CTRL == 2’b01 :  Set Counter Register to 1.
                                                         3. CNTR_CTRL == 2’b10 :  Increment Counter Register value by 1.
                                                         4. CNTR_CTRL == 2’b11 :  No-Op.
Every instruction has a counter control field, every instruction can set counter value to 0 or 1, or increment counter value by 1, or don’t do anything on counter value.  
________________


Note: 
                                                         1. Setting or incrementing Counter Register value is completely independent of instruction functions. Instruction operations can proceed as is while Counter Register is updated. 
                                                         2. The Counter Register update occurs within a single cycle. The updated counter value is available precisely one cycle after the instruction is issued, regardless of whether the instruction execution has completed or not.
                                                         3. The maximum value of Counter Register is positive 16’h7fff. When Counter Register reaches the maximum value, it will saturate at maximum value, unless reset to 0 or 1 by the instructions.
________________


Counter Register can be the source register of an instruction, accessed in the same way as GPR, INFIFO, NN-Reg, ZERO and ALU_ID, therefore the counter value can be used as an operand for integer operations. Counter value can not participate in floating point operation, but can be converted into floating point value (See mixed mode operation in section 6.2.7). Therefore, Counter Register can only be accessed by INT16 instructions, Load instructions, Store instructions or mixed-op instructions (convert INT16 to FP16). 
The CNTR_CTRL field can be generated by the sequencer which has the information about the tensor row number or index. The tensor row number or index can be used in the ArgMax function, or be used to calculate memory physical addresses to support row scatter / gather functions.
                                                         13. Exponent Adjustment to Align EB (Bias) For Tensors
Bias (EB) adjustment is very common in floating point tensor operations. For example, two tensors which will be summed together have to have the same bias, the result of two tensor multiplication needs a bias adjustment to avoid double bias counting. 
Vector processor instructions offer multiple options for the bias adjustment through exponent adjustment.
                                                         1. Exponent Adjustment: Instruction STORE_REG in Table 6.10 can be used to modify the exponent, which is equivalent to EB adjustment. For example, when transferring data from the source (specified by RS1, which can be INFIFO, NN_REG, or GPR) to the destination (specified by RD, which can be OUTFIFO, GPR, or NN_REG of the nearest neighboring ALU), the exponent of the source data can be adjusted.
                                                         2. Instruction Built-In Exponent Adjustment: Some instructions come with built-in exponent adjustment capabilities, allowing for operations such as exponent or EB adjustment, as seen in the floating-point multiplication instruction. For more details, see sections 6.2.5 to 6.2.16. Here are some example instructions which have built-in exponent adjustment are
                                                         1. MUL_W_EB_IND_FP in Table 6.3
                                                         2. MUL_W_EB_DEP_FP in Table 6.3
                                                         3. ASQRT_IND_FP in Table 6.6
                                                         4. ASQRT_DEP_FP in Table 6.6
                                                         5. All the instructions to convert data formats between FP16 and INT16 in Table 6.4    
                                                         1. R-type OP-INT Instruction for INT16
Table 6.2 shows the R-type instruction for INT16 operations. All data are in INT16 format. 
OPCODE = x_1110_1


	FUNCT4 / FUNCT7
	OPCODE[5]
	Function
	Pipe Stages
	Data / cyc
	ADD
	4’h0 / 7’h0
	1’b0
	Summation GPR(RD) = GPR(RS1) + GPR(RS2)


Note: Regular signed INT16 summation. If the result is beyond the range which can be represented by INT16, detect underflow or overflow, and set the result to the saturation value. Overflow saturation value is 0x7fff, and underflow value is 0x8000
	1
	1
	SUB
	4’h1 / 7’h0
	1’b0
	Subtraction GPR(RD) = GPR(RS1) -GPR( RS2)


Note: If RS1 is GPR(ZERO), this is effectively negation
	1
	1
	MUL_IND
	4h2 / 7’h0
	1’b0
	Multiplication GPR(RD) = GPR(RS1) x GPR(RS2) 
Produces INT16 results.


The next instruction can be fetched BEFORE the current instruction completes. 


Note: 
                                                         1. The result may overflow. When overflow happens, the value will saturate at positive or negative maximum values.
                                                         2. The positive maximum value is 0x7FFF. The negative maximum value is 0x8000.
	2
	1
	MUL_DEP
	4’h2 / 7’h0
	1’b1
	Same as MUL_IND, except that the next instruction can NOT be fetched UNTIL the current instruction completes.
	2
	0.5
	MULL_IND
	4’h2 / 7’h1
	1’b0
	Multiplication GPR(RD) = GPR(RS1) x GPR(RS2) 


Produces INT32 results. Store lower 16 bits in GPR(RD)


The next instruction can be fetched BEFORE the current instruction completes.


Note: The result will NOT overflow. 
	2
	1
	MULL_DEP
	4’h2 / 7’h1
	1’b1
	Same as MULL_IND, except that the next instruction can NOT be fetched UNTIL the current instruction completes.
	2
	0.5
	MULH_IND
	4’h2 / 7’h2
	1’b0
	Multiplication  GPR(RD) = GPR(RS1) x GPR(RS2)  


Produces INT32 results. This instruction stores the upper 16 bits in GPR(RD).


The next instruction can be fetched BEFORE the current instruction completes. 


Note: The result will NOT overflow. 
	2
	1
	MULH_DEP
	4’h2 / 7’h2
	1’b1
	Same as MULH_IND, except that the next instruction can NOT be fetched UNTIL the current instruction completes.
	2
	0.5
	SLL
	4’h3 / 7’h0
	1’b0
	Logic left shift on data in GPR(RS1) by lsb 4b in GPR(RS2), result is stored in GPR(GD)
	1
	1
	SLT
	4’h4 / 7’h0
	1’b0
	GPR(RD) = (RS1 < RS2) ? 1 : 0
Signed Comparison


Note:
                                                         1.  If the value in GPR(RS2) is set to 0, it can support SLTZ (Set if Less Than Zero).
                                                         2.  If the value in GPR(RS1) is set to 0, it can support SGTZ (Set if Greater Than Zero).
	1
	1
	SLTU
	4’h5 / 7’h0
	1’b0
	GPR(RD) = (GPR(RS1) <GPR( RS2)) ? 1 : 0 
Unsigned Comparison. Treat RS1, RS2  as 16-bit unsigned numbers.


Note: If the value in GPR(RS1) is 0, it can support SNEZ (Set if Not Equal to Zero), GPR(RD) = (GPR(RS2)!=0) ? 1 : 0
	1
	1
	SEQ
[bi]	
4’he / 7’h0
	1’b0
	GPR(RD) = (RS1 == RS2) ? 1 : 0
	1
	1
	XOR
	4’h6 / 7’h0
	1’b0
	Logic Operation XOR: GPR(RD) = GPR(RS1) ^ GPR(RS2)
	1
	1
	SRL
	4’h7 / 7’h0
	1’b0
	Logic right shift on the data in GPR(RS1) by lsb 4b in GPR(RS2)
	1
	1
	SRA
	4’h8 / 7’h0
	1’b0
	Arithmetic right shift on data in GPR(RS1) by lsb 4b in GPR(RS2)
	1
	1
	OR
	4’h9 / 7’h0
	1’b0
	Logic Operation OR:  GPR(RD) = GPR(RS1) | GPR(RS2)
	1
	1
	AND
	4’ha / 7’h0
	1’b0
	Logic Operation AND:  GPR(RD) = GPR(RS1) & GPR(RS2)
	1
	1
	MAX
	4’hb / 7’h0
	1’b0
	Find MAX Data: GPR(RD) = (GPR(RS1) > GPR(RS2)) ? GPR(RS1) : GPR(RS2)
Signed comparison
	1
	1
	MIN
	4’hc / 7’h0
	1’b0
	Find MIN Data:GPR( RD) = (GPR(RS1) < GPR(RS2)) ? GPR(RS1) : GPR(RS2)
Signed comparison
	1
	1
	ABS
	4’hd / 7’h0
	1’b0
	Take Absolute Value: GPR(RD) = ABS(GPR(RS1))


Note: 
                                                         1. If GPR(RS1) has a maximum negative value (-2^15), the result would be set to the positive maximum value, which is (2^15-1).
                                                         2. RS2 invoked GPR read, INFIFO pop or NN-Reg pop are disabled in this instruction by hardware.[bj][bk][bl]
	1
	1
	Table 6.2: R-type OP instruction for INT16 operation
________________


Note: There are dedicated instructions to support INT16 multiplication in both Table 6.2 and Table 6.5, but there is no equivalent instruction to support integer division. With LUT and existing instructions, integer division can be supported in ALU. See Appendix 7.1 for more information.
________________


                                                         2.  R-type OP-FP Instruction for FP16
Table 6.3 shows the R-type instruction for FP16 operations. OPCODE = x_1110_0. 
The result is in FP16 format (sign bit and absolute value with exponent and fraction)
Exponent bias does not necessarily participate in operation, unless specifically specified (e.g. multiplication  and square-root operations). 


	FUNCT4 / FUNCT7
	OPCODE[5]
	Data Type
	Function
	Pipe Stages
	Data / cyc
	ADD_IND_FP
	4’h0 /  7’h0
	1’b0
	FP16
	Summation. GPR(RD) = GPR(RS1) + GPR( RS2)


The next instruction can be fetched BEFORE the current instruction completes. 


Note: No EB adjustment in this instruction.
	3
	1
	ADD_DEP_FP
	4’h0 / 7’h0
	1’b1
	FP16
	Same as AND_IND_FP, except that the next instruction can NOT be fetched UNTIL the current instruction completes. 
	3
	0.33
	SUB_IND_FP
	4’h1 / 7’h0
	1’b0
	FP16
	Subtraction GPR(RD) = GPR(RS1) - GPR(RS2). 


The next instruction can be fetched BEFORE the current instruction completes. 


Note: 
                                                         1. No EB adjustment in this instruction.
                                                         2. If RS1 is GPR(ZERO), this is effectively negation.
	3
	1
	SUB_DEP_FP
	4’h1 / 7’h0
	1’b1
	FP16
	Same as SUB_IND_FP, except that the next instruction can NOT be fetched UNTIL the current instruction completes. 
	3
	0.33
	MUL_W_EB_IND_FP
	4’h2 / 7’h0
	1’b0
	FP16
	Multiplication. GPR(RD) = GPR(RS1) x GPR(RS2)


The next instruction can be fetched BEFORE the current instruction completes. 


Note: 
                                                         1. Exponent bias adjustment has been included in this operation. Funct7[5:0] is a signed value introduced for EB adjustment. The exponent of the product is: Exp(RS1) + Exp(RS2) + Funct7[5:0]
                                                         2. Assuming GPR(RS1), GPR(RS2) and GPR(RD) have EB values ,  and , respectively, then   
	2
	1
	MUL_W_EB_DEP_FP
	4’h2 / 7’h0
	1’b1
	FP16
	Same as MUL_W_EB_IND_FP, except that the next instruction can NOT be fetched UNTIL the current instruction completes. 
	2
	0.5
	SLT_FP
	4’h3 / 7’h0
	1’b0
	FP16
	GPR(RD) = (GPR(RS1) < GPR(RS2)) ? 1 : 0, Signed Comparison[bm][bn][bo][bp]


If data in RS1 is set to 0, this instruction can can support SGTZ (Set if Greater Than Zero)


If data in RS2 is set to 0, this instruction can support SLTZ (Set if Less Than Zero)


Note: No EB adjustment in this instruction.
	1
	1
	SLE_FP
	4’h4 / 7’h0
	1’b0
	FP16
	GPR(RD) = (GPR(RS1) <= GPR(RS2)) ? 1 : 0, 
Signed Comparison


Note: No EB adjustment in this instruction.
	1
	1
	SEQ_FP
	4’h5 / 7’h0
	1’b0
	FP16
	GPR(RD) = (GPR(RS1)==GPR(RS2)) ? 1 : 0;


Note: No EB adjustment in this instruction.
	1
	1
	SNE_FP
	4’h6 / 7’h0
	1’b0
	FP16
	GPR(RD) = (GPR(RS1) != GPR(RS2)) ? 1 : 0;


Note: No EB adjustment in this instruction.
	1
	1
	PRELU_IND_FP
	4’h7 / 7’h0[bq][br]
	1’b0
	FP16
	GPR(RD) = (GPR(RS1)>=0) ? GPR(RS1) : (GPR(RS1) * GPR(RS2))


Note: 
                                                            1. Exponent bias adjustment has been included in GPR(RS1) * GPR(RS2). Funct7[5:0] is a signed value introduced for EB adjustment. The exponent of the product is: Exp(RS1) + Exp(RS2) + Funct7[5:0]
                                                            2. Assuming GPR(RS1), GPR(RS2) and GPR(RD) have EB values ,  and , respectively, then   
	2
	1
	PRELU_DEP_FP
	4’h7 / 7’h0
	1’b1
	FP16
	Same as PRELU_IND_FP, except that the next instruction can NOT be fetched UNTIL the current instruction completes. 
	2
	0.5
	MAX_FP
	4’h8 / 
(7’h0, 7’h1)
	1’b0
	FP16
	GPR(RD) = (Condition) ? GPR(RS1) : GPR(RS2)


IF Funct7==0, Condition = (GPR(RS1) > GPR(RS2))
IF Funct7==1, Condition = (abs(GPR(RS1)) > abs(GPR(RS2)))


Note: No EB adjustment in this instruction.
	1
	1
	MIN_FP
	4’h9 / 
(7’h0, 7’h1)
	1’b0
	FP16
	GPR(RD) = (Condition) ? GPR(RS2) : GPR(RS1)


IF Funct7==0, Condition = (GPR(RS1) > GPR(RS2))
IF Funct7==1, Condition = (abs(GPR(RS1)) > abs(GPR(RS2)))


Note: No EB adjustment in this instruction.
	1
	1
	ABS_FP
	4’ha / 7’h0
	1’b0
	FP16
	GPR(RD) = ABS(GPR(RS1))


Note: RS2 invoked GPR read, INFIFO pop[bs][bt] or NN-Reg pop are disabled in this instruction by hardware.[bu][bv][bw][bx][by]
	1
	1
	DYN_EXP_ADJ_FP
	4’hb / 7’h0
	1b0
	FP16
	Dynamic exponent adjustment.





RS1: FP16 Input source. It can be GPR, INFIFO and NN-REG.


RS2: GPR(RS2)[6:0] specifies signed exponent adjustment value. 


Note: 
                                                               1. The reason a 7-bit value is used for the exponent adjustment, rather than a 6-bit value, is that the same adjustment value in GPR (RS2) is used for FP21 exponent adjustment in FlashAttention to scale FP21 accumulated value and FP16 tensor.
                                                               2. If overflow is detected, e.g. result’s exponent is larger than 31, the result will saturate at +/1 max value.
                                                               3. If underflow is detected, e.g. result’s exponent is smaller than 0, the result will be set to 0.[bz][ca][cb] Two special cases for rounding near zero when underflow occurs due to exponent adjustment. 
a). After adjustment, exp_fp16 = -1, fraction_fp16 > 1, it should round up (exp_fp16=0, fraction_fp16=1), rather than round to 0.
b). After adjustment, exp_fp16 = 0, the input fraction = 0, it should round up (exp_fp16=0, fraction_fp16=1), rather than round to 0
	1
	1
	RND_FP_TO_INT
	4’hc
	1’b0
	FP16
	Round FP16 to INTEGER


RS1: FP16 Input source. It can be GPR, INFIFO and NN-REG.


RS2: Ignored.


Funct7[5:0]: Bias of FP16 data in GPR(RS1), not bias adjustment.


Note:
                                                               1. Positive and negative numbers are treated in the same way. See the section for details.
                                                               2. This instruction is introduced specifically for SoftMax calculation in Attention algorithms, and should not be used as a general instruction for converting the floating point values to integers.
	1
	1
	Table 6.3: R-type OP instruction for FP16 operation
                                                               3. R-type OP-MIXED Instruction for Mixed Mode 
Current mixed model operations include data format conversion between FP16 and INT16
Table 6.4 shows the mixed operations. OPCODE = x_0110_1
For the mixed mode operations, Funct7[5:0] is repurposed to store signed bias EB data for FP16. 


	Funct4
	Data Type
	Function
	Pipe Stages
	Data / cyc
	CAST_INT_TO_FP16_MIXED
	1011
	Mixed
	Cast data specified by RS1 (INT16) to FP16 and place it into RD. Data specified by RS2 is ignored. 


Funct7[5:0] can be used as a signed EB.
The FP exponent with EB is: 
      Exp + signed(Funct7[5:0])  


OPCODE[5]=0
	1
	1
	CAST_FP16_TO_INT_MIXED
	1100
	Mixed
	Cast data specified by RS1 (FP16) to INT16 and place it into RD. Data specified by RS2 is ignored.


Funct7[5:0] can be used as a signed EB.
The FP exponent with EB is: 
      Exp + signed(Funct7[5:0]) 


OPCODE[5]=0


Note[cc]: 
                                                               1. Simple rounding will be applied. If the msb of the shifted out data is 1, round up. Otherwise round down.
                                                               2. The rounding is applied to the absolute value first, then take 2’s complement if data is negative. E.g. 95.5 will round to 96, and -95.5 will round to -96.  95.4 will round to 95, and -95.4 will wound to -95.
	1
	1
	CAST_FP16_TO_INT_FLOOR_MIXED
	1101
	Mixed
	Cast data specified by RS1 (FP16) to INT16 and place it into RD. Data specified by RS2 is ignored.


Funct7[5:0] can be used as a signed EB.
The FP exponent with EB is: 
      Exp + signed(Funct7[5:0]) 


OPCODE[5]=0


Note: Numerical flooring policy is applied (7.8 will be converted to 7, -7.8 will be converted to -8)
	1
	1
	CAST_FP16_TO_INT_CEIL_MIXED
	1110
	Mixed
	Cast data specified by GPR(RS1) (FP16) to INT16 and place it into GPR(RD). Data specified by RS2 is ignored.


Funct7[5:0] can be used as a signed EB.
The FP exponent with EB is: 
      Exp + signed(Funct7[5:0]) 


OPCODE[5]=0


Note: Numerical ceiling policy is applied (7.2 will be converted to 8, -7.8 will be converted to -7)
	1
	1
	Table 6.4: OP-MIXED instruction for INT16 / FP16 mixed operation




                                                               4. I-type OP-IMM-INT Instruction for INT16
Table 6.5 shows the I-type OP-IMM-INT instruction for INT16 operations. OPCODE = x_0100_1


	Funct4
	Data Type
	Function
	Pipe Stages
	Data / cyc
	ADDI
	0000
	INT16
	Summation GPR(RD) = GPR(RS1) + IMM[15:0]
Set IMM to zero, it will move data in GPR(RS1) to GPR(RD)
	1
	1
	MULI_IND
	0010
	INT16
	GPR(RD) = GPR(RS1) x IMM. Keep the lower 16 bits


Next instruction can be fetched BEFORE the current instruction completes. 
OPCODE[5]=0


IMM has an INT16 format.
	2
	1
	MULI_DEP
	0010
	INT16
	Same as MULI_IND, except that the next instruction can NOT be fetched UNTIL the current instruction completes. 
OPCODE[5]=1
	2
	0.5
	SLLI
	0011
	INT16
	Logic left shift on value in GPR(RS1) by IMM[3:0]
	1
	1
	SEQI
	0001
	INT16
	GPR(RD) = (GPR(RS1) == IMM) ? 1 : 0, Signed Comparison
	1
	1
	SLTI
	0100
	INT16
	GPR(RD) = (GPR(RS1) < IMM) ? 1 : 0, Signed Comparison
	1
	1
	SLTUI
	0101
	INT16
	GPR(RD) = (GPR(RS1) < IMM) ? 1 : 0, Unsigned Comparison
If IMM is 1, GPR(RD) = (GPR(RS1)==0)? 1 : 0;  Support SEQZ
	1
	1
	XORI
	0110
	INT16
	XOR on GPR(RS1) and 16-bit immediate, and store in GPR(RD)


Note: XORI GPR(RD), GPR(RS1), -1: Performs a bitwise logical inversion on GPR(RS1)
	1
	1
	SRLI
	0111
	INT16
	Logic right shift on the value in GPR(RS1) by IMM[3:0] (no sign-extension)
	1
	1
	SRAI
	1000
	INT16
	Arithmetic right shift on value in RS1 by IMM[3:0] (With sign-extension)
	1
	1
	ORI
	1001
	INT16
	OR on GPR(RS1) and sign-extended 16-bit immediate[cd], and store in GPR(RD)
	1
	1
	ANDI
	1010
	INT16
	AND on GPR(RS1) and 16-bit immediate, and store in GPR(RD)
	1
	1
	MAXI
	1011
	INT16
	GPR(RD) = (GPR(RS1) > IMM) ? GPR(RS1) : IMM


IMM is a signed 16-bit value
	1
	1
	MINI
	1100
	INT16
	GPR(RD) = (GPR(RS1) < IMM) ? GPR(RS1) : IMM


IMM is a signed 16-bit value
	1
	1
	EXTRACTI
	1101
	INT16
	Extract field from 16 bits data in GPR(RS1). 
Store result in GPR(RD)
IMM[3:0] specifies “start”. IMM[7:4] specifies “length” of the field.
If IMM[7:4]==0, it means length = 16. 


Note: If data in GPR(RS1) is FP16 data, this instruction will treat it the same way as an INT16.[ce]
	1
	1
	INSERTI
	1110
	INT16
	Insert the field from GPR(RS1) (starting from lsb, with length specified by IMM[7:4]) into GPR(RD), at “start” point specified by IMM[3:0] 


Note: 
                                                               1. If data in GPR(RS1) is FP16 data, this instruction will treat it the same way as an INT16.
                                                               2. RD can NOT be OUTFIFO, nor NN-Reg.  This is because this instruction has two accesses to GPR(RD), read and write. ALU can't read from OUTFIFO, and can't write to its own NN-REG.
	1
	1
	FFSI
	1111
	INT16
	Find-First-Set index  in GPR(RS1) (from msb), and store result in GPR(RD)


Note: 
                                                               1. If data in GPR(RS1)[15]==1, FFS will return 15.
                                                               2. If data in GPR(RS1) is zero, FFS will return value 16.
	1
	1
	Table 6.5: I-type OP-IMM-INT instruction for INT16 operation
                                                               5. I-type OP-IMM-FP Instruction for FP16
Table 6.6 shows the I-type OP-IMM-FP instruction for FP16 operations. OPCODE = x_0100_0. 
The result is in FP16 format (sign bit and absolute value with exponent and fraction)
Exponent bias does not necessarily participate in operation, unless specifically specified (e.g. multiplication  and square-root operations). 


	Funct4
	Data Type
	Function
	Pipe Stages
	Data / cyc
	ADDI_IND_FP
	4’h0
	FP16
	RD = RS1 + signed IMM. 


Next instruction can be fetched BEFORE the current instruction completes. 
OPCODE[5]=0


Note: IMM should have the same EB as data in GPR(RS1). The Exponent in IMM should NOT include EB.
	3
	1
	ADDI_DEP_FP
	4’h0
	FP16
	Same as ADDI_IND_FP, except that the next instruction can NOT be fetched UNTIL the current instruction completes. 
OPCODE[5]=1
	3
	0.33
	MULI_IND_FP
	4’h1
	FP16
	GPR(RD) = GPR(RS1) x IMM. 


The next instruction can be fetched BEFORE the current instruction completes.
OPCODE[5]=0


Note: 
                                                               1. IMM is in FP16 format, and should have standard FP16 bias EB=-15, which is used in EB adjustment during multiplication.
                                                               2. The multiplication result has the same EB as value in GPR(RS1)
                                                               3. If EB=-15 is NOT what users want, IMM should be loaded into GPR first, then use instruction MUL_W_EB_IND_FP to conduct multiplication with explicit and flexible EB adjustment.
	2
	1
	MULI_DEP_FP
	4’h1
	FP16
	Same as MULI_IND_FP, except that the next instruction can NOT be fetched UNTIL the current instruction completes
OPCODE[5]=1
	2
	0.5
	RECIPI_IND_FP[cf]
	4’h2
	FP16
	Conduct reciprocal operation on value GPR(RS1), Refer to section 4.4.1 for details


The next instruction can be fetched BEFORE the current instruction completes. 
OPCODE[5]=0


Note: 
                                                               1. IF IMM[15]==0, choose LUT_0; Otherwise choose LUT_1
                                                               2. IMM[5:0] field should be set to 0, NO EB adjustment.
                                                               3. Divided by zero will be detected and trigger an interrupt[cg][ch]. The output will be the value programmed in the LUT table (special entry which stores the data corresponding to input equal to 0). For example, positive maximum value or NaN can be the option in this scenario.
	5
	1
	RECIPI_DEP_FP
	4’h2
	FP16
	Same as RECIPI_IND_FP, except that the next instruction can NOT be fetched UNTIL the current instruction completes.
OPCODE[5]=1
	5
	0.5
	SLTI_FP
	4’h3
	FP16
	RD = (RS1 < IMM) ? 1 : 0, Signed Comparison[ci]
(If RS1 or IMM is set to 0, can support SLTZ (set if less than 0), SGTZ(set if greater than 0))


Note: IMM should have the same EB as data in GPR(RS1). 
	1
	1
	SLEI_FP
	4’h4
	FP16
	RD = (RS1 <= IMM) ? 1 : 0, Signed Comparison


Note: IMM should have the same EB as data in GPR(RS1). 
	1
	1
	SEQI_FP
	4’h5
	FP16
	RD = (RS1 == IMM) ? 1 : 0, Signed Comparison


Note: IMM should have the same EB as data in GPR(RS1). 
	

	

	SNEI_FP
	4’h6
	FP16
	RD = (RS1 != IMM) ? 1 : 0, Signed Comparison


Note: IMM should have the same EB as data in GPR(RS1). 
	

	

	MAXI_FP
	4’h7
	FP16
	RD = (RS1 > IMM) ? RS1 : IMM


Note: IMM should have the same EB as data in GPR(RS1). 
	1
	1
	MINI_FP
	4’h8
	FP16
	RD = (RS1 < IMM) ? RS1 : IMM


Note: IMM should have the same EB as data in GPR(RS1). 
	1
	1
	LUT_IND_FP
	4’h9
	FP16
	Access Look Up Table (LUT) based on data in GPR(RS1) and generate the result based on value and slope read from LUT (Section 4.1),


IMM[15]: If IMM[15]==0, choose LUT_0; otherwise choose LUT_1.[cj][ck]


signed EB = IMM[5:0] is specified by the instruction, which is used in LUT input scaling and LUT content generation, and EB adjustment in the multiplication involved in final LUT result generation. 


Fetch the next instruction BEFORE the current instruction  completes.
OPCODE[5]=0
	5
	1
	LUT_DEP_FP
	4’h9
	FP16
	Same as LUT_IND_FP, except that except that the next instruction can NOT be fetched UNTIL the current instruction completes.
OPCODE[5]=1
	5
	0.2
	ELUT_IND_FP
	4’ha
	FP16
	Access enhanced Look Up Table (eLUT) based on data in GPR(RS1) and generate the result based on value and slope read from LUT (Section 4.2)


IMM[15]: If IMM[15]==0, choose LUT_0; otherwise choose LUT_1.


signed EB = IMM[5:0] is specified by the instruction, which is used in LUT input scaling and LUT content generation, and EB adjustment in the multiplication involved in final LUT result generation.


Fetch the next instruction BEFORE the current instruction  completes.
OPCODE[5]=0
	5
	1
	ELUT_DEP_FP
	4’ha
	FP16
	Same as ELUT_IND_FP, except that the next instruction can NOT be fetched UNTIL the current instruction completes.
OPCODE[5]=1
	5
	0.2
	THRESHOLD_RELU_FP
	4’hb
	FP16
	Set the value to 0 if the value is less than a threshold.
The threshold is specified by IMM
GPR(RD) = (GPR(RS1)< IMM) ? 0 : GPR(RS1);[cl]


Note: 
                                                               1. IMM should have the same EB as data in GPR(RS1).
                                                               2. Sometimes, THRESHOLD_RELU is called JumpRELU, IMM is the jump value. 
	1
	1
	LEAKY_RELU_IND_FP
	4’hc
	FP16
	GPR(RD) = (GPR(RS1)>=0) ? GPR(RS1) : IMM * GPR(RS1)
The next instruction can be fetched BEFORE the current instruction completes. 
OPCODE[5]=0


Note: 
                                                               1. IMM is in FP16 format, and should have standard FP16 bias EB=-15, which is used in EB adjustment during multiplication.
                                                               2. If EB=-15 is NOT what users want, IMM should be loaded into GPR first, then use instruction MUL_W_EB_IND_FP to conduct multiplication with explicit and flexible EB adjustment.
	2
	1
	LEAKY_RELU_DEP_FP
	4’hc
	FP16
	Same as LEAKY_RELU_IND_FP, except that the next instruction can NOT be fetched UNTIL the current instruction completes.
OPCODE[5]=1
	2
	0.5
	ASQRT_IND_FP
	4’hd
	FP16
	Take the square root of the value in GPR(RS1) through LUT. Data in GPR(RS1) has to be non-negative.


Procedure: Refer to section 4.4.2 for details


Fetch the next instruction BEFORE the current instruction  completes.
OPCODE[5]=0


Note: 
                                                               1. IF IMM[15]==0, choose LUT_0; Otherwise choose LUT_1
                                                               2. IMM[5:0] is the signed EB adjustment. Section 4.4.2.1 shows how it should be set.
                                                               3. If GPR(RS1) is negative, the result is NaN
	5
	1
	ASQRT_DEP_FP
	4’hd
	FP16
	Same as ASQRT_INT_FP, except that the next instruction can NOT be fetched UNTIL the current instruction completes.
OPCODE[5]=1
	5
	0.2
	EXP_IND_FP
	4’he
	FP16
	Take the exponential function of the value in GPR(RS1). Data in GPR(RS1) can be positive, negative or zero.
Procedure: Refer to section 4.3.1 for details


Fetch the next instruction BEFORE the current instruction  completes.
OPCODE[5]=0


Note: 
                                                               1. If data in GPR(RS1) has bias = (-15), set the IMM field to 0.
                                                               2. If data in GPR(RS1) has bias  (-15), signed value IMM[5:0] will be used to adjust EB. Set IMM[5:0] as follow: 
                                                               3. The default bias for exponential output is  (-15), signed value IMM[11:6] can be used to adjust output EB. Set IMM[11:6] as follow:     [cm][cn]
                                                               4. IMM[15:0] should NOT be specified as a NaN number (16’h8000). Otherwise, the output would be NaN.


Bias adjustment may involve rounding operations. Two special cases for rounding near zero when underflow occurs due to exponent adjustment. 
                                                                  1. After adjustment, exp_fp16 = -1, fraction_fp16 > 1, it should round up (exp_fp16=0, fraction_fp16=1), rather than round to 0.
                                                                  2. After adjustment, exp_fp16 = 0, the input fraction = 0, it should round up (exp_fp16=0, fraction_fp16=1), rather than round to 0.
                                                                  3. If input is NaN (0x8000), the output will be NaN (0x8000)
	3


	1
	EXP_DEP_FP
	4’he
	FP16
	Same as EXP_IND_FP, except that the next instruction can NOT be fetched UNTIL the current instruction completes.
OPCODE[5]=1
	3
	0.33
	RECIPI_SQRT_IND_FP
	4’hf
	FP16
	Take the reciprocal square root of the value in GPR(RS1) through LUT. Data in GPR(RS1) has to be non-negative.


Procedure: Refer to section 4.4.3 for details


Fetch the next instruction BEFORE the current instruction  completes.
OPCODE[5]=0


Note: 
                                                                  1. IF IMM[15]==0, choose LUT_0; Otherwise choose LUT_1
                                                                  2. IMM[5:0] is the signed EB adjustment. Section 4.4.3.1 shows how it should be set.
                                                                  3. If GPR(RS1) is negative, the result is NaN
                                                                  4. If GPR(RS1) is zero, The result will be the value programmed in the LUT table (special entry which stores the data corresponding to input equal to 0)
                                                                  5. If GPR(RS1) is negative or zero, an interrupt will be triggered. Negative value or zero will have different interrupts. 
	5
	1-
	RECIPI_SQRT_DEP_FP
	4’hf
	FP16
	Same as RECIPI_SQRT_INT_FP, except that the next instruction can NOT be fetched UNTIL the current instruction completes.
OPCODE[5]=1
	5
	0.2
	Table 6.6: I-type OP-IMM-FP instruction for FP16 operation
________________


Note: FP16 Division can be supported in ALU through a combination of instructions, for example, reciprocal and multiplication instructions.
________________
                                                                  6. R-type Enhanced OP-INT Instruction for INT16
Some applications need to generate integer values outside the range which can be represented by INT16. For example, ALU is being used to compute memory addresses and the memory space can be as big as 64MB, which needs 26-bits. The enhanced OP-INT instructions (both R-type and I-type) are introduced to support generation of integer values beyond INT16. With the combination of new instructions discussed in this section, INT32, INT48 or INT64 results can be generated. A single-bit carry-register is established to store the carry-out data from the lower 16-bits addition to the higher 16-bits addition. 
Table 6.7 shows the enhanced R-type OP-INT instruction for INT16 operations. OPCODE = x_11110. 


	Funct4
	Data Type
	Function
	Pipe Stages
	Data / cyc
	ENH_SIGN_EXT
	0000
	INT16
	Extend the sign of the INT16 data in GPR(RS1) and convert it to a signed INT32 format. The upper 16 bits of INT32 are stored in GPR(RD).


Optionally clear the single bit carry-register 


                                                                  * Funct7==0, don’t clear carry-register
                                                                  * Funct7==1, clear carry-register.


Note:
                                                                  1. RS1 has to be GPR, can NOT be INFIFO, nor NN_REG, because the result has to occupy two GPR locations.
	1
	1
	ENH_ADD_W_SATURATION
	0001
	INT16
	Signed Summation RD = signed(GPR(RS1)) + signed(GPR(RS2)) + Carry


Note: Regular signed INT16 summation, msb in GPR(RS1) and GPR(RS2) are signed bit. Carry is from the special 1-bit carry-register. If the result is beyond the range which can be represented by INT16, detect underflow or overflow.


After execution of this instruction, the carry-register will be cleared automatically. 
	1
	1
	ENH_ADD_WO_SATURATION
	0010
	INT16
	Unsigned Summation {Carry_out, GPR(RD)} = GPR(RS1) + GPR(RS2) + Carry_in[co][cp][cq]


Note: 
                                                                  1. Unsigned INT16 summation, with both Carry_in and Carry_out. Carry_in is from Carry-register, and Carry_out will be stored in the Carry-register. msb in GPR(RS1) and GPR(RS2) are NOT signed bit  
                                                                  2. No overflow detection based on Carry out.
	1
	1
	Table 6.7: R-type Enhanced OP-INT Instructions for INT16
                                                                  7. I-type Enhanced OP-IMM-INT Instruction for INT16
Table 6.8 shows the enhanced I-type OP-INT instruction for INT16 operations. OPCODE = x_01010. 


	Funct4
	Data Type
	Function
	Pipe Stages
	Data / cyc
	ENH_ADDI_W_SATURATION
	0001
	INT16
	Signed Summation RD = signed(GPR(RS1)) + signed(IMM) + Carry


Note: 
                                                                  1. Regular signed INT16 summation, msb in GPR(RS1) and IMM are signed bit. Carry is from the special 1-bit carry-register. If the result is beyond the range which can be represented by INT16, detect underflow or overflow. After execution of this instruction, the carry-register will be cleared automatically. 
                                                                  2. If IMM is a signed value, 16-bits IMM should be loaded to GPR first, convert it into 32-bits integer by sign extension  (ENH_SIGN_EXT)  
	1
	1
	ENH_ADDI_WO_SATURATION
	0010
	INT16
	Unsigned Summation {Carry_out, GPR(RD)} = GPR(RS1) + IMM + Carry_in


Note: 
                                                                  1. Regular unsigned INT16 summation, with both Carry_in and Carry_out. Carry_out will be stored in the Carry-register. msb in GPR(RS1) and IMM are NOT signed bit
                                                                  2. No overflow detection based on Carry out.
                                                                  3. If IMM is a signed value, 16-bits IMM should be loaded to GPR first, convert it into 32-bits integer by sign extension  (ENH_SIGN_EXT)
	1
	1
	Table 6.8: I-type Enhanced OP-INT Instructions for INT16
                                                                  8. I-type LOAD Instruction
LOAD Instruction can perform the following operations
                                                                  * Pop one data from INFIFO and load it into GPR.
                                                                  * Pop one data from  INFIFO, adjust the exponent and load it into GPR.
                                                                  * Pop one data from INFIFO and load it into OUTFIFO directly.
                                                                  * Conditional load (either INT16 or FP16): pop data from INFIFO, if data is larger than the IMM field in the instruction or data in GPR(RS1), then load it to GPR. This is useful to screen data, e.g. screening NMS data based on the confidence level.
                                                                  * Conditional load (either INT16 and FP16): pop data from INFIFO, if data is less than or equal to the IMM field in the instruction or data in GPR(RS1), then load it to GPR(RD).
                                                                  * Load data in NN-Reg into GPR. This is useful to move data from one ALU to another for processing.
Table 6.9 shows the I-type LOAD instruction for INT16 / FP16 operations. OPCODE = 0_0000_1.


	Funct4
	Data Type
	Function
	Pipe Stages
	Data / cyc
	LOAD_INFIFO[cr][cs][ct][cu]
	0000
	INT16 / FP16
	Pop INFIFO data and load it to RD.


RS1: Ignored.


Note: RD can be FP16 GPR, OUTFIFO, NN_LEFT or NN_RIGHT.
	1
	1
	LOAD_IMM
	0001
	INT16 / FP16
	Load IMM data into GPR(RD)


RS1: Ignored


Note: RD can be FP16 GPR, OUTFIFO, NN_LEFT or NN_RIGHT.
	1
	1
	LOAD_GT_IMM
	0010


	INT16
	Conditional Load.
Pop INFIFO data. If data > IMM, move data to GPR(RD). If condition is NOT met, data will NOT be loaded into GPR(RD)[cv][cw]


Note: RD in this instruction can NOT be OUTFIFO, nor NN_REG.
	1
	1
	LOAD_LE_IMM
	0011
	INT16
	Conditional Load.
Pop INFIFO data. If data <= IMM, move data to GPR(RD).  If the condition is NOT met, data will NOT be loaded into GPR(RD).


Note: RD in this instruction can NOT be OUTFIFO, nor NN_REG.
	1
	1
	LOAD_GT_RS
	0100
	INT16
	Conditional Load for INT16.
If INFIFO data >  GPR(RS1), move INFIFO data to GPR(RD).  If the condition is NOT met, data will NOT be loaded into GPR(RD). GPR(RS1) should be INT16


Note: 
                                                                  1. RS1 in this instruction can NOT be INFIFO
                                                                  2. RD in this instruction can NOT be OUTFIFO, nor NN_REG.
	1
	1
	LOAD_LE_RS
	0101
	INT16
	Conditional Load.
Pop INFIFO data. If data <= data in GPR(RS1), move data to GPR(RD).  If the condition is NOT met, data will NOT be loaded into GPR(RD)


Note: 
                                                                  1. RS1 in this instruction can NOT be INFIFO.
                                                                  2. RD in this instruction can NOT be OUTFIFO, nor NN_REG.
	1
	1
	LOAD_GT_IMM_FP[cx][cy]
	0110
	FP16
	Conditional Load for FP16
Pop INFIFO data. If data > IMM, move INFIFO data to GPR(RD).  If the condition is NOT met, data will NOT be loaded into GPR(RD)


RS1: Ignored


Note: RD in this instruction can NOT be OUTFIFO, nor NN_REG.
	1
	1
	LOAD_LE_IMM_FP
	0111
	FP16
	Conditional Load for FP16
Pop INFIFO data. If data <= IMM, move INFIFO data to GPR(RD).  If the condition is NOT met, data will NOT be loaded into GPR(RD)


RS1: Ignored


Note: RD in this instruction can NOT be OUTFIFO, nor NN_REG.
	1
	1
	LOAD_GT_RS_FP
	1000
	FP16
	Conditional Load for FP16
Pop INFIFO data. If data > GPR(RS1), move data to GPR(RD). If the condition is NOT met, data will NOT be loaded into GPR(RD)


Note: 
                                                                  1. RS1 in this instruction can NOT be INFIFO
                                                                  2. RS1 in this instruction can NOT be ALU_ID, or CNTR_REG
                                                                  3. RD in this instruction can NOT be OUTFIFO, nor NN_REG.
	1
	1
	LOAD_LE_RS_FP
	1001
	FP16
	Conditional Load for FP16
If INFIFO data <= GPR(RS1), move INFIFO data to GPS(RD).  If the condition is NOT met, data will NOT be loaded into GPR(RD)


Note:
                                                                  1. RS1 can NOT be INFIFO, nor ALU_ID, nor CNTR_REG
                                                                  2. RS1 can be FP16 GPR, NN_REG, ZERO
                                                                  3. RD in this instruction can NOT be OUTFIFO, nor NN_REG.
	1
	1
	LOAD_NN
	1010
	INT16 / FP16
	Load data in NN register into GPR(RD)


RS1 and RS2 are ignored.
	1
	1
	LOAD_CNTR
	1011
	INT16
	Load data in Counter register into GPS(RD)
	1
	1
	LOAD_LUT_H_IND
	1100
	INT16
	Use lower 8bits in GPR(RS1) as index to access LUT and store upper 16 bits of LUT data in GPR(RD). 


IMM[15]: If IMM[15]==0, choose LUT_0; otherwise choose LUT_1.


IMM[14]: If IMM[14]==1, access LUT entry 256 and store the upper 16 bits into GPR(RD).  If IMM[14]==0, access LUT entries from 0 to 255.


OPCODE[5]=0
Fetch the next instruction BEFORE the current instruction completes.


Note:
                                                                  1. RS1 in this instruction can NOT be NN_REG
                                                                  2. RS1 can be GPR, INFIFO, CNTR_REG, ZERO and ALU_ID
	3
	1
	LOAD_LUT_H_DEP
	1100
	INT16
	Same as instruction LOAD_LUT_H_IND, except that the next instruction can NOT be fetched UNTIL the current instruction completes.
OPCODE[5]=1
	3
	0.33
	LOAD_LUT_L_IND
	1101
	INT16
	Use lower 8bits in GPR(RS1) as index to access LUT and store lower 16 bits of LUT data in GPR(RD). 


IMM[15]: If IMM[15]==0, choose LUT_0; otherwise choose LUT_1.


IMM[14]: If IMM[14]==1, access LUT entry 256 and store the lower 16 bits into GPR(RD).  If IMM[14]==0, access LUT entries from 0 to 255.


OPCODE[5]=0
Fetch the next instruction BEFORE the current instruction  completes.


Note: 
                                                                  1. RS1 in this instruction can NOT be NN_REG
                                                                  2. RS1 can be GPR, INFIFO, CNTR_REG, ZERO and ALU_ID
	3
	1
	LOAD_LUT_L_DEP
	1101
	INT16
	Same as instruction LOAD_LUT_L_IND, except that the next instruction can NOT be fetched UNTIL the current instruction completes.
OPCODE[5]=1
	3
	0.33
	Table 6.9: I-type LOAD instruction for INT16 / FP16 operation.
________________


Pending Items: For ALUs on the left-most and right-post positions, what is the padding rule to move data to NN-Reg?
________________


                                                                  9. R-type STORE Instruction
STORE Instruction can perform the following operations
                                                                  * Store data (INT16 or FP16) in GPR(RS1) / INFIFO / NN_REG / CNTR_REG into OUTFIFO or GPR(RD). For FP16, exponent adjustment can be enabled in the same instruction.
                                                                  * Conditional Store: If data in GPR(RS2) is 1, perform STORE. Otherwise NOP.
                                                                  * Store data in GPR(S1) into NN-Reg in the nearest left or right ALU.
                                                                  * Store data in INFIFO into NN-Reg in the nearest left or right ALU.
                                                                  * Store data in NN-Reg into NN-Reg in the nearest left or right ALU.
Table 6.10 shows the R-type STORE instruction for INT16 / FP16 operations. OPCODE = 0_1000_1


	Funct4
	Data Type
	Function
	Pipe Stages
	Data / cyc
	STORE_REG
	0000
	INT16 / FP16
	Transfer data from the source to the destination.
The source specified by RS1 can be INFIFO, NN_REG, ZERO, GPR(RS1), CNTR_REG, and ALU_ID.
The destination specified by RD can be OUTFIFO, GPR(RD), Left or Right NN_REG


RS2 is ignored.


RSS[0] is used to distinguish between FP16 and INT16 data types, as their NaN handling differs.
                                                                  1. If RSS[0]=0, it is FP16 data
                                                                  2. If RSS[0]=1, it is INT16 data


Funct7[5:0] is a signed exponent adjustment value if source data is FP16. Funct7[5:0] is added to the exponent of data from source. This is equivalent to EB adjustment. Funct7[5:0] should be set to 0 if no exponent adjustment is needed.


Two special cases for rounding near zero when underflow occurs due to exponent adjustment. 
                                                                  1. After adjustment, exp_fp16 = -1, fraction_fp16 > 1, it should round up (exp_fp16=0, fraction_fp16=1), rather than round to 0.
                                                                  2. After adjustment, exp_fp16 = 0, the input fraction = 0, it should round up (exp_fp16=0, fraction_fp16=1), rather than round to 0.


Note: 
                                                                  1. If data specified by RS1 is INT16, Funct7[5:0] should be set to 0, because no exponent adjustment is applied to INT16.
	1
	1
	COND_STORE_REG
	0001
	INT16 / FP16
	Conditional store data in GPR: If the condition is true, store data in GPR(RS1) into GPR (RD=0_xxxx); If the condition is not true, the original value in GPR(RD) is untouched.


GPR(RD) = (Condition)? GPR(RS1) : No-Op


The condition is true under two circumstances
                                                                  1. IF ((Funct7[6]==0) && (GPR(RS2)==0)), the condition is true
                                                                  2. IF ((Funct7[6]==1) && (GPR(RS2)!=0)), the condition is true 


RS1 and RS2 can be INFIFO, or NN-Reg, ALU_ID or CNTR-Reg, or GPRs, or ZERO. 
RD can NOT be OUTFIFO or NN-Reg


RSS[0] is used to distinguish between FP16 and INT16 data types, as their NaN handling differs.
                                                                  1. If RSS[0]==0, it is FP16 data
                                                                  2. If RSS[0]==1, it is INT16 data


Funct7[5:0] is a signed exponent adjustment value, which is added into the exponent of data from GPR(RS1). This is equivalent to EB adjustment. Funct7[5:0] should be set to 0 if no exponent adjustment is needed.


One special case for rounding near zero when underflow occurs due to exponent adjustment. For example, after adjustment, exp_fp16 = -1, fraction_fp16 > 1, it should round up (exp_fp16=0, fraction_fp16=1), rather than round to 0.


Note: 
                                                                  1. The reason that RD can NOT be OUTFIFO or NN-Reg is because the condition may or may not be met. When the condition is not met, the original value in GPR(RD) should be untouched.
                                                                  2. If data specified by RS1 is INT16, Funct7[5:0] should be set to 0, because no exponent adjustment is applied to INT16.
	1
	1
	STORE_SHIFT_REG
	0010
	INT16 / FP16
	Special instruction to transfer data from the sources (INFIFO or NN_REG) to two destinations (OUTFIFO and NN_REG in the nearest neighbor).


RS1: INFIFO
RS2: NN_REG
RD:   NN_REG (left or right).


OUTFIFO always implies a secondary destination.


FUNCT7[6:0] and ALU_ID[6:0] will determine which source to use. 
IF (FUNCT7[6:0] == ALU_ID[6:0]), data from INFIFO is the source data; Otherwise, NN_REG is the source data. 


RSS[0] is used to distinguish between FP16 and INT16 data types, as their NaN handling differs.
                                                                  1. If RSS[0]=0, it is FP16 data
                                                                  2. If RSS[0]=1, it is INT16 data


Note: No EB adjustment in this instruction.


This instruction was created to support Conv1D, here is the Conv1D support proposal.
	1
	1
	Table 6.10: R-type STORE instruction for INT16 / FP operation.
                                                                  10. R-type SEA Instructions For SoftMax
SEA instructions are R-type FP16.
Table 6.11 shows the details of SEA instructions. OPCODE = x_10010. 
Each SEA operation will take multiple cycles. Some operations depend on the data from the previous instruction (e.g. exponential function  depends on the result of ), some other operations do not depend on the data from the previous instruction (e.g. streaming in elements x in a tile continuously, and generating ). SEA instructions will support both dependent and independent operations.
________________


Note: It is expected that SEA instructions will NOT interleave with other VPU instructions. That means other VPU instructions which may use the results of SEA instruction will be invoked ONLY after all the data in a tile are processed by SEA instructions.
________________




	FUNCT4 
	OPCODE[5]
	Data Type
	Function
	Pipe Stages
	Data / cyc
	SEA_SCALING_GEN_DEP
	4’h0 
	1’b1
	FP16
	Several operations will be conducted by this instruction, described in the section 5.2.2


For details on each data format, refer to Figure 5.4.


IF (FUNCT7[6]==0): Ignore the value in m_partial register, load input tile m value (maximum value) into m_partial register. The difference between new and prior m value is set to 0. This will effectively clear the data left from the previous run.


IF (FUNCT7[6]==1): Don’t ignore the value in m_partial register. Update m_partial register and compute scaling factor in normal way.


FUNCT7[5:0] should be set as follow: , where  is the bias of the input. This will enable the use of the SEA function even when the input doesn't have an EB value of -15, despite the fact that the exponential function in SEA is designed to assume an input with EB equal to -15.


RS1: RS1 is used to specify the source of the input data. Typically RS1 should specify INFIFO or GPR in ALU. If RS1==ZERO, the read value based on RS1 will be zero.
RS2: Ignored.
RD: The scaling result will be written into GPR(RD).


OPCODE[5]=1
Fetch the next instruction AFTER the current instruction completes.


Data Access Pipeline (ps: pipeline stage): 
                                                                  * PS0: 
                                                                  * RS1 read 
                                                                  * m_partial read 
                                                                  * m_partial write 
                                                                  *  PS4: 
                                                                  * RD write


Note: 
                                                                  1. If the next instruction has to use the scaling factor in GPR, the next instruction has to wait until the scaling factor is written into GPR. OPCODE[5] should be set to 1.
                                                                  2. RS1 can specify INFIFO, ZERO, NN_REG, or GPR, but NOT ALU_ID which is INT16.
                                                                  3. RD can be OUTFIFO, or GPR, but can NOT be NN_REG.
	5
	0.2
	SEA_SCALING_GEN_IND
	4’h0
	1’b0
	FP16
	Same as SEA_SCALING_GEN_DEP, except for
OPCODE[5]=0
Fetch the next instruction BEFORE the current instruction completes.


Note: 
                                                                  1. If the next instruction does NOT use the scaling factor in GPR, the next instruction does NOT have to wait until the scaling factor is written into GPR. OPCODE[5] should be set to 0. For example, one common application case is when the next instruction wants to generate exp(x-m), it can be issued immediately without waiting. Max value m can be updated in one cycle.
                                                                  2. RS1 can specify INFIFO, ZERO, NN_REG, or GPR, but NOT ALU_ID which is INT16.
                                                                  3. RD can be OUTFIFO, or GPR, but can NOT be NN_REG.
	5
	1
	SEA_SOFTMAX_SCALING_EB_GEN_DEP[cz][da][db][dc]
	4’h2 
	1’b1
	FP16
	Generate the scaling factor which can be expressed as , the scaling process becomes FP21 exponent adjustment. See sections 5.2.2.1 and 5.2.2.2, Refer to Figure 5.7 for the register assignment. 
IF (FUNCT7[6]==0): Ignore the value in m_partial register, load input tile m value (scaled maximum value) into m_partial register. The difference between new and prior m value is set to 0. This will effectively clear the data left from the previous run.
IF (FUNCT7[6]==1): Don’t ignore the value in m_partial register. Update m_partial register and compute scaling factor.
FUNCT7[5:0]: Specifies the EB of values in GPR(RS1) and GPR(RS2).
RS1: RS1 specifies the new tile max value , after rounding, FP16 format.  Typically RS1 should specify FP16 GPR in ALU. If RS1==ZERO, the read value based on RS1 will be zero.
RS2[dd][de][df][dg][dh][di][dj][dk][dl]: RS2[dm] specifies the max value of previously processed tile. GPR(RS2) will be read and written in the same instruction.
RSS[dn][do][dp][dq][dr]: Store the scaled value of GPR(RS1).
          GPR(RSS) = 0.693147 x GPR(RS1).
RD: The scaling result (7-bit integer) will be written into GPR(RD).
OPCODE[5]=1
Fetch the next instruction AFTER the current instruction completes.
Data Access Pipeline (ps: pipeline stage): 
                                                                  * PS0: 
                                                                  * RS1 read
                                                                  * RS2 read
                                                                  * RSS read 
                                                                  * m_partial read 
                                                                  * RS2 write
                                                                  * m_partial write 
                                                                  *  PS4: 
                                                                  * RD write
Note: 
                                                                  1. The read from and write to GPR(RS2) both occur in the first cycle, the new write data to GPR(RS2) will be available in the second cycle, and should not impact GPR(RS2) read operation. The write to GPR(RD) occurs in the fifth cycle, matching the write latency of the instruction SEA_SCALING_GEN.
                                                                  2. If the next instruction has to use the scaling factor in GPR, the next instruction has to wait until the scaling factor is written into GPR.
                                                                  3. RS1 can specify INFIFO, ZERO, NN_REG, or GPR, but NOT ALU_ID or CNTR_REG which are INT16.
                                                                  4. RS2 can ONLY specify GPR, but can not be INFIFO, OUTFIFO, ZERO, NN_REG, or CNTR_REG, because GPR(RS2) is both source and destination in this instruction, see Figure 5.7.
                                                                  5. RSS can ONLY specify GPR.
                                                                  6. RD can be OUTFIFO, or GPR, but can NOT be NN_REG. RD and RS2 can NOT be the same, because they store different results.
                                                                  7. If the result is NaN, the entire 16-bit in GPR(RD) will be used to represent NaN, rather than 7-bit.
	5
	0.2
	SEA_SOFTMAX_SCALING_EB_GEN_IND
	4’h2 
	1’b0
	FP16
	Same as SEA_SOFTMAX_SCALING_EB_GEN_DEP, except that the next instruction can NOT be fetched UNTIL the current instruction completes.
OPCODE[5]=0
	5
	1
	SEA_SOFTMAX_EXP_GEN_DEP
	4’h1
	1’b1
	FP16
	Several operations will be conducted by this instruction, described in the section 5.2 both exponential data and accumulation of exponential data are generated.


For details on each data format, refer to Figure 5.4.


If (FUNCT7[6]==0): Set the read data based on RS2 to zero during accumulation. This will effectively clear the accumulated data left from the previous run.


If (FUNCT7[6]==1): Use the read data based on RS2 as is in accumulation.


FUNCT7[5:0] should be set as follow: 
, where  is the bias of the input. This will enable the use of the SEA function even when the input doesn't have an EB value of -15, despite the fact that the exponential function in SEA is designed to assume an input with EB equal to -15.


RS1: RS1 is used to specify the source of the input data. Typically RS1 should specify INFIFO in ALU. If RS1==ZERO, the read value based on RS1 will be zero.
RS2: RS2 is used to specify the source of the 8-entry FP21 dedicated accumulator (NOT regular GPR, discussed in the section 3.3). The accumulated result is written back to the same accumulation buffer. 
RD: The exponential data will be written to the destination specified by RD. Typically RD should specify OUTFIFO


OPCODE[5]=1
Fetch the next instruction AFTER the current instruction completes.


Data Access Pipeline (ps: pipeline stage): 
                                                                  * PS0: 
                                                                  * RS1 read
                                                                  * m_partial read 
                                                                  * PS4:
                                                                  * RS2 read
                                                                  * RD write
                                                                  *  PS6: 
                                                                  * RS2 write
Note: 
                                                                  1. The total pipeline latency is 7 cycles when including accumulation operation. The final accumulation result will be available 7 cycles later.
                                                                  2. RS1 can specify INFIFO, ZERO, NN_REG, but NOT ALU_ID.
                                                                  3. RD can be OUTFIFO, or GPR, but can NOT be NN_REG.
	7
	1/7
	SEA_SOFTMAX_EXP_GEN_IND
	4’h1
	1’b0
	FP16
	Same as SEA_SOFTMAX_EXP_GEN_DEP, except that the next instruction can NOT be fetched UNTIL the current instruction completes.
OPCODE[5]=1


Note: 
                                                                  1. By default, SoftMax should use this instruction to generate exponential data, one per cycle, to reduce latency.
                                                                  2. Only three cycles (accumulation) out of seven have data dependence, therefore it needs three FP21 GPRs (specified by RS2) to eliminate data dependence. 
	7
	1
	SEA_STORE_MAX_REG
	4’h3
	1’b0
	FP16
	Move data in m_partial register into another register specified by RD.


RD can be GPR, or OUTFIFO, but NOT NN-Reg for simplicity.


Data Access Pipeline (ps: pipeline stage): 
                                                                  * PS0: 
                                                                  * m_partial read 
                                                                  * RD write




Note: This instruction can output the value in m_partial, and can be useful in Flash-Decoding when merging results from different splits, because the scaling is required in this process.
	1
	1
	Table 6.11: SEA instructions
________________


Note: 
                                                                  1. The SEA function comprises eight dedicated FP21 registers, aligning with the latency of FP21 adders. The instruction SEA_SOFTMAX_EXP_GEN_IND can employ a round-robin fashion to specify RS2 (e.g., 0, 1, 2, 0, 1, 2, etc.), thus avoiding data hazards. This allows for the feeding of new input data every cycle, ensuring optimal performance at full speed. The fourth FP21 register can be used for hierarchical split accumulation, e.g. add data in the registers 0, 1 and 2, put them into the fourth register, clear values in registers 0, 1 and 2, then continue accumulating.
                                                                  2. Instruction EXTD_FP_ACCUM_DEP (Table 6.12) can be used to aggregate results from three dedicated FP21 registers in SEA function. Instruction EXTD_FP_TO_FP16 (Table 6.12) can be used to move the final Accumulation result from a dedicated FP21 register to general GPR (FP16).
________________


                                                                  11. R-type Extended Floating Point Instructions
Extended floating point instructions are R-type FP21.
Table 6.12 shows the details of Extended FP Accumulation instructions. OPCODE = x_10011. 


	FUNCT4 
	OPCODE[5]
	Data Type
	Function
	Pipe Stages
	Data / cyc
	EXTD_FP_ACCUM_IND
	4’h0 
	1’b0
	FP21
	Accumulation in FP21. 



Fetch the next instruction BEFORE the current instruction completes.
OPCODE[5]=0


Note: 
                                                                  1. The result has to be stored into the 8-entry FP21 GPRs dedicated for Extended FP Accumulation. That means RD can ONLY be 0, 1,  2,  3, …, 7. It can NOT be OUTFIFO, nor NN_REG
                                                                  2. The source of operands can be general GPRs (16-bits) or dedicated GPRs (21-bits). 
                                                                  1. FUNCT7==7’h0, both RS1 and RS2 specify general GPRs (16-bits). They can be INFIFO, ZERO, or NN_REG. They can not be an integer data source ALU_ID, nor CNTR_REG.
                                                                  2. FUNCT7==7’h1, RS1 specifies general GPR (16-bits), RS2 specifies dedicated FP21 GPR(21-bits). RS2 = 0, 1, 2, …, 7.
                                                                  3. FUNCT7==7’h2, both RS1 and RS2 specify dedicated FP21 GPRs
                                                                  3. Initialization of the dedicated GPR can be done by setting FUNCT==7’h0, and both RS1/RS2 specify ZERO.
	3
	1
	EXTD_FP_ACCUM_DEP
	4’h0
	1’b1
	FP21
	Same as EXTD_FP_ACCUM_IND, except that the next instruction can NOT be fetched UNTIL the current instruction completes.
OPCODE[5]=1
	3
	0.33
	EXTD_FP_TO_FP16
	4’h1
	1’b0
	FP21
	Converting FP21 to FP16 with bias adjustment and rounding. FUNCT7[5:0] represents the signed adjustment value for the exponent. The output exponent is Exp+FUNCT7[5:0], where Exp is the original exponent of the data in dedicated GPR(RS1) .


The rounding scheme follows IEEE standard rounding policy (e.g. round-to-even). Three special cases for rounding near zero:
                                                                  1. When exp_fp16 = -1, fraction_fp21[13:0] > 16, it should round up (exp_fp16=0, fraction_fp16=1), rather than round to 0.
                                                                  2. When exp_fp16 = 0, fraction_fp21[13:0] < 16, it should round up (exp_fp16=0, fraction_fp16=1), rather than round to 0.
                                                                  3. When final exp_fp16=0, the fraction rounding introduces exponent increment, exp_fp16 would be -1 without exponent increment. After fraction rounding, the fraction is 0.  In this case it should round up to exp_fp16=0, fractioin_fp16=1.


If exp_fp16=0 and fraction_fp21[13:0] >= 16, no special handling is required, because fp16 value will not be rounded to zero.


Note:
                                                                  1. GPR(RS1) has to be the dedicated 8-entry GPRs, RS1 = 0, 1, 2, 3, 4, 5, 6, or 7. Or GPR(RS1) can be ZERO.
                                                                  2. The destination specified by RD has to be the general purpose GPRs (16 entries), or OUTFIFO, NN_REG.
	1
	1
	EXTD_FP_MUL_IND
	4’h2
	1’b0
	FP17
	Multiplication produces results in FP17. 



Fetch the next instruction BEFORE the current instruction completes.
OPCODE[5]=0


Note:
                                                                  1. Exponent bias adjustment has been included in this operation. One hardware bias is added back into the exponent of the product. Funct7[5:0] is the exponent signed adjustment value. The exponent of the product is: Exp(RS1) + Exp(RS2) + Funct7[5:0]
                                                                  2. Both GPR(RS1) and GPR(RS2) have to be FP16 values. RS1/RS2 can specify GPR, INFIFO, or NN_REG, but can not specify integer data source, e.g. ALU_ID, or CNTR_REG.
                                                                  3. The result is in FP17, and GPR(RD) has to be stored in a dedicated FP21 register.
	2
	1
	EXTD_FP_MUL_DEP
	4’h2
	1’b1
	FP17
	Same as EXTD_FP_MUL_IND, except that the next instruction can NOT be fetched UNTIL the current instruction completes.
OPCODE[5]=1
	2
	0.5
	EXTD_FP_MAC_IND
	4’h3
	1’b0
	FP21
	MAC Operation: Multiplication in FP17 and Accumulation in FP21.





OPCODE[5]=0
Fetch the next instruction BEFORE the current instruction completes.


Data Access Pipeline (ps: pipeline stage): 
                                                                  * PS0: 
                                                                  * RS1 read
                                                                  * RS2 read 
                                                                  * PS1:
                                                                  * RD read
                                                                  *  PS3: 
                                                                  * RD write
Note:
                                                                  1. Exponent bias adjustment has been included in this operation. One hardware bias is added back into the exponent of the product. Funct7[5:0] is the exponent signed adjustment value. The exponent of the product is: Exp(RS1) + Exp(RS2) + Funct7[5:0]
                                                                  2. Both GPR(RS1) and GPR(RS2) have to be FP16 values. RS1/RS2 can specify GPR, INFIFO, or NN_REG.
                                                                  3. RD not only specifies the destination of the results, but also specifies one operand used in MAC operation. RD has to be the dedicated FP21 register, can NOT be GPR, INFIFO, NN_REG, nor OUTFIFO.
	4
	1
	EXTD_FP_MAC_DEP
	4’h3
	1’b1
	FP21
	Same as EXTD_FP_MAC_IND, except that the next instruction can NOT be fetched UNTIL the current instruction completes.
OPCODE[5]=1
	4
	0.25
	EXTD_FP_DYN_EXP_ADJ
	4’h4
	1’b0
	FP21
	Dynamic exponent adjustment.





RS1: FP21 GPR Input source. It can be ZERO.


RS2: FP16 GPR, INFIFO, NN-reg, ZERO


RD: FP21 GPR.


Note: 
                                                                  1. If overflow is detected, e.g. result’s exponent is larger than 63, the result will saturate at +/1 max value.
                                                                  2. If underflow is detected, e.g. result’s exponent is smaller than 0, the result will be set to 0.[ds][dt][du] Two special cases for rounding near zero when underflow occurs due to exponent adjustment. 
a). After adjustment, exp_fp21 = -1, fraction_fp21 > 1, it should round up (exp_fp21=0, fraction_fp21=1), rather than round to 0.
b). After adjustment, exp_fp16 = 0, the input fraction = 0, it should round up (exp_fp21=0, fraction_fp21=1), rather than round to 0
	1
	1
	Table 6.12: Extended FP instructions
                                                                  12. R-type SEA Instructions For LayerNorm Variance
Table 6.13 shows the details of SEA instructions for LayerNorm variance calculation. OPCODE = x_10100. 


	FUNCT4
	OPCODE[5]
	Data Type
	Function
	Pipe Stages
	Data / cyc
	SEA_LAYRN_LOAD_MEAN
	4’h0
	1’b0
	FP16
	Load value in GPR(RS1) (general GPR) into SEA function register m_partial (shown in Figure 5.10).
For LayerNorm variance calculation, value in GPR(RS1) should be the mean of a vector.


For details on each data format, refer to Figure 5.10.


RS1: RS1 is used to specify the source of the input data.
RS2: Ignored.
RD: Ignored.


Data Access Pipeline (ps: pipeline stage): 
                                                                  * PS0: 
                                                                  * RS1 read
                                                                  * m_partial write 


Note: RS1 can specify INFIFO, ZERO, NN_REG.
	1
	1
	SEA_LAYRN_ACCUM_IND
	4’h1
	1’b0
	FP16/FP21
	Several operations will be conducted by this instruction, subtraction, square and accumulation, described in the section 5.5


For details on each data format, refer to Figure 5.10.


If (FUNCT7[6]==0): Set the read data based on RS2 to zero during accumulation. This will effectively clear the accumulated data left from the previous run.


If (FUNCT7[6]==1): Use the read data based on RS2 as is in accumulation.


FUNCT7[5:0] is used to specify signed bias adjustment in square function. The exponent of the square product has 6-bit, and is: 2xExp(RS1) + Funct7[5:0] 


RS1: RS1 is used to specify the source of the input data.
RS2: RS2 is used to specify the source of the 8-entry FP21 dedicated accumulator (NOT regular GPR, discussed in the section 3.3). The accumulated result is written back to the same accumulation buffer. RS2 can NOT be ZERO.
RD: Ignored.


Fetch the next instruction BEFORE the current instruction completes.
OPCODE[5]=0


Data Access Pipeline (ps: pipeline stage): 
                                                                  * PS0: 
                                                                  * RS1 read
                                                                  * m_partial read 
                                                                  * PS4:
                                                                  * RS2 read
                                                                  *  PS4: 
                                                                  * RS2 write


Note: 
                                                                  1. Subtraction and square are conducted in FP16. The square output has FP17 format. Accumulation is conducted in F21. FP21 results are stored in a dedicated FP21 register in the SEA function.
                                                                  2. No access to ALU_ID.
	7
	1
	SEA_LAYRN_ACCUM_DEP
	4’h1
	1’b1
	FP16/F21
	Same as SEA_LAYRN_ACCUM_IND,  except that the next instruction can NOT be fetched UNTIL the current instruction completes.
OPCODE[5]=1
	7
	1/7
	Table 6.13: SEA instructions for LayerNorm variance calculation
________________


Note: 
                                                                  1. The SEA function comprises eight dedicated FP21 registers, aligning with the latency of FP21 adders. The instruction SEA_LAYRN_ACCUM_IND can employ a round-robin fashion to specify RD (e.g., 0, 1, 2, 0, 1, 2, etc.), thus avoiding data hazards. This allows for the feeding of new input data every cycle, ensuring optimal performance at full speed. The fourth FP21 register can be used for hierarchical split accumulation, e.g. add data in the registers 0, 1 and 2, put them into the fourth register, clear values in registers 0, 1 and 2, then continue accumulating.
                                                                  2. Instruction EXTD_FP_ACCUM_DEP (Table 6.12) can be used to aggregate results from three dedicated FP21 registers in SEA function. Instruction EXTD_FP_TO_FP16 (Table 6.12) can be used to move the final Accumulation result from a dedicated FP21 register to general GPR (FP16).
________________


                                                                  13. Counter Register Operations
Table 6.14 shows the details of Counter Register Operations. 


	CNTR_CTRL
	Data Type
	Function
	Pipe Stages
	CNTR_CTRL_SET_TO_0
	2’b00
	INT16
	Set Counter Register value to 0
	1
	CNTR_CTRL_SET_TO_1
	2’b01
	INT16
	Set Counter Register value to 1
	1
	CNTR_CTRL_INCRM_CNTR
	2’b10
	INT16
	Increment Counter Register value by 1
	1
	CNTR_CTRL_NO_OP
	2’b11
	INT16
	Don’t change the Counter Register value.
	1
	Figure 6.14: Counter Control and Counter Register Operations
                                                                  14. No-Op Operation
No-Op is a very useful operation in many applications. There are many logically correct ways to implement it, such as two examples as follow
ADD  GPR1 <- GPR1, ZERO;                        (No-Op)
MULI GPR1 <- GPR1,1(FP16 constant);        (No-Op)
Another way is to invoke conditional store instruction (COND_STORE_REG, section 6.2.13), and ensure the condition is never met, shown below (RS2 is set to ZERO, and FUNCT7[6] is set to 1)
COND_STORE_REG  GPR1 <- GPR1, ZERO;        (No-Op)
The conditional store instruction is preferred over ADD or MUL instructions because it consumes less power, completes in a single cycle, and avoids dependency issues.
                                                                  21. Rules For Register Access And Instruction Sequences
To avoid errors, mainly data hazards or register access errors (e.g. write collisions), there are certain rules for register access and instruction sequences. The instruction sequence rules apply only to instructions which take multiple cycles to complete. Instructions that can be executed in one cycle are exempt from these sequence rules.It is worth noting that many register access errors are due to issues with the instruction sequence and can be resolved by proper instructions.
As discussed in section 6.2.1, bit OPCODE[5] is being used to indicate whether the subsequent instruction(s) has data dependence on the current instruction. This can be utilized by the compiler to resolve data hazards and register write collision issues. Instructions which take multiple cycles to complete typically have two versions, e.g. MUL_IND and MUL_DEP, both take two cycles to complete. MUL_IND has OPCODE[5] = 0, indicating that the subsequent instruction has no dependency on the current instruction, allowing it to start before the current instruction is completed.; MUL_DEP has OPCODE[5]=1, indicating the subsequent instruction has dependence on the current instruction, the subsequent instruction can not start until the current instruction completes. 
                                                                  15. Rules For Accessing VPU Registers 
VPU instructions have variable latency from 1 cycle to 7 cycles, and every instruction involves register read and write accesses. This section discusses the rules for accessing VPU registers to avoid access collision and data hazards.
                                                                  8. Rules For Accessing GPRs
In a back-to-back sequence of instructions, if instructions take more than one cycle to complete, the instructions must follow specific rules to avoid data hazards, such as reading data from the GPR before it's ready or write collisions in a destination GPR.
________________


Note: 
                                                                  1. It is worth noting that instructions requiring more than one cycle to complete just have to keep the source data stable for the first cycle, and the source data can be changed after the first cycle. This rule applies to all 16b instructions (FP16 and INT16). 
                                                                  2. This rule does NOT apply to composite instructions which involve multiple operations, e.g. SEA instruction for SoftMax or LayerNorm, or EXTD_FP_MAX_IND / EXTD_FP_MAX_DEP instructions.
________________




Case 1: The following back-to-back sequence of instructions is NOT legal due to read data hazard, because the instruction ADD reads GPR2 before the previous instruction MUL_IND writes the result into GPR2.
MUL_IND GPR2 <- GPR1, GPR2; // Takes two cycles to complete 
ADD GPR3 <- GPR1, GPR2; // Takes one cycle to complete
Case 2: The following back-to-back sequence of instructions is legal without read data hazard, because instruction ADD will not start before the previous instruction MUL_DEP completes and writes the result into GPR2.
MUL_DEP GPR2 <- GPR1, GPR2; // Takes two cycles to complete 
ADD GPR3 <- GPR1, GPR2; // Takes one cycle to complete
Case 3: The following back-to-back sequence of instructions is legal. Both instructions write into GPR3 and GPR4 at the same time.
MUL_IND GPR3 <- GPR1, GPR2; // Takes two cycles to complete 
ADD GPR4 <- GPR1, GPR2; // Takes one cycle to complete
Case 4: The following back-to-back sequence of instructions is NOT legal due to write collision, because both instructions write into GPR3 at the same time.
MUL_IND GPR3 <- GPR1, GPR2; // Takes two cycles to complete 
ADD GPR3 <- GPR1, GPR2; // Takes one cycle to complete
Case 5: The following back-to-back sequence of instructions is legal without write collision, because the second instruction (ADD) will not start before the first one (MUL_DEP) completes.
MUL_DEP GPR3 <- GPR1, GPR2; // Takes two cycles to complete 
ADD GPR3 <- GPR1, GPR2; // Takes one cycle to complete




                                                                  9. Rules For Writing To OUTFIFO
If a series of instructions set OUTFIFO as the destination, the VPU design guarantees that the data will be written to OUTFIFO in the exact order the instructions were issued. The following back-to-back sequence of instructions is legal
MUL_IND OUTFIFO <- GPR1, GPR2; // Takes two cycles to complete 
ADD OUTFIFO <- GPR1, GPR2; // Takes one cycle to complete
                                                                  10. Rules For Reading INFIFO
Each instruction can read INFIFO once, or twice (R-type instruction). For example, all the following instructions are legal
MUL_IND GPR2 <- INFIFO, INFIFO (legal)
MUL_IND GPR2 <- GPR1, INFIFO   (legal)
MUL_IND OUTFIFO <- INFIFO, GPR1(legal)
                                                                  11. Rules For Accessing NN-REG
Instructions should follow the same rules to access register NN-REG as they do for accessing GPRs. 
The same instruction can access NN-REG once or twice (R-type), as shown in following Case 1.
Case 1: The following examples show the same instruction accesses NN-REG once or twice. Both are legal. 
MUL_DEP GPR3 <- NN-REG, NN-REG; // Takes two cycles to complete 
ADD GPR3 <- GPR1, NN-REG; // Takes one cycle to complete


                                                                  12. Rules For Accessing FP21 Registers
FP21 registers (total 8 entries) are introduced for some special operations (e.g. SEA for SoftMax and LayerNorm, EXTD_FP_ACCUM, EXTD_FP_MUL and EXTD_FP_MAC). The instructions which access FP21 registers have different latency. In order to support the typical applications without complicating the design, FP21 registers can have only one write operation at any cycle. This means the compiler cannot interleave instructions that use the FP21 register as the destination without setting OPCODE[5] to 1 when transitioning between different types of instructions. The same type of instruction that uses the FP21 register as the destination can still be issued back-to-back, provided that there is no data dependence hazard. This is a typical use case in tensor applications. A few examples below illustrate legal and illegal use cases. 
Case 1: The following back-to-back sequence of three instructions is NOT legal (between the second and the third instruction) due to two reasons, one is the data hazard in the FP21_GPR2 register, the third instruction starts BEFORE the second instruction completes. The other issue is potential hardware resource collision, because both instructions (SEA_SOFTMAX_EXP_GEN_IND and EXTD_FP_MAC_IND) share the same multiplier hardware but have different latency. The issue can be fixed in the example shown in Case 2
SEA_SOFTMAX_EXP_GEN_IND OUTFIFO <- GPR1, FP21_GPR2; // Seven cycles to complete 
SEA_SOFTMAX_EXP_GEN_IND OUTFIFO <- GPR1, FP21_GPR2; // Seven cycles to complete
EXTD_FP_MAC_IND FP21_GPR2 <- GPR1, GPR2;    // Four cycles to complete
Case 2: The following back-to-back sequence of three instructions is legal without data hazard, and no hardware resource collision. The third instruction starts AFTER the second instruction completes.
SEA_SOFTMAX_EXP_GEN_IND OUTFIFO <- GPR1, FP21_GPR2; // Seven cycles to complete 
SEA_SOFTMAX_EXP_GEN_DEP OUTFIFO <- GPR1, FP21_GPR2; // Seven cycles to complete
EXTD_FP_MAC_IND FP21_GPR2 <- GPR1, GPR2;    // Four cycles to complete
Case 3: The following back-to-back sequence of three instructions is NOT legal due to a write collision in FP21_GPR2. The issue can be fixed in the example shown in Case 4.
EXTD_FP_MAC_IND FP21_GPR2 <- INFIFO, GPR2;  // Four cycles to complete 
EXTD_FP_MAC_IND FP21_GPR2 <- INFIFO, GPR2;  // Four cycles to complete
EXTD_FP_ACCUM_IND FP21_GPR2 <- GPR3, GPR4;  // Three cycles to complete
Case 4: The following back-to-back sequence of three instructions is legal without a write collision in FP21_GPR2.
EXTD_FP_MAC_IND FP21_GPR2 <- INFIFO, GPR2;  // Four cycles to complete 
EXTD_FP_MAC_DEP FP21_GPR2 <- INFIFO, GPR2;  // Four cycles to complete
EXTD_FP_ACCUM_IND FP21_GPR2 <- GPR3, GPR4;  // Three cycles to complete


                                                                  16. Rules For Instruction Sequences 
Instruction sequences need to follow certain rules to avoid data hazards, write collisions or out-of-order write operations.
                                                                  13. Rules To Avoid Data Dependence Hazards
If the current instruction needs more than one cycle to complete (e.g. two to five cycles), and one of the next few instructions (e.g. one to four instructions) depends on the result of the current instruction, the next few instructions can NOT be issued until the result of the current instruction is ready. The current instruction should set OPCODE[5]=1. For example:
                                                                  1. If the current instruction completes in two cycles, and the immediate next instruction will use the result of the current instruction, the next instruction can NOT be issued immediately, it has to wait for one cycle before issuing. 
                                                                  2. If the current instruction completes in five cycles, and one of the next four instructions will use the result of the current instruction, the next four instructions can NOT be issued until the current instruction completes. 
                                                                  1. Rules To Avoid Write Operation Hazards
If the current instruction needs more than one cycle to complete (e.g. two to five cycles), and one of the next few instructions (e.g. one to four instructions) share the same destination (RD) with the current instruction and its latency is shorter than the current instruction by more than 1 cycle,  the next few instructions can NOT be issued until the current instruction completes. The current instruction should set OPCODE[5]=1. This rule does NOT apply if the shared destination specified by RD is OUTFIFO, but does apply if the shared destination specified by RD is NN_REG. For example
                                                                  1. If the current instruction completes in three cycles and stores the result in GPR6, and the immediate next instruction completes in one cycle and will also write the result into GPR6, the next instruction can NOT be issued immediately, it has to wait until the current instruction completes.
                                                                  2. If the current instruction completes in five cycles, and one of the next four instructions shares the same destination (not including OUTFIFO as the destination) as the current instruction and its latency is less than four cycles, the next four instructions can NOT be issued until the current instruction completes.
Each instruction may take one, two, up to five cycles (except for SEA instruction) to complete. Up to five write operations may happen concurrently. In each cycle, there should be NO more than one write operation to put data into the same GPR location; writes into five different locations are allowed.
                                                                  2. Rules For SEA Instruction Sequences
Special instructions for SEA function (section 6.2.14 and 6.2.16) should NOT be interleaved with other non-SEA instructions, because the SEA function has a longer pipeline, and reuse some hardware resources of non-SEA instructions (e.g. multiplier). SEA instruction itself can be executed one per cycle as long as data dependence does not exist. Non-SEA instructions should not be executed until the SEA instruction completes. That means the last SEA instruction should always set OPCODE[5]=1.
                                                                  3. Rules For SEA_LAYRN Instruction Sequences
If the instruction sequence has a series of SEA_LAYRN_ACCUM[dv] instructions, followed by SEA_LAYRN_LOAD_MEAN instruction(s), and then by a non-SEA instruction, the last SEA_LAYRN_ACCUM instruction must set OPCODE[5]=1. This ensures that SEA_LAYRN_ACCUM completes before SEA_LAYRN_LOAD_MEAN begins. SEA_LAYRN_ACCUM takes 7 cycles to finish, while SEA_LAYRN_LOAD_MEAN only takes one cycle. If SEA_LAYRN_LOAD_MEAN starts before all SEA_LAYRN_ACCUM instructions complete, non-SEA instructions may begin before SEA_LAYRN_ACCUM instructions finish. This could lead to conflicts, as SEA_LAYRN_ACCUM reuses the same hardware resources as non-SEA instructions, such as the multiplier.    
                                                                  4. Rules For FP21 and FP16 Instruction Sequences
The extended FP instructions (as discussed in Section 6.2.15) and non-extended FP instructions should not be interleaved. This means that a new type of instruction (either extended or non-extended FP) cannot be issued until the previous instruction of a different type is completed. This is because if interleaving is allowed, scenarios like the following instruction sequence could occur: EXTD_MAC, ADD, EXTD_MUL. In such cases, both the first and the third extended FP instructions would attempt write operations to FP21 GPR simultaneously. The last instruction before transitioning to different types should always set OPCODE[5]=1
________________


Note: 
                                                                  1. As long as the bit OPCODE[5] is specified properly to indicate the existence of data dependence between consecutive instructions, VPU design will ensure the dependence to be handled automatically.
                                                                  2. Design guarantees NO more than one write operation to move data into OUTFIFO in one cycle by forcing each data to OUTFIFO will take 5-cycle latency in the write path, which is the longest latency of instruction (except for some SEA related instructions). This also implies that if an instruction can produce the result within one cycle and the destination is OUTFIFO, the result will not be forwarded to the OUTFIFO until five cycles later.
                                                                  3. SEA instructions SEA_SOFTMAX_EXP_GEN_DEP / SEA_SOFTMAX_EXP_GEN_IND take 7 cycles to complete. But they write their accumulation result into a dedicated FP21 register, rather than to GPR directly. It takes an instruction to move data from an FP21 register to a GPR (including OUTFIFO).
                                                                  4. Assertions and detections will be included in design. When the rules are violated, simulation will show errors, so error detection becomes possible.
________________




                                                                  7. Special Cases In Design
                                                                  1. Overflow And Underflow
WIP
                                                                  2. NaN Handling
NaN handling applies only to floating point operations (both FP16 and FP21) and not to INT16 operations.
FP16 NaN representation in Pyxis is: 16’h8000
FP21 NaN representation in Pyxis is: 21’h100000
                                                                  17. Basic Rules To Handle NaN
If the input data is NaN, VPU operations follow four basic rules
                                                                  1. In computing operations, if any operand is NaN, the final output will also be NaN.
                                                                  2. In logical comparison operations, if any operand is NaN, the comparison result will be false.
                                                                  3. In FP16 to INT16 data conversions, if the FP16 input data is NaN, the integer output will be 0.
                                                                  4. If an opcode is composed of multiple operations, each individual operation follows rules listed above. 
________________


Note: 
                                                                  1. For FP16 to INT16 data conversion when FP16 input data is NaN,  Python doesn’t handle it, shown as follows
>>> x = float('nan') 
>>> int(x) 
Traceback (most recent call last): 
  File "<stdin>", line 1, in <module> 
ValueError: cannot convert float NaN to integer
                                                                  2. The following is a piece of C++ code which supports VPU’s decision to map FP16 NaN to INT16 0.
foo.cpp 
#include <iostream> 
int main() 
{ 
  float NaN = 0.0 / 0.0; 
  std::cout << "fp32 NaN " << NaN << std::endl; 
  std::cout << "convert NaN to 16 bit signed " << static_cast<int16_t>(NaN) << std::endl;   
  std::cout << "convert NaN to 16 bit unsigned " << static_cast<uint16_t>(NaN) << std::endl; 
  return 0; 
} 
./foo 
fp32 NaN nan 
convert NaN to 16 bit signed 0 
convert NaN to 16 bit unsigned 0
________________




                                                                  18. A Few NaN Handing Cases
                                                                  1. IMM[15:0] Field In I-Type Instruction
The I-type instruction includes an IMM[15:0] field where users can specify a constant. If IMM[15:0] is defined as a NaN (e.g. 16’h8000, relevant only for floating-point operations), it will be interpreted as a NaN and may result in the output also being a NaN. Users should NOT specify IMM[15:0] as a NaN, regardless of how many bits of IMM[15:0] are utilized in the operations.
                                                                  2. FP16 Max / Min Instructions
For Max or Min instructions, out = max(a, b) or out = min(a, b), if a or b is NaN, the comparison between a and b can not be performed, max or min value can not be identified, so the output is set to be NaN.
                                                                  3. PreLu / Leaky_ReLu Instructions
The functions of both PreLu and Leaky_ReLu can be described as 
 
where a and b are two input signals. 
The following RreLu / Leaky_ReLu NaN handling matches PyTorch behavior:
                                                                  * If (a == NaN), output is NaN
                                                                  * If (a > 0), output is a, regardless of whether b is NaN or not.
                                                                  * If (((a < 0) || (a==0)) && (b == NaN)), output is NaN.
                                                                  1. Threshold_ReLu Instruction
The threshold_relu function can be described as GPR(RD) = (GPR(RS1)<IMM) ? 0 : GPR(RS1). 
The following threshold_relu NaN handing matches PyTorch behavior:
                                                                  * If (GPR(RS1) == NaN), GPR(RD) is NaN
                                                                  * If ((GPR(RS1) != NaN) && (IMM == NaN)), GPR(RD) is GPR(RS1). 
                                                                  1. M_partial Register Update in SEA Instruction
The m_partial​ register is introduced to store the maximum value of processed tiles to support Flash Attention. When the maximum value of a new tile is received, it is compared with the current value in the m_partial​ register, and the larger value is retained.
If the existing value in m_partial register or the maximum value of a newly received tile is NaN, the m_partial register will be updated with NaN. This is consistent with the NaN handling of max function.
                                                                  2. LUT Functions
Any instruction (regular LUT instruction, eLUT instruction, reciprocal instruction, square root instruction, reciprocal square root instruction, exponential instruction) that uses the LUT table will handle NaN as follow: 
                                                                  * If the input to the LUT table is NaN, the output will be NaN.
                                                                  * If certain entries in the LUT table are configured as NaN (either lower 16 bits or upper 16 bits), the instruction that accesses those entries will produce NaN as a result. This would not apply to exponential instruction, as its LUT is hardwired.


                                                                  3. Mul(0, inf)
See issue
                                                                  4. Divided By Zero
When a divide-by-zero condition occurs during the execution of the RECIPI_IND_FP (or RECIPI_DEP_FP) instruction, the output will be the value programmed in the LUT table (special entry which stores the data corresponding to input equal to 0). For example, positive maximum value is one option in this scenario. Users can also set the output to be NaN by programming the LUT in such a way.


                                                                  5. Square Root Of A Negative Value
If the input to the square root instructions (ASQRT_IND_FP or ASQRT_DEP_FP) is negative, the output will be NaN.


                                                                  6. Add(-max, +max) 
















                                                                  8. Algorithm Examples Implemented By VPU 
This chapter discusses several examples of implementing complex algorithms using VPU instructions, such as Flash Attention and LayerNorm.
                                                                  7. SoftMax-Based FlashAttention
Chapter 5 discusses the details of the FlashAttention algorithm implemented on the VPU. This chapter focuses on the step-by-step implementation of the algorithm with the goal of aiding microcode generation.
Assuming that tensors for Query, Key, and Value are already available, each with a size of 1024x128 (eight tiles, corresponding to 1024 tokens and 128 features), this chapter will discuss two approaches (Single-Tile FlashAttention, and Dual-Tile FlashAttention) for generating the Attention tensor (ATT). These approaches utilize Grid for MatMul, VPU instructions (e.g., SEA), VBuff (storage) and snooping circuit.
Both approaches achieve nearly 100% VPU utilization. The Single-Tile FlashAttention uses less VBuff space (4 tiles) but has lower Grid utilization (~66%). The Dual-Tile FlashAttention uses more VBuff space (6 tiles), resulting in higher Grid utilization (~100%) and thus lower latency.
________________


Note: The FlashAttention performance analysis has the following assumptions
                                                                  1. Query, Key and Value tensors all have FP8 data format. All the intermediate results (e.g. , attention score A, and AV, etc) are in FP16 data format to preserve high precision.
                                                                  2. The FlashAttention implementation in Pyxis primarily involves tile-based tensor operations and a small number of single or few-instruction operations (e.g., scaling factor generation). The performance analysis focuses solely on the tile-based tensor operations and does not account for the single or few-instruction operations.
                                                                  3. The analysis focuses solely on the steady-state performance and excludes the initial and final phases of FlashAttention
________________




                                                                     19. Algorithmic-Level Math
The algorithmic-level math is shown in Figure 8.1, it does NOT represent the FlashAttention algorithm steps. The FlashAttention is introduced to implement the math shown in Figure 8.1. 
Each square box represents a tile (128x128). The boxes with solid color are the tensors in normal shape, and the ones with gradient color are transposed. 
The Attention generation has consisted of three parts: MatMul to compute , SoftMax on results to generate attention score , then MatMul to compute . This is consistent with Figure 5.2.  
  

Figure 8.1: Arithmetic-level math for Attention generation






________________


Note: To allow the same ALU to process all elements in the SoftMax operation, the elements must be aligned vertically, as shown in Figure 8.1. Therefore, MatMul operation is replaced by , and MatMul  is replaced by . The final Attention tensor is , instead of 
________________




                                                                     20. Single-Tile FlashAttention (STFA)
Single-Tile FlashAttention can perform the entire operation shown in Figure 8.1, but it processes one tile at a time, such as generating  for one tile, using the snooping circuit to detect the maximum value of each token vector within the tile for SoftMax computation (see Equation 5.2), and performing MatMul  for the tile.
FlashAttention operation will involve Grid (MatMul), VPU, VBuff (storage) and Snooping Circuit (max value detection). Figure 8.1 shows  tensor has 64 tiles (8x8), the SoftMax will be conducted along each column. Figure 8.2 illustrates the operation pipelines of each component, and VBuff occupancy along a single column (corresponding to 128 tokens). Figure 8.3 shows the data movement between components.
Some terminologies used in Figure 8.2: 
                                                                     * QK0:            MatMul result for tile 0.
                                                                     * SEA(QK0): Perform VPU SEA instruction on QK0. It will generate scaling factor, attention score (A) and the sum of the  
                   exponential function result (
                                                                        * A(QK0):      Attention score of QK0 without division by the sum of the exponential function result. 
                   For example , not . The division by    will be performed in the final step to reduce the number of operations, save power and enhance performance.
                                                                        * Max0:         A single row vector (128 elements), each element representing the maximum value of one token vector in the      
                   tile.
                                                                        * AV0:           The MatMul of A(QK0) and corresponding Value (V) tile. E.g AV00 = MatMul(A00, V00) in Figure 8.1.
                                                                        * Scale_0:     Apply scaling factor to Acum_AV based on the Max0
                                                                        * Acum_AV:  Accumulation of AV over multiple tiles. For example, 8 tiles of AV would be accumulated together along each
                   column to produce final attention in Figure 8.1
Each square box in the pipeline diagram in Figure 8.2 represents 128 clock cycles, some operations (e.g. scale_0(Accum_AV)+AV0) occupy two square boxes, and take 256 clock cycles to complete the work. The procedure to complete scale_0(Accum_AV)+AV0) in 256 clock cycles is described as follow (assuming scaling factor is already in VPU GPR, the sequencer steps do not cost VPU instructions and their latencies can be hidden)
        for (i = 0; i < 128; i++) begin
                move Accum_AV[i] into INFIFO by sequencer
                move AV0[i] into INFIFO by sequencer
                GPR0 <= mul (scale_0, Accum_AV[i])
                OUTFIFO <= GPR0 + AV0[i] from INFIFO
                move data from OUTFIFO to VBuff by sequencer
        end
Due to the physical design, two tiles in VBuff are combined into a single physical bank, sharing one read port and one write port. Tile0/Tile1 (BankA), Tile2/Tile3 (BankB), Tile4/Tile5 (BankC), and Tile6/Tile7 (BankD) are each paired into a single physical bank. Each physical bank can support one read and one write operation in a single cycle.
To satisfy the sequencer's program requirements and prevent potential memory bank access conflicts, the four physical memory banks are assigned to the following tasks:
                                                                        * BankA: Move data from Grid (or ROD) to Vbuff, then to VPU.
                                                                        * BankB: Move data from VPU to VBuff, then to Grid (or ROD)
                                                                        * BankC: Move data from Grid (or ROD) to Vbuff, then to VPU.
                                                                        * BankD: Move data from VBuff to VPU, then to VBuff
As shown in Figure 8.2, we can draw the following conclusions for STFA
                                                                        1. Grid utilization can reach ~66% at steady state.
                                                                        2. VPU utilization can reach ~100% at steady state.
                                                                        3. VBuff peak occupancy is 6 tiles.
                                                                        4. There is no need to transmit intermediate data which are in FP16 between Grid and AMEM, or between VPU and AMEM. 
________________


Note: 
                                                                        1. Figure 8.1 contains eight tiles, while Figure 8.2 illustrates the pipeline for only the first four tiles. The operations for the remaining four tiles simply repeat those of the first four.
                                                                        2. Max0, Max1, etc are single row vectors, and each tile in VBuff has 128 entries, therefore multiple Max vectors generated by the snooping circuit can share one tile.
                                                                        3. One last step is not shown in Figure 8.2, which is to divide final Acum_AV by the sum of exponential function , which is already in VPU, because Figure 8.2 only shows processing of the first four tiles. The element-wise division results will initially be stored in other banks (BankA, BankB or BankC) before being moved to AMEM via ROD.
________________


  

Figure 8.2: FlashAttention pipeline and VBuff occupancy for STFA
  

Figure 8.3: Single-Tile FlashAttention data flow chart and pipelines
                                                                           21. Dual-Tile FlashAttention (DTFA)
Dual-Tile FlashAttention processes two tiles at a time. For example, it generates  in Grid for two tiles before initiating SoftMax, uses the snooping circuit to detect the maximum value for each vector within the two tiles, performs SoftMax for the two tiles, and then conducts MatMul  for both tiles.
The four physical memory banks are assigned to the following tasks (different from STFA) :
                                                                           * BankA: Move data from Grid (or ROD) to Vbuff, then to VPU.
                                                                           * BankB: Move data from Grid (or ROD) to Vbuff, then to VPU.
                                                                           * BankC: Move data from VPU to VBuff, then to Grid (or ROD).
                                                                           * BankD: Move data from VBuff to VPU, then to VBuff
Figure 8.4 illustrates the operation pipelines of each component, and VBuff occupancy along a single column in Figure 8.1 (128 tokens). The last pipeline step is to divide Acum_AV by . Figure 8.5 shows the data movement between components.
As shown in Figure 8.4, we can draw the following conclusions for DTFA
                                                                           1. Grid utilization can reach ~100% at steady state. 
                                                                           2. VPU utilization can reach ~100% at steady state.
                                                                           3. The total number of cycles to generate Attention is , where  is the number of tiles in token length.   
                                                                           4. VBuff peak occupancy is 7 tiles. 
                                                                           5. There is no need to transmit intermediate data which are in FP16 between Grid and AMEM, or between VPU and AMEM. 




  

Figure 8.4: FlashAttention pipeline, VBuff occupancy and data distribution inside VBuff for DTFA.
  

Figure 8.5: Dual-Tile FlashAttention data flow chart and pipelines
                                                                              22. Additional Details
As mentioned in section 8.1, the performance analysis in section 8.1.2 and section 8.1.3 do not include some operations which are controlled by single (or a few) VPU instructions. Those operations are
                                                                              1. Scaling Parameter Generation: After Max value (e.g. Max1) is generated by Snooping CKT, it will call a VPU instruction (SEA_SCALING_GEN) to generate the scaling factor, store the scaling factor in GPR, and update Max value stored in VPU (m_partial register in Figure 5.4). This will take one cycle for each tile in STFA, or one cycle for two tiles in DTFA.
                                                                              2. Scaling of Sum Of Exponential Values: The value , generated by the SEA function and stored in the FP21 register, needs to be scaled by a scaling parameter. This scaling occurs once per tile in STFA, or once for every two tiles in DTFA. 
There are two ways to conduct scaling. One way is to convert the sum value from FP21 domain to FP16 domain, then scale the same value through floating point multiplication, this is because floating point multiplication in FP21 is not supported.  This approach has to take two instructions, the first instruction moves  into FP16 GPR, the second instruction conducts the scaling. The second approach is to convert the multiplication-based scaling process into exponent adjustment within the FP21 domain, eliminating data conversion between FP21 and FP16 and thereby avoiding precision loss in the first approach, as discussed in this section. The second approach is recommended.
                                                                                 8. Sigmoid-Based FlashAttention
Sigmoid-based attention has been proposed as a computationally cheaper alternative to Softmax-based attention due to the element-wise nature of the sigmoid function. Softmax- and Sigmoid-based attention are expressed in Equation 8.2.1 and 8.2.2, respectively.   
                                                (E.q. 8.2.1)
                                                (E.q.8.2.2)
The Sigmoid function is described in Equation 8.2.3, where b is a hyper-parameter
                                                (E.q.8.2.3)
The Sigmoid function can be supported by the eLUT function in Pyxis and takes one instruction to complete (takes input from INFIFO directly, and writes the result to OUTFIFO in the same instruction).
The Signmoid-based attention has two MatMul operations and one Sigmoid function in between. The Sigmoid-based attention can also be implemented with hardware-aware and memory-friendly techniques, similar to Softmax-based FlashAttention discussed previously. Grid still performs two MatMul operations, while VPU handles the Sigmoid function and accumulation of the second MatMul's results across multiple tiles. 
In order to achieve near perfect performance, dual-tile FlashAttention (DTFA) is recommended. 
Implementing Sigmoid-based FlashAttention (assuming DTFA, as illustrated in Figure 8.1) is very similar to the procedure described in Figures 8.4 and 8.5. The only differences are replacing the SEA function with the Sigmoid function and removing the scaling operation between tiles. It can be described as follows, assuming Q, K, T tensors are divided into tiles, and each tile has size 128x128.
                                                                                 1. Read one tile of Q and and one tile of K from AMEM, send them to the grid to conduct the first MatMul operation.
                                                                                 2. Send the result in step 1 to VBuff.
                                                                                 3. VPU reads data in VBuff stored in step 2 and conducts Sigmoid function. Writes the result back into VPU OUTFIFO directly.
                                                                                 4. Move data generated in step 3 into VBuff.
                                                                                 5. Read one tile of tensor V from AMEM and feed it to the grid in the horizontal direction. Moves data generated in step 4 from VBuff to the grid in the vertical direction. Perform the second MatMul operation.
                                                                                 6. Moves the result in step 5 back to VBuff.
                                                                                 7. VPU reads data in VBuff stored in step 6 and conducts accumulation. The results will stay in VBuff.
                                                                                 8. At the end of Sigmoid-based attention computation, move data from VBuff back to AMEM.


                                                                                 9. LayerNorm
WIP.  Notion page
                                                                                 10. ArgMax
WIP


                                                                                 11. Multinomial Sample
WIP.  Notion page and Slack discussion.




                                                                                 9. Appendix
This chapter documents some additional VPU related information.
                                                                                 12. Special Handling In Near-Zero Rounding
The rounding near zero is improved by applying some special handling. The special handling is applied in the following cases, some are related to VPU.
                                                                                 1. FP16 to FP8 conversion (macro convert_fp16_to_fp8.vp, used in both VPU and the input path from AMEM to Grid)
                                                                                 2. FP16 EB adjustment (macro fp16_to_fp16_eb_adj.vp, used in both VPU, e.g. alu_fp.vp, alu_store.vp, and alu_sea_func.vp. It is also used in the input path from AMEM to Grid. The same macro is used for FP8 to FP16 conversion with EB adjustment)
                                                                                 3. FP21 to FP16 conversion (module alu_convert_fp21_to_fp16.vp, used in VPU)
                                                                                 4. MNS to FP16 conversion (macro convert_wb_mns_to_fp16.vp, used in the writeback path from Grid to AMEM or to VPU)
                                                                                 5. The instructions STORE_REG, DYN_EXP_ADJ_FP and EXTD_FP_DYN_EXP_ADJ


 


                                                                                 10. Design Reviews
                                                                                 1. Functional Specification Review August 10, 2023
Attendees: Jian Hui Huang, Gary Goldman, Arun Vaidyanathan, Kalyana Venkataraman, Harold Zable, Ashwin Radhakrishnan, Chung Lau, Jigar Savla, Pradeep Joginipally, YingFan Yang 
Meeting Minutes:
                                                                                 1. Update names in Figure 1 in the functional specification
                                                                                 2. Update the equation E.q. 2.1, bias EB is not fixed to be -7.
                                                                                 3. Add an instruction to let software set bias EB value, and store the value in a register (e.g. a dedicated EB Reg).
                                                                                 4. Convert FP16 to INT16, four scenarios
                                                                                 1. Conventional conversion: Round to zero (2.8 round to 2, -1.7 round to -1)
                                                                                 2. Flooring (2.9 round to 2; -2.9 round to -3)
                                                                                 3. Ceiling (2.1 round to 3; -2.9 round to -2)
                                                                                 4. Round to nearest (not including round to even, 2.5 will be round to 3)


                                                                                 2. Functional Specification Review, Part 2, November 16, 2023
Attendees: Jian Hui Huang, Gary Goldman, Arun Vaidyanathan, Kalyana Venkataraman, Harold Zable, Ashwin Radhakrishnan, Chung Lau, Jigar Savla, Pradeep Joginipally, YingFan Yang, John Keen, Siddarth Seetharaman, Michelle Kong. 
Meeting Minutes:
                                                                                 1. Better description of the instruction SLTUI
                                                                                 2. For instructions MAXI and MINI, make it clear that the IMM field represents signed value.
                                                                                 3. Remove instructions SLTU_FP and SLTUI_FP. There is no use of them.
                                                                                 4. For the MULI_FP instruction, the IMM field should be treated identically to any value stored in GPRs, implying that the IMM field should be subjected to the same bias (EB) during computation. 
                                                                                 5. If floating point data have a 5-bits exponent, fixed EB (-15) to be used.
                                                                                 6. For reciprocal instruction, if the denominator is zero, the output should be maximum value.
                                                                                 7. How to support the flexible way to construct LUT index ? 
                                                                                 8. The review covered up to half of Table 6.6




CONFIDENTIAL - Recogni Inc. | Recogni GmbH                                         -                 
[a]Maybe it makes sense to use variables (Tools -> Variables) for equation/figure/table numbers, instead of having to manually chase down back links whenever things are renumbered?
[b]Agreed
[c]We also support NaN with the code point 0x8000 (otherwise known as negative zero in IEEE). We do not support Inf or -Inf.
[d]I would have expected the EB for x to be used for the index computation and delta. And the EB for y for basevalue and slope. This is not reflected in the wording of this paragraph.
[e]EB for x shold not be included in index construction, it's not necessary, but should be included in LUT content generation.
[f]I have the impression that we are aligned. I did not mean index in terms of bits, but index in terms of represented value, which includes the EB. Maybe we can reformulate in such a way that EB_x is used for the input of "function" and EB_y is used to encode the output. WDYT? This could make it clearer to the reader.
[g]The Equations 4.3 and 4.4 have EBx included to show how the basevalue and slope are generated.
[h]@pradeepj@recogni.com stated "For RCP(0), we output max saturated value with input sign."


Besides my request to add this info to the doc (FYI @jhhuang@recogni.com and @harold.zable@recogni.com) I am a bit confused. Where are the data types defined for the VPU? If they are identical to the UIE (see Pyxis spec), then we have no negative zero, so sign of input that is zero is not meaningful.
[i]Yeah, you are right. It will always be sign=0, MaxE and MaxF for RECIP(0) as the input sign will always be 0.
[j]Since FP16 with a default bias of -15 has numbers both larger and smaller than one, it'd be more straightforward to have a RCP opcode do a simple 1/x, instead of (2^32)/x.
[k]If we do simple 1/x, the result may not representable by FP16. E.g. exponent can be negative.
[l]Only if you ignore EB. With EB, the value should generally be in range.


Does the RCP instruction pay attention to EB? Or does it allow you to specify an EB?
[m]See section 4.4.1.1 (in next page)
[n]I wish that the RECIP instruction did allow for its own EB adjust. If a user wants to do a RECIP that isn't immediately followed by a MUL to form a divide, they're going to have to keep track of the intermediate change in the EB. If they want to output the answer, they may need to perform an extra EB on the way out to compensate.


I suppose it could all be handled with careful programming, but it feels to me like one of those rough edges that complicates the software. Math operations should be atomic, when at all possible.
[o]The current approach is chosen to prevent premature underflow during reciprocal calculation, it's guaranteed. Careful programming is not an option, is a must. Users have to know what they do and how to avoid premature underflow.  If users want to do simple RECIP that isn't floowed by a MUL to form a divide, they can always do a MUL by 1 (e.g. 2^EB). 


For the popular applications we know (e.g. variance calculation in LayerNorm), the current approach should serve the purpose.
[p]@philipp@recogni.com , this is what I was wondering about earlier. Will it be acceptable to the compiler for RECIP to work like this without EB adjust? This causes the output EB to be different from the input EB, and not adjustable within the RECIP instruction.
[q]It is indeed weird, that it's not possible to do `1/B` as an individual instruction, but have to do `1 * 1/B`. This will be really unexpected behavior for people writing VPU algorithms.


We could hide this in the compiler. But that would then require extra specialized logic in the compiler, just to make `1/B` division work. Also it would introduce additional instructions, causing additional cycles and additional complexity for an optimization algorithm to deal with.


IIUC the described procedure is implemented in HW? Wouldn't the only change that needs to be made here to swap out the `31` in Eq 4.32 with e.g. `IMM[5:0]`?
[r]You always do A/B. 1/B is just a special case when A=1.  Nothing weird about.  In the applications we have seen so far, A/B is typical cases, not 1/B
[s]If that is the assumption, I would even argue that `A/B` should then be one instruction and RECIP to be removed. 


Because the instruction set currently gives the impression, that 1/B is the typical case for the RECIP instruction and `A/B` is just a short form for writing `A * 1/B`, where if `A==1` it simplifies to `1/B`.
[t]A/B is too much for one instruction in implementation.
[u]Yes I agree. But that results in an (semi-)unusable instruction (on its own) that requires special handling.
[v]Combining A/B in one instruction is not good for the applications we are targeting (e.g. LayerNorm calculation), it hurts the performance, power and area. 


I disagree with the statement of "that results in an (semi-)unusable instruction (on its own) that requires special handling."  Users have to know exactly what they do and what they want to achieve.
[w]But if users want to do `1/B`, they want to do `1/B`, not `1 * 1/B`. This is definitely something we will have to hide in the compiler or the SDK or somewhere else in the SW stack.
[x]I would suggest supporting the general case A/B by compiler or SDK, and enable 1/B by setting A=1.
[y]I'm more talking about generating the VPU program. This is kinda a really special case for codegen.


Michael L. also just pointed out, that one could do a `STORE` after the `RECIP` to do the EB adjustment. Same "problem", but you save a few cycles over doing `MUL 1`.
[z]In all the applications we know so far, all of them are A/B (e.g. LayerNorm, SoftMax, etc).  I assume compiler / SDK would provide the basic operation like A/B, 1/B is covered automatically.
[aa]As this can be define by software, we should mark it as example values for base and slope.
[ab]The slope part can be defined by software, but not basevalue part.
[ac]This is confusing to me, why can't the base values also be defined in software?
[ad]Where does this constraint come from?
[ae]If EB of slope is -2, the slope in Equation 4.32 may have exponent less than 0, which is not properly representable in FP16. If EB is -3 or more negative, it would avoid this issue.
[af]Consider adding a formula that explicitly calculates `rounding`. Also consider rewording this paragraph as it's somewhat confusing
[ag]I have some doubts, I feel like EB_B should be rescaled by half relative to EB_D on account of the square root..?
[ah]EB_B is used to compensate for (32-EB_x)/sqrt(x).
[ai]To further clarify, every CSR entry refers to a 64-bit = 8-byte entry, so in the NOC mapping, you add 0x800 to the address to refer to the LUT 1, as opposed to LUT 0.
[aj]A general note on this section:


It feels like this section is interleaving descriptions of a few different things:


o The motivation for creating SEA.
o The expense of various hardware options.
o The math followed by SEA & FlashAttention.
o Descriptions of how multiple tiles are computed for some parts of the algorithm.


If you could separate these components, that would make it all easier to understand.


Also, it would really help to have a section that only contained the instructions of the steps needed to perform this action, along with the results of each step along the way. That would help guide potential users into using the technique, without having to worry about its history.
[ak]assuming this was also rounded via RNDUP_EVEN? If so would be nice to be explicit
[al]Value stored in RS2 is previous Max value. As shown in Figure 5.7, it should have UNDUP_EVEN value, e.g. green_box to blue_box then to yellow_box (RS2)
[am]I guess I'm asking because the text for RS1 is explicit about saying that this value has been rounded, so I think it makes sense and doesn't hurt to be just as explicit in the text for RS2
[an]OK, I added a statement to make it clear.
[ao]this comment is outdated
[ap]sign_bit needs to be zero if in.e == 0
[aq]Yes.
[ar]@jhhuang@recogni.com @wei.liu@recogni.com noticed that you need to assign sign to 0 if e and f are both 0. This behavior is already tested and c-emu code matches.
[as]Done. Thanks
[at]sign_bit needs to be zero if in.e == 0
[au]Yes.
[av]@jhhuang@recogni.com @wei.liu@recogni.com noticed that you need to assign sign to 0 if e and f are both 0. This behavior is already tested and c-emu code matches.
[aw]Done. Thanks
[ax]OP-CONV, maybe?
[ay]"Enhanced" provides information about the history of how the instruction came about. It doesn't provide information to the reader as to how to use it.


If this is really about wider data values, maybe these should be called "wide" instructions.
[az]Could you add a short section that describes how the NN-Reg works? I think you have to write to it (either left or right) before reading from it? 


Also, could we have some ALUs conditionally writing to the left, others to the right, and then... something weird happens?
[ba]Yes, the ALU_ID is 7 bits wide, but I don't think adding the "[6:0]" here helps explain that. It makes me wonder if other bit selections would mean something under other conditions.
[bb]ALU_ID is just a constant value from 0 to 127. That's all. Other bits are not being used.
[bc]Whenever there's one of these forbidden operations, it's important to describe what will happen if someone does this, anyways. Does the chip catch fire? Does the instruction get ignored?
[bd]That is unfortunate. It's the kind of "gotcha" that's going to complicate the compiler guys' lives. I'm curious as to what caused this hardware restriction.
[be]This is a pretty gnarly restriction to place on the compiler which will make register allocation much more difficult. Why do we have this restriction?
[bf]The reason is very simple, just need to see the special code points reserved for destination RD. 1_1111, 1_1110 and 1_1101 are reserved for OUTFIFO, Left NN-Reg and Right NN-Reg. 


On top of that, writing data to both OUTFIFO and GPR at the same time is not typical use case.
[bg]A better alternative, if possible, would be for both inputs to pop the same value from INFIFO, if RS1 == RS2. Only one pop, just the same value is read twice.
[bh]We just realized recently that existing design can actually support, TB (test bench) added the constraint. Now the spec is updated,  both RS1 and RS2 can specify INFIFO in the same instruction.  Thanks for your suggestion.
[bi]@jhhuang@recogni.com - this entry is out of place, it should be at the bottom of the table (0xe encoding)
_Assigned to jhhuang@recogni.com_
[bj]That's a little odd. Why?
[bk]Because this instruction just needs data in GPR(RS1), so RS2 should be ignored.
[bl]Oh. Okay. That's would be a general rule for operations that only take one opcode, right? It's just that this is the only such opcode.
[bm]Just a reminder to make sure that any comparison with NaN fails. Maybe mention that here; it surprises people sometimes.
[bn]Also, as @miclaraia@recogni.com  has alluded to elsewhere, the output should be INT16 1/0, not FP 1/0.
Further, given the special behavior of NaN (all comparisons fail), it would be useful to have an SGT_FP as well. Probably a lot more useful than an SLTU_FP.
[bo](NaN == NaN) = 0 right?
[bp]Correct.
[bq]It looks like RTL is using funct7 in this operation the same way as MUL_W_EB. It is a bit confusing why all operations here have funct7 set to 0.
[br]@jhhuang@recogni.com there are a whole host of instructions where the encoding for the Funct7 is basically the EB value - can we instead of saying 7'h0 all over the place say "RS1 EB" or "OUT EB" etc?
[bs]Why does ABS have this restriction?
[bt]This restriction only applies to RS2 which is unused during ABS
[bu]Can we change this to during this instruction RS2 has to be 5'd0?
[bv]I hope you mean that RS2 can be the special ZERO register. Expecting that RS2 is a valid GPR set to 0 is unlikely to be something the compiler will want to do.
[bw]@yingfan.yang@recogni.com
[bx]Hi Shaba, RTL have varies instructions that is R Type (both RS1 and RS2 field are present), but RS2 isn't used. This caused issue during randomization, when stimulus has RS2 generated as INFIFO. This would cause a INFIFO pop when RS2 is don't care. After discussion with JianHui, the intent of the design is to not pop the INFIFO in this case but the implementation still popped the INFIFO. I asked the original question if JH wants to enforce in programming that RS2 value must be zero when the field is don't care. This would allow the intent to match implementation without changing the design. This was the only special case back then, but as more instructions are added there are more cases when this happened. Thus later while fixing another edge case, JH updated the design to blanket handle don't care.
[by]👍
[bz]So does this use the "simple underflow" scheme, or the "perfect underflow" scheme? The difference is that "simple underflow" rounds 2^0 = 1 down to zero, while "perfect underflow" sets the rounding threshold at 2^(-1)=0.5.


I'm asking because this sounds like the "simple underflow" scheme, and I thought in previous discussions it was agreed that the "perfect underflow" scheme would be used for FP->FP kinds of conversions
FYI @tom@recogni.com @jan@recogni.com
[ca]agree, ideally this should be aligned with other FP->FP ops
[cb]I have updated the spec, as shown in the Table 6.3. The underflow handling (e.g. rounding near zero) is the same as EB adjustment in FP16
[cc]What's the behavior for NaN?
[cd]A sign-extended OR and AND seem kind of odd to me. I think the non-sign-extended would be more generically useful. Probably the same is true for XOR, although having a way to do a bitwise logical inversion is quite useful.
[ce]This is, in fact, true for all the 16-bit INT instructions. Maybe you should mention that somewhere up above.
[cf]The instruction is called "RECIPI", even though the value it processes is not an immediate operand, but a register, yes?
[cg]what does trigger an interrupt concretely mean?
[ch]interrupt is a way that hardware informs software that something is wrong. In this case, divide by zero occurs.
[ci]You might again want to note that all comparisons with NaN fail.
[cj]LUT_1 is gone now, right?
[ck]Not yet.
[cl]Particularly because of the "all comparisons with NaN fail" rule, make sure that you've got this expression written the way you want it.
[cm]EXP(x,eb_in) could result in saturation(7fff)
What should be the expected behavior when we adj the result with eb_out? Should it be sticky at 7fff?
[cn]Did this get resolved?


This is an unusual situation, but sticky is not a great solution. We run the risk that someone will use a negative output EB and expect that the value will be moved into range. Similarly, a very small output might be expected to be biased into range... but that won't happen, either.


The only safe thing to do here would be to not provide an output EB at all, since it's not going to produce a numerically correct result.
[co]If you're doing an unsigned add, you'd have to always clear the carry before you start. Could we have an unsigned add that doesn't include the carry?
[cp]That may not be true. Think about a case to support INT48, or INT64, etc. We need carry in for unsigned add.
[cq]That's fair. But every INT48, INT64, etc. add starts with an unsigned add that wants to ignore the carry.
[cr]Why do we need separate LOAD_NN and LOAD_INFIFO instructions? Wouldn't ADD zero do the same thing?
[cs]Same question, plus: I assume for all those instructions, RS1 _must_ be the register the instruction expects, i.e. `INFIFO` here?
[ct]Also, why not one `LOAD` instruction, that can just take all FP16 registers (GPRs, NN, CNTR, ZERO, INFIFO)?
[cu]ADD zero can do the same thing logically, but FP ADD will take three cycles, so you can use the data immediately.
[cv]@jhhuang@recogni.com what happens if data is <= IMM, what ends up written to RD? A stale value? Would it be 0? Would it be the IMM?
_Assigned to jhhuang@recogni.com_
[cw]If the condition is NOT met, value is GPR(RD) is untouched.
[cx]The same as LOAD_GT_IMM, except for NaN, I believe. Good.
[cy]Now the NaN handling was added.
[cz]This whole instruction is a problem for the compiler, as it reads 3 values (RS1, RS2, RSS) and writes to 3 values (RD, RS2, RSS). This is completely out of line with every other instruction, even outside of the VPU.
[da]This instruction is a fused instruction which combine several operations into on instruction, similar to other SEA instructions introduced to speed up FlashAttention.
[db]The main difference is that this instruction has 3 arguments, while all other only have 2 arguments. The compiler also is based on (at most) binary operations, because we were able to express everything with that. This instruction changes this, which makes it unlikely that we'll support this any time soon in the compiler.
[dc]This instruction fused more operations and may need some special handling.
1 total reaction
Philipp Krones reacted with 👍 at 2025-06-25 11:14 AM
[dd]In the algorithm we can assume that RS1 and RS2 are even-rounded integers in FP format. Does this assumption extend to the instruction as well? Can we require that any inputs to this instruction are even-rounded integers in FP format as a pre-condition, or do we have to correctly handle non-even-integer inputs as well?


If so, how does this instruction behave if it receives invalid inputs? My current assumption is that we should cast inputs to integer and then do the subtraction and multiplication by 1.5 using integer math
[de]It requires that any inputs to this instruction are even-rounded integers in FP format as a pre-condition.  It is users' responsibility to meet this requirement. RTL has its way to handle the calculation. Emulation can cast inputs to integer, then do the calculation.
[df]What will RTL do if it receives invalid inputs (in this case specifically talking about non-even-int rounded floats, and maybe also nan)? In the past we had to make sure that emu matches RTL outputs even when the inputs are invalid.
[dg]If it's NaN, the output will be NaN.


If it's non-even-int rounded floats, RTL doesn't know, it will treat it in the same was as if the input is even-int rounded.
[dh]Ok, I'll stick with casting to integer and doing the calculation via integer math for now. I'm curious to see whether this will end up matching RTL for invalid inputs
[di]> If it's NaN, the output will be NaN.


The output (RD) is an 8-bit integer and cannot be NaN
[dj]@jhhuang@recogni.com bump, still wondering about NaN handling
[dk]If the result is NaN, it will use entire 16-bit to represent it,  rather than 8-bit
[dl]Let me update the spec to make it clear.
[dm]@jhhuang@recogni.com RS2 can't be ALU_ID or NN_REG?
[dn]What does the instruction do with RSS? RD is computed from RS1 and RS2 only, right? So why do we need RSS as well?
[do]I guess this instruction also updates m_partial?
[dp]Figure 5.7 shows that the value 1.039720xGPR(RS1) is stored in RSS. This value is needed in this operation.


It does updates m_partial
[dq]> This value is needed in this operation


I don't understand, how is it needed by this operation? This instruction calculates the integer `k`, right? And from what I understand, `k = 1.5 (RS1 - RS2)`, so where is RSS used?


Or are we just saying that a side-effect of this instruction is that 3/2·log(2)·RS1 is stored in RSS?
[dr]Pls. see Figure 5.7. This instruction not only computes 1.5(RS1-RS2), it will also update m_partial register.
[ds]So does this use the "simple underflow" scheme, or the "perfect underflow" scheme? The difference is that "simple underflow" rounds 2^0 = 1 down to zero, while "perfect underflow" sets the rounding threshold at 2^(-1)=0.5.


I'm asking because this sounds like the "simple underflow" scheme, and I thought in previous discussions it was agreed that the "perfect underflow" scheme would be used for FP->FP kinds of conversions
FYI @tom@recogni.com @jan@recogni.com
[dt]agree, ideally this should be aligned with other FP->FP ops
[du]I have updated the spec, as shown in the Table 6.3. The underflow handling (e.g. rounding near zero) is the same as EB adjustment in FP16
[dv]I think SEA_SOFTMX could have the same issue if followed by LAYRN_LOAD_MEAN